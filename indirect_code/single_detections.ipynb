{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending HelioLinC to Single Detections\n",
    "#### Matt Holman, Matt Payne, Ed Lin, Dave Gerdes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27 July 2018\n",
    "\n",
    "\n",
    "The plan is to:\n",
    "\n",
    "    * Import some libraries.\n",
    "    * Read in Ed's test data set.\n",
    "    * Explore that data set.\n",
    "    * Write routines to do the single detection shift-and-add kind of thing.\n",
    "    \n",
    "The goal is to finish what is needed for a quick paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NOVAS package\n",
    "\n",
    "First, get the USNO's python NOVAS package.  We'll need that.\n",
    "\n",
    "http://aa.usno.navy.mil/software/novas/novas_py/novaspy_intro.php\n",
    "\n",
    "Just type \n",
    "\n",
    "pip install novas\n",
    "\n",
    "pip install novas_de405\n",
    "\n",
    "Here's the reference:\n",
    "\n",
    "Barron, E. G., Kaplan, G. H., Bangert, J., Bartlett, J. L., Puatua, W., Harris, W., & Barrett, P. (2011) “Naval Observatory Vector Astrometry Software (NOVAS) Version 3.1, Introducing a Python Edition,” Bull. AAS, 43, 2011.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble \n",
    "\n",
    "Here we import the packages that will be needed in the rest of the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy.interpolate\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import math\n",
    "import kepcart as kc\n",
    "import healpy as hp\n",
    "import collections\n",
    "import astropy\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import MPC_library\n",
    "import scipy.spatial\n",
    "import pickle\n",
    "from operator import add\n",
    "import bisect\n",
    "import operator\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# This part is instantiating objects with observatory \n",
    "# position information\n",
    "Observatories = MPC_library.Observatories\n",
    "ObservatoryXYZ = Observatories.ObservatoryXYZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize the detections into exposures.  \n",
    "\n",
    "For the detections we need to keep:\n",
    "    * objid\n",
    "    * RA\n",
    "    * Dec\n",
    "    * mag\n",
    "    * fakeid\n",
    "    \n",
    "For the exposures we need to keep:\n",
    "    * JD_utc\n",
    "    * xe, ye, ze\n",
    "    * RA_center\n",
    "    * Dec_center\n",
    "    * date\n",
    "    * nite\n",
    "    * expnum\n",
    "    * band\n",
    "    * exptime\n",
    "    * limiting_mag\n",
    "    \n",
    "Select reference vector and reference time.\n",
    "We want the reference vector to be toward the center of the data, as seen from the sun, assuming a distance.  \n",
    "Try g=0.025 at first.\n",
    "\n",
    "Rotate the observatory positions and RA, Dec values to ecliptic coordinates.\n",
    "\n",
    "Transform all ecliptic long/lat values to the plane tangent to reference direction\n",
    "\n",
    "Select gamma and gamma-dot, then compute factors for each exposure\n",
    "\n",
    "Select alpha-dot\n",
    "\n",
    "Select beta-dot\n",
    "\n",
    "Compute alpha and beta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reading Dave and Ed's test data file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are the files that only have the detections for the synthetic moving objects.\n",
    "filelist= [\n",
    "    'des_proof_of_concept_data/fakesonly/wsdiff_season301_Y1_griz_fakesonly.fixed.csv',\n",
    "    'des_proof_of_concept_data/fakesonly/wsdiff_season301_Y2_griz_fakesonly.fixed.csv',\n",
    "    'des_proof_of_concept_data/fakesonly/wsdiff_season301_Y3_griz_fakesonly.fixed.csv',\n",
    "    'des_proof_of_concept_data/fakesonly/wsdiff_season301_Y4_griz_fakesonly.fixed.csv'\n",
    "    ]\n",
    "\n",
    "# These are the files that have both the synthetic moving objects and false detections, but\n",
    "# the false detections are not in the correct location for the corresponding exposures.\n",
    "scrambledlist= [\n",
    "    'des_proof_of_concept_data/scrambled_fakes/wsdiff_season301_Y1_griz_scrambled_fakes.fixed.csv',\n",
    "    'des_proof_of_concept_data/scrambled_fakes/wsdiff_season301_Y2_griz_scrambled_fakes.fixed.csv',\n",
    "    'des_proof_of_concept_data/scrambled_fakes/wsdiff_season301_Y3_griz_scrambled_fakes.fixed.csv',\n",
    "    'des_proof_of_concept_data/scrambled_fakes/wsdiff_season301_Y4_griz_scrambled_fakes.fixed.csv'\n",
    "    ]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the exposure information first and then add some fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EXPNUM</th>\n",
       "      <th>RADEG</th>\n",
       "      <th>DECDEG</th>\n",
       "      <th>DATE_OBS</th>\n",
       "      <th>BAND</th>\n",
       "      <th>EXPTIME</th>\n",
       "      <th>MJD_OBS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>229683</td>\n",
       "      <td>0.468621</td>\n",
       "      <td>0.377768</td>\n",
       "      <td>2013-09-02T06:10:47.837043</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.257498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>229684</td>\n",
       "      <td>2.065533</td>\n",
       "      <td>1.392268</td>\n",
       "      <td>2013-09-02T06:12:46.472481</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.258871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>229685</td>\n",
       "      <td>3.659758</td>\n",
       "      <td>2.406768</td>\n",
       "      <td>2013-09-02T06:15:01.886059</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.260439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>229688</td>\n",
       "      <td>3.663221</td>\n",
       "      <td>0.377768</td>\n",
       "      <td>2013-09-02T06:21:30.678404</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.264938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>229691</td>\n",
       "      <td>0.457838</td>\n",
       "      <td>-1.651232</td>\n",
       "      <td>2013-09-02T06:27:54.655251</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.269383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>229692</td>\n",
       "      <td>3.655137</td>\n",
       "      <td>-1.651232</td>\n",
       "      <td>2013-09-02T06:30:06.609589</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.270910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>229693</td>\n",
       "      <td>5.258321</td>\n",
       "      <td>-0.636732</td>\n",
       "      <td>2013-09-02T06:32:06.124497</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.272293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>229694</td>\n",
       "      <td>6.857721</td>\n",
       "      <td>0.377768</td>\n",
       "      <td>2013-09-02T06:34:05.969268</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.273680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>229695</td>\n",
       "      <td>8.454933</td>\n",
       "      <td>1.392268</td>\n",
       "      <td>2013-09-02T06:36:08.302435</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.275096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>229696</td>\n",
       "      <td>10.051358</td>\n",
       "      <td>2.406768</td>\n",
       "      <td>2013-09-02T06:38:20.162801</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.276622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>229697</td>\n",
       "      <td>2.046671</td>\n",
       "      <td>-2.665632</td>\n",
       "      <td>2013-09-02T06:40:45.869068</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.278309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>229698</td>\n",
       "      <td>5.246971</td>\n",
       "      <td>-2.665632</td>\n",
       "      <td>2013-09-02T06:42:57.631171</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.279834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>229699</td>\n",
       "      <td>0.431417</td>\n",
       "      <td>-3.680132</td>\n",
       "      <td>2013-09-02T06:45:02.443958</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.281278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>229700</td>\n",
       "      <td>6.839817</td>\n",
       "      <td>-3.680132</td>\n",
       "      <td>2013-09-02T06:47:16.871230</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.282834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>229701</td>\n",
       "      <td>3.635617</td>\n",
       "      <td>-3.680132</td>\n",
       "      <td>2013-09-02T06:49:35.049841</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.284433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>229702</td>\n",
       "      <td>6.852538</td>\n",
       "      <td>-1.651232</td>\n",
       "      <td>2013-09-02T06:51:37.853865</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.285855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>229703</td>\n",
       "      <td>10.049938</td>\n",
       "      <td>-1.651232</td>\n",
       "      <td>2013-09-02T06:53:48.025210</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.287361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>229716</td>\n",
       "      <td>2.016979</td>\n",
       "      <td>-4.694632</td>\n",
       "      <td>2013-09-02T08:06:15.993847</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.337685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>229717</td>\n",
       "      <td>5.226079</td>\n",
       "      <td>-4.694632</td>\n",
       "      <td>2013-09-02T08:08:27.501084</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.339207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>229718</td>\n",
       "      <td>3.604454</td>\n",
       "      <td>-5.709132</td>\n",
       "      <td>2013-09-02T08:10:25.842980</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.340577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>229719</td>\n",
       "      <td>0.389354</td>\n",
       "      <td>-5.709132</td>\n",
       "      <td>2013-09-02T08:12:26.816230</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.341977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>229720</td>\n",
       "      <td>1.973450</td>\n",
       "      <td>-6.723632</td>\n",
       "      <td>2013-09-02T08:14:26.176951</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.343359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>229721</td>\n",
       "      <td>5.195450</td>\n",
       "      <td>-6.723632</td>\n",
       "      <td>2013-09-02T08:16:28.137979</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.344770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>229722</td>\n",
       "      <td>6.819554</td>\n",
       "      <td>-5.709132</td>\n",
       "      <td>2013-09-02T08:18:26.944471</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.346145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>229723</td>\n",
       "      <td>8.435179</td>\n",
       "      <td>-4.694632</td>\n",
       "      <td>2013-09-02T08:20:25.216666</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.347514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>229724</td>\n",
       "      <td>8.447271</td>\n",
       "      <td>-2.665632</td>\n",
       "      <td>2013-09-02T08:22:25.719124</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.348909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>229725</td>\n",
       "      <td>11.647571</td>\n",
       "      <td>-2.665632</td>\n",
       "      <td>2013-09-02T08:24:33.295422</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.350385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>229726</td>\n",
       "      <td>8.453821</td>\n",
       "      <td>-0.636732</td>\n",
       "      <td>2013-09-02T08:26:35.074324</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>229727</td>\n",
       "      <td>10.052321</td>\n",
       "      <td>0.377768</td>\n",
       "      <td>2013-09-02T08:28:33.629931</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.353167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>229728</td>\n",
       "      <td>11.649633</td>\n",
       "      <td>1.392268</td>\n",
       "      <td>2013-09-02T08:30:31.974521</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.354537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>599349</td>\n",
       "      <td>6.142708</td>\n",
       "      <td>-3.037608</td>\n",
       "      <td>2016-12-04T02:29:22.832617</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57726.103737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>599351</td>\n",
       "      <td>6.152925</td>\n",
       "      <td>-1.008708</td>\n",
       "      <td>2016-12-04T02:33:19.111757</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57726.106471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>599697</td>\n",
       "      <td>3.148275</td>\n",
       "      <td>2.119332</td>\n",
       "      <td>2016-12-05T01:23:12.695404</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.057786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>599699</td>\n",
       "      <td>3.132283</td>\n",
       "      <td>4.148232</td>\n",
       "      <td>2016-12-05T01:26:24.625545</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.060007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>599700</td>\n",
       "      <td>3.132283</td>\n",
       "      <td>4.148232</td>\n",
       "      <td>2016-12-05T01:28:24.022244</td>\n",
       "      <td>i</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.061389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>599707</td>\n",
       "      <td>2.957521</td>\n",
       "      <td>-1.008708</td>\n",
       "      <td>2016-12-05T01:41:48.338083</td>\n",
       "      <td>r</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.070698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>599708</td>\n",
       "      <td>1.554317</td>\n",
       "      <td>1.104832</td>\n",
       "      <td>2016-12-05T01:43:45.865633</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.072059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>599710</td>\n",
       "      <td>1.542900</td>\n",
       "      <td>3.133732</td>\n",
       "      <td>2016-12-05T01:46:56.902576</td>\n",
       "      <td>i</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.074270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>599711</td>\n",
       "      <td>1.542900</td>\n",
       "      <td>3.133732</td>\n",
       "      <td>2016-12-05T01:48:55.348865</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.075641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>599712</td>\n",
       "      <td>1.542900</td>\n",
       "      <td>3.133732</td>\n",
       "      <td>2016-12-05T01:50:53.846887</td>\n",
       "      <td>r</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.077012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>599714</td>\n",
       "      <td>3.132283</td>\n",
       "      <td>4.148232</td>\n",
       "      <td>2016-12-05T01:54:07.163708</td>\n",
       "      <td>r</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.079250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>599715</td>\n",
       "      <td>6.333375</td>\n",
       "      <td>4.148232</td>\n",
       "      <td>2016-12-05T01:56:06.716377</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.080633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>599718</td>\n",
       "      <td>1.363079</td>\n",
       "      <td>0.005792</td>\n",
       "      <td>2016-12-05T02:01:24.124961</td>\n",
       "      <td>r</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.084307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>599719</td>\n",
       "      <td>2.960129</td>\n",
       "      <td>1.020292</td>\n",
       "      <td>2016-12-05T02:03:21.991212</td>\n",
       "      <td>r</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.085671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>599720</td>\n",
       "      <td>2.960129</td>\n",
       "      <td>1.020292</td>\n",
       "      <td>2016-12-05T02:05:22.311540</td>\n",
       "      <td>i</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.087064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>599721</td>\n",
       "      <td>4.554767</td>\n",
       "      <td>2.034792</td>\n",
       "      <td>2016-12-05T02:07:20.795405</td>\n",
       "      <td>r</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.088435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>599722</td>\n",
       "      <td>6.154829</td>\n",
       "      <td>1.020292</td>\n",
       "      <td>2016-12-05T02:09:19.565549</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.089810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>599724</td>\n",
       "      <td>7.750575</td>\n",
       "      <td>2.034792</td>\n",
       "      <td>2016-12-05T02:13:15.604826</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.092542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>599726</td>\n",
       "      <td>4.557675</td>\n",
       "      <td>0.005792</td>\n",
       "      <td>2016-12-05T02:17:15.486513</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.095318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>599728</td>\n",
       "      <td>4.550158</td>\n",
       "      <td>-2.023208</td>\n",
       "      <td>2016-12-05T02:21:12.132313</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.098057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>599730</td>\n",
       "      <td>4.532342</td>\n",
       "      <td>-4.052108</td>\n",
       "      <td>2016-12-05T02:25:09.216850</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.100801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>600111</td>\n",
       "      <td>2.914979</td>\n",
       "      <td>-5.066608</td>\n",
       "      <td>2016-12-06T02:29:36.325825</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57728.103893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>602137</td>\n",
       "      <td>1.328129</td>\n",
       "      <td>-4.052108</td>\n",
       "      <td>2016-12-19T01:22:40.980411</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57741.057419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>602138</td>\n",
       "      <td>2.957521</td>\n",
       "      <td>-1.008708</td>\n",
       "      <td>2016-12-19T01:24:40.623946</td>\n",
       "      <td>i</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57741.058803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>602139</td>\n",
       "      <td>2.942400</td>\n",
       "      <td>-3.037608</td>\n",
       "      <td>2016-12-19T01:26:39.578978</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57741.060180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>602140</td>\n",
       "      <td>1.328129</td>\n",
       "      <td>-4.052108</td>\n",
       "      <td>2016-12-19T01:28:37.923112</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57741.061550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>602526</td>\n",
       "      <td>1.551733</td>\n",
       "      <td>-0.924168</td>\n",
       "      <td>2016-12-20T01:00:36.607920</td>\n",
       "      <td>r</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57742.042090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>602527</td>\n",
       "      <td>1.542900</td>\n",
       "      <td>3.133732</td>\n",
       "      <td>2016-12-20T01:02:39.390833</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57742.043512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>602913</td>\n",
       "      <td>1.352867</td>\n",
       "      <td>-2.023208</td>\n",
       "      <td>2016-12-21T01:12:06.179235</td>\n",
       "      <td>i</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57743.050072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>602914</td>\n",
       "      <td>2.957521</td>\n",
       "      <td>-1.008708</td>\n",
       "      <td>2016-12-21T01:14:05.211018</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57743.051449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1970 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      EXPNUM      RADEG    DECDEG                    DATE_OBS BAND  EXPTIME       MJD_OBS\n",
       "0     229683   0.468621  0.377768  2013-09-02T06:10:47.837043    g     90.0  56537.257498\n",
       "1     229684   2.065533  1.392268  2013-09-02T06:12:46.472481    g     90.0  56537.258871\n",
       "2     229685   3.659758  2.406768  2013-09-02T06:15:01.886059    g     90.0  56537.260439\n",
       "3     229688   3.663221  0.377768  2013-09-02T06:21:30.678404    g     90.0  56537.264938\n",
       "4     229691   0.457838 -1.651232  2013-09-02T06:27:54.655251    g     90.0  56537.269383\n",
       "5     229692   3.655137 -1.651232  2013-09-02T06:30:06.609589    g     90.0  56537.270910\n",
       "6     229693   5.258321 -0.636732  2013-09-02T06:32:06.124497    g     90.0  56537.272293\n",
       "7     229694   6.857721  0.377768  2013-09-02T06:34:05.969268    g     90.0  56537.273680\n",
       "8     229695   8.454933  1.392268  2013-09-02T06:36:08.302435    g     90.0  56537.275096\n",
       "9     229696  10.051358  2.406768  2013-09-02T06:38:20.162801    g     90.0  56537.276622\n",
       "10    229697   2.046671 -2.665632  2013-09-02T06:40:45.869068    g     90.0  56537.278309\n",
       "11    229698   5.246971 -2.665632  2013-09-02T06:42:57.631171    g     90.0  56537.279834\n",
       "12    229699   0.431417 -3.680132  2013-09-02T06:45:02.443958    g     90.0  56537.281278\n",
       "13    229700   6.839817 -3.680132  2013-09-02T06:47:16.871230    g     90.0  56537.282834\n",
       "14    229701   3.635617 -3.680132  2013-09-02T06:49:35.049841    g     90.0  56537.284433\n",
       "15    229702   6.852538 -1.651232  2013-09-02T06:51:37.853865    g     90.0  56537.285855\n",
       "16    229703  10.049938 -1.651232  2013-09-02T06:53:48.025210    g     90.0  56537.287361\n",
       "17    229716   2.016979 -4.694632  2013-09-02T08:06:15.993847    g     90.0  56537.337685\n",
       "18    229717   5.226079 -4.694632  2013-09-02T08:08:27.501084    g     90.0  56537.339207\n",
       "19    229718   3.604454 -5.709132  2013-09-02T08:10:25.842980    g     90.0  56537.340577\n",
       "20    229719   0.389354 -5.709132  2013-09-02T08:12:26.816230    g     90.0  56537.341977\n",
       "21    229720   1.973450 -6.723632  2013-09-02T08:14:26.176951    g     90.0  56537.343359\n",
       "22    229721   5.195450 -6.723632  2013-09-02T08:16:28.137979    g     90.0  56537.344770\n",
       "23    229722   6.819554 -5.709132  2013-09-02T08:18:26.944471    g     90.0  56537.346145\n",
       "24    229723   8.435179 -4.694632  2013-09-02T08:20:25.216666    g     90.0  56537.347514\n",
       "25    229724   8.447271 -2.665632  2013-09-02T08:22:25.719124    g     90.0  56537.348909\n",
       "26    229725  11.647571 -2.665632  2013-09-02T08:24:33.295422    g     90.0  56537.350385\n",
       "27    229726   8.453821 -0.636732  2013-09-02T08:26:35.074324    g     90.0  56537.351795\n",
       "28    229727  10.052321  0.377768  2013-09-02T08:28:33.629931    g     90.0  56537.353167\n",
       "29    229728  11.649633  1.392268  2013-09-02T08:30:31.974521    g     90.0  56537.354537\n",
       "...      ...        ...       ...                         ...  ...      ...           ...\n",
       "1940  599349   6.142708 -3.037608  2016-12-04T02:29:22.832617    z     90.0  57726.103737\n",
       "1941  599351   6.152925 -1.008708  2016-12-04T02:33:19.111757    z     90.0  57726.106471\n",
       "1942  599697   3.148275  2.119332  2016-12-05T01:23:12.695404    z     90.0  57727.057786\n",
       "1943  599699   3.132283  4.148232  2016-12-05T01:26:24.625545    z     90.0  57727.060007\n",
       "1944  599700   3.132283  4.148232  2016-12-05T01:28:24.022244    i     90.0  57727.061389\n",
       "1945  599707   2.957521 -1.008708  2016-12-05T01:41:48.338083    r     90.0  57727.070698\n",
       "1946  599708   1.554317  1.104832  2016-12-05T01:43:45.865633    z     90.0  57727.072059\n",
       "1947  599710   1.542900  3.133732  2016-12-05T01:46:56.902576    i     90.0  57727.074270\n",
       "1948  599711   1.542900  3.133732  2016-12-05T01:48:55.348865    z     90.0  57727.075641\n",
       "1949  599712   1.542900  3.133732  2016-12-05T01:50:53.846887    r     90.0  57727.077012\n",
       "1950  599714   3.132283  4.148232  2016-12-05T01:54:07.163708    r     90.0  57727.079250\n",
       "1951  599715   6.333375  4.148232  2016-12-05T01:56:06.716377    z     90.0  57727.080633\n",
       "1952  599718   1.363079  0.005792  2016-12-05T02:01:24.124961    r     90.0  57727.084307\n",
       "1953  599719   2.960129  1.020292  2016-12-05T02:03:21.991212    r     90.0  57727.085671\n",
       "1954  599720   2.960129  1.020292  2016-12-05T02:05:22.311540    i     90.0  57727.087064\n",
       "1955  599721   4.554767  2.034792  2016-12-05T02:07:20.795405    r     90.0  57727.088435\n",
       "1956  599722   6.154829  1.020292  2016-12-05T02:09:19.565549    z     90.0  57727.089810\n",
       "1957  599724   7.750575  2.034792  2016-12-05T02:13:15.604826    z     90.0  57727.092542\n",
       "1958  599726   4.557675  0.005792  2016-12-05T02:17:15.486513    z     90.0  57727.095318\n",
       "1959  599728   4.550158 -2.023208  2016-12-05T02:21:12.132313    z     90.0  57727.098057\n",
       "1960  599730   4.532342 -4.052108  2016-12-05T02:25:09.216850    z     90.0  57727.100801\n",
       "1961  600111   2.914979 -5.066608  2016-12-06T02:29:36.325825    z     90.0  57728.103893\n",
       "1962  602137   1.328129 -4.052108  2016-12-19T01:22:40.980411    g     90.0  57741.057419\n",
       "1963  602138   2.957521 -1.008708  2016-12-19T01:24:40.623946    i     90.0  57741.058803\n",
       "1964  602139   2.942400 -3.037608  2016-12-19T01:26:39.578978    z     90.0  57741.060180\n",
       "1965  602140   1.328129 -4.052108  2016-12-19T01:28:37.923112    z     90.0  57741.061550\n",
       "1966  602526   1.551733 -0.924168  2016-12-20T01:00:36.607920    r     90.0  57742.042090\n",
       "1967  602527   1.542900  3.133732  2016-12-20T01:02:39.390833    g     90.0  57742.043512\n",
       "1968  602913   1.352867 -2.023208  2016-12-21T01:12:06.179235    i     90.0  57743.050072\n",
       "1969  602914   2.957521 -1.008708  2016-12-21T01:14:05.211018    z     90.0  57743.051449\n",
       "\n",
       "[1970 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('TNOfakes_exposureinfo.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fakeid</th>\n",
       "      <th>a</th>\n",
       "      <th>e</th>\n",
       "      <th>i</th>\n",
       "      <th>lan</th>\n",
       "      <th>aop</th>\n",
       "      <th>M</th>\n",
       "      <th>epoch JD</th>\n",
       "      <th>H</th>\n",
       "      <th>mag</th>\n",
       "      <th>sun_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10005</td>\n",
       "      <td>54.603174</td>\n",
       "      <td>0.030831</td>\n",
       "      <td>70.951523</td>\n",
       "      <td>196.002411</td>\n",
       "      <td>193.558284</td>\n",
       "      <td>3.089665</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>7.410195</td>\n",
       "      <td>24.80</td>\n",
       "      <td>52.922298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10009</td>\n",
       "      <td>45.952954</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>18.692049</td>\n",
       "      <td>108.402264</td>\n",
       "      <td>121.216072</td>\n",
       "      <td>74.650853</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>5.885719</td>\n",
       "      <td>22.64</td>\n",
       "      <td>45.837868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10010</td>\n",
       "      <td>41.275114</td>\n",
       "      <td>0.128412</td>\n",
       "      <td>68.334839</td>\n",
       "      <td>17.053595</td>\n",
       "      <td>151.336680</td>\n",
       "      <td>203.948441</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>7.199051</td>\n",
       "      <td>24.02</td>\n",
       "      <td>46.214043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10011</td>\n",
       "      <td>46.164918</td>\n",
       "      <td>0.057041</td>\n",
       "      <td>11.421039</td>\n",
       "      <td>26.332649</td>\n",
       "      <td>33.323566</td>\n",
       "      <td>246.075320</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>3.396686</td>\n",
       "      <td>20.29</td>\n",
       "      <td>47.353920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10012</td>\n",
       "      <td>40.128168</td>\n",
       "      <td>0.010586</td>\n",
       "      <td>84.096966</td>\n",
       "      <td>128.176545</td>\n",
       "      <td>42.591766</td>\n",
       "      <td>117.896197</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>2.245534</td>\n",
       "      <td>18.47</td>\n",
       "      <td>40.330406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10022</td>\n",
       "      <td>45.237742</td>\n",
       "      <td>0.089611</td>\n",
       "      <td>74.035231</td>\n",
       "      <td>241.186446</td>\n",
       "      <td>94.726200</td>\n",
       "      <td>130.603474</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>7.903337</td>\n",
       "      <td>24.84</td>\n",
       "      <td>48.067886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10024</td>\n",
       "      <td>43.121750</td>\n",
       "      <td>0.168840</td>\n",
       "      <td>20.662362</td>\n",
       "      <td>276.393452</td>\n",
       "      <td>353.776064</td>\n",
       "      <td>45.734849</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>6.884150</td>\n",
       "      <td>22.97</td>\n",
       "      <td>38.792221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10028</td>\n",
       "      <td>54.020608</td>\n",
       "      <td>0.243808</td>\n",
       "      <td>9.050033</td>\n",
       "      <td>99.961688</td>\n",
       "      <td>322.487042</td>\n",
       "      <td>253.955759</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>2.821032</td>\n",
       "      <td>20.72</td>\n",
       "      <td>60.272758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10029</td>\n",
       "      <td>50.277402</td>\n",
       "      <td>0.151148</td>\n",
       "      <td>67.737131</td>\n",
       "      <td>106.252410</td>\n",
       "      <td>303.505116</td>\n",
       "      <td>315.638477</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>6.883018</td>\n",
       "      <td>23.65</td>\n",
       "      <td>45.505531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10034</td>\n",
       "      <td>52.980594</td>\n",
       "      <td>0.149964</td>\n",
       "      <td>46.852903</td>\n",
       "      <td>49.159801</td>\n",
       "      <td>54.750601</td>\n",
       "      <td>257.574750</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>6.129218</td>\n",
       "      <td>23.75</td>\n",
       "      <td>55.759872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10038</td>\n",
       "      <td>46.689398</td>\n",
       "      <td>0.088686</td>\n",
       "      <td>37.641367</td>\n",
       "      <td>79.764156</td>\n",
       "      <td>197.667019</td>\n",
       "      <td>35.078121</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>5.601735</td>\n",
       "      <td>22.16</td>\n",
       "      <td>43.436356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10046</td>\n",
       "      <td>51.220571</td>\n",
       "      <td>0.161463</td>\n",
       "      <td>31.088543</td>\n",
       "      <td>129.682221</td>\n",
       "      <td>121.111434</td>\n",
       "      <td>101.824794</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>6.384694</td>\n",
       "      <td>23.88</td>\n",
       "      <td>54.115498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10050</td>\n",
       "      <td>40.579963</td>\n",
       "      <td>0.099706</td>\n",
       "      <td>17.841340</td>\n",
       "      <td>169.902103</td>\n",
       "      <td>34.308938</td>\n",
       "      <td>129.007486</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>7.453341</td>\n",
       "      <td>24.01</td>\n",
       "      <td>43.348297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10052</td>\n",
       "      <td>40.383850</td>\n",
       "      <td>0.089285</td>\n",
       "      <td>6.235992</td>\n",
       "      <td>352.806055</td>\n",
       "      <td>51.359082</td>\n",
       "      <td>296.482493</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>7.307654</td>\n",
       "      <td>23.42</td>\n",
       "      <td>39.048943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10053</td>\n",
       "      <td>44.593431</td>\n",
       "      <td>0.123046</td>\n",
       "      <td>39.543000</td>\n",
       "      <td>304.682904</td>\n",
       "      <td>238.730520</td>\n",
       "      <td>139.081138</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>4.883148</td>\n",
       "      <td>21.95</td>\n",
       "      <td>48.992455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10057</td>\n",
       "      <td>40.351858</td>\n",
       "      <td>0.047286</td>\n",
       "      <td>28.679043</td>\n",
       "      <td>195.642050</td>\n",
       "      <td>187.531516</td>\n",
       "      <td>43.454090</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>7.803756</td>\n",
       "      <td>23.82</td>\n",
       "      <td>39.011669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10060</td>\n",
       "      <td>44.283489</td>\n",
       "      <td>0.124087</td>\n",
       "      <td>0.786626</td>\n",
       "      <td>308.322664</td>\n",
       "      <td>148.899488</td>\n",
       "      <td>222.891098</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>3.050556</td>\n",
       "      <td>20.07</td>\n",
       "      <td>48.585873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10066</td>\n",
       "      <td>40.305560</td>\n",
       "      <td>0.029177</td>\n",
       "      <td>49.540530</td>\n",
       "      <td>100.200296</td>\n",
       "      <td>85.346075</td>\n",
       "      <td>229.461941</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>7.055316</td>\n",
       "      <td>23.31</td>\n",
       "      <td>41.089176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10067</td>\n",
       "      <td>46.038718</td>\n",
       "      <td>0.047960</td>\n",
       "      <td>1.351583</td>\n",
       "      <td>355.538526</td>\n",
       "      <td>346.132659</td>\n",
       "      <td>330.061487</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>4.979211</td>\n",
       "      <td>21.59</td>\n",
       "      <td>44.153461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10073</td>\n",
       "      <td>48.896644</td>\n",
       "      <td>0.164960</td>\n",
       "      <td>50.141373</td>\n",
       "      <td>116.028197</td>\n",
       "      <td>209.658981</td>\n",
       "      <td>354.374812</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>3.559081</td>\n",
       "      <td>19.85</td>\n",
       "      <td>40.886307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10074</td>\n",
       "      <td>47.188831</td>\n",
       "      <td>0.223105</td>\n",
       "      <td>15.648236</td>\n",
       "      <td>103.616779</td>\n",
       "      <td>132.647065</td>\n",
       "      <td>51.242260</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>2.488574</td>\n",
       "      <td>18.92</td>\n",
       "      <td>42.342079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10088</td>\n",
       "      <td>42.324558</td>\n",
       "      <td>0.155270</td>\n",
       "      <td>59.544534</td>\n",
       "      <td>162.557189</td>\n",
       "      <td>345.375745</td>\n",
       "      <td>191.162623</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>3.890407</td>\n",
       "      <td>20.95</td>\n",
       "      <td>48.802959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10091</td>\n",
       "      <td>41.635839</td>\n",
       "      <td>0.081713</td>\n",
       "      <td>64.170315</td>\n",
       "      <td>216.579688</td>\n",
       "      <td>270.858189</td>\n",
       "      <td>340.474946</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>6.633939</td>\n",
       "      <td>22.63</td>\n",
       "      <td>38.464344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10099</td>\n",
       "      <td>50.898781</td>\n",
       "      <td>0.280716</td>\n",
       "      <td>6.407607</td>\n",
       "      <td>6.734515</td>\n",
       "      <td>185.240380</td>\n",
       "      <td>73.678854</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>5.452529</td>\n",
       "      <td>22.64</td>\n",
       "      <td>50.839375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10103</td>\n",
       "      <td>40.712225</td>\n",
       "      <td>0.004151</td>\n",
       "      <td>34.370100</td>\n",
       "      <td>123.453153</td>\n",
       "      <td>330.159341</td>\n",
       "      <td>307.923543</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>7.685916</td>\n",
       "      <td>23.92</td>\n",
       "      <td>40.608803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10109</td>\n",
       "      <td>59.038950</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>83.829887</td>\n",
       "      <td>352.093561</td>\n",
       "      <td>184.633508</td>\n",
       "      <td>145.756347</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>2.304629</td>\n",
       "      <td>21.12</td>\n",
       "      <td>73.885704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10115</td>\n",
       "      <td>40.460926</td>\n",
       "      <td>0.134130</td>\n",
       "      <td>67.874417</td>\n",
       "      <td>198.851935</td>\n",
       "      <td>11.891108</td>\n",
       "      <td>228.630404</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>6.485377</td>\n",
       "      <td>23.11</td>\n",
       "      <td>44.407066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10120</td>\n",
       "      <td>46.548478</td>\n",
       "      <td>0.075784</td>\n",
       "      <td>32.918228</td>\n",
       "      <td>13.378733</td>\n",
       "      <td>13.094969</td>\n",
       "      <td>310.658360</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>7.686451</td>\n",
       "      <td>24.35</td>\n",
       "      <td>44.415737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10121</td>\n",
       "      <td>56.449019</td>\n",
       "      <td>0.218270</td>\n",
       "      <td>62.516514</td>\n",
       "      <td>116.021985</td>\n",
       "      <td>50.999071</td>\n",
       "      <td>286.628642</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>5.999353</td>\n",
       "      <td>23.54</td>\n",
       "      <td>55.556335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10130</td>\n",
       "      <td>42.223888</td>\n",
       "      <td>0.038563</td>\n",
       "      <td>39.848275</td>\n",
       "      <td>1.494000</td>\n",
       "      <td>342.627707</td>\n",
       "      <td>357.774777</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>3.847350</td>\n",
       "      <td>20.13</td>\n",
       "      <td>40.596943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30090</th>\n",
       "      <td>199698</td>\n",
       "      <td>755.506381</td>\n",
       "      <td>0.087058</td>\n",
       "      <td>21.327895</td>\n",
       "      <td>99.864740</td>\n",
       "      <td>352.056401</td>\n",
       "      <td>239.117706</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-5.014901</td>\n",
       "      <td>24.00</td>\n",
       "      <td>793.203857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30091</th>\n",
       "      <td>199722</td>\n",
       "      <td>606.219261</td>\n",
       "      <td>0.578780</td>\n",
       "      <td>34.978563</td>\n",
       "      <td>70.137189</td>\n",
       "      <td>15.555075</td>\n",
       "      <td>345.351957</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-2.202041</td>\n",
       "      <td>22.76</td>\n",
       "      <td>310.081543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30092</th>\n",
       "      <td>199726</td>\n",
       "      <td>730.845537</td>\n",
       "      <td>0.138209</td>\n",
       "      <td>82.124621</td>\n",
       "      <td>178.744498</td>\n",
       "      <td>311.337616</td>\n",
       "      <td>280.524887</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-4.193813</td>\n",
       "      <td>24.45</td>\n",
       "      <td>726.238525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30093</th>\n",
       "      <td>199730</td>\n",
       "      <td>593.315181</td>\n",
       "      <td>0.580618</td>\n",
       "      <td>79.547543</td>\n",
       "      <td>292.381139</td>\n",
       "      <td>128.884903</td>\n",
       "      <td>278.834597</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-4.957049</td>\n",
       "      <td>23.65</td>\n",
       "      <td>722.410461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30094</th>\n",
       "      <td>199741</td>\n",
       "      <td>587.081124</td>\n",
       "      <td>0.736647</td>\n",
       "      <td>87.577932</td>\n",
       "      <td>86.674145</td>\n",
       "      <td>100.578292</td>\n",
       "      <td>274.168102</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-4.309824</td>\n",
       "      <td>24.82</td>\n",
       "      <td>814.527832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30095</th>\n",
       "      <td>199750</td>\n",
       "      <td>153.478572</td>\n",
       "      <td>0.098305</td>\n",
       "      <td>78.489855</td>\n",
       "      <td>127.732863</td>\n",
       "      <td>224.281951</td>\n",
       "      <td>18.314222</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>1.371355</td>\n",
       "      <td>22.90</td>\n",
       "      <td>139.324722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30096</th>\n",
       "      <td>199753</td>\n",
       "      <td>134.452806</td>\n",
       "      <td>0.389669</td>\n",
       "      <td>76.472737</td>\n",
       "      <td>109.662859</td>\n",
       "      <td>113.907289</td>\n",
       "      <td>198.286163</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>0.806425</td>\n",
       "      <td>23.55</td>\n",
       "      <td>185.462524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30097</th>\n",
       "      <td>199756</td>\n",
       "      <td>205.818696</td>\n",
       "      <td>0.029061</td>\n",
       "      <td>33.099065</td>\n",
       "      <td>180.764306</td>\n",
       "      <td>186.466931</td>\n",
       "      <td>16.549893</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-2.281428</td>\n",
       "      <td>20.80</td>\n",
       "      <td>200.099976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30098</th>\n",
       "      <td>199762</td>\n",
       "      <td>210.001785</td>\n",
       "      <td>0.179834</td>\n",
       "      <td>60.223958</td>\n",
       "      <td>86.969555</td>\n",
       "      <td>284.571758</td>\n",
       "      <td>10.089690</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-0.345414</td>\n",
       "      <td>22.11</td>\n",
       "      <td>173.100555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30099</th>\n",
       "      <td>199766</td>\n",
       "      <td>560.616677</td>\n",
       "      <td>0.402436</td>\n",
       "      <td>69.194299</td>\n",
       "      <td>98.147008</td>\n",
       "      <td>156.878903</td>\n",
       "      <td>24.826455</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-1.720267</td>\n",
       "      <td>24.19</td>\n",
       "      <td>386.502777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30100</th>\n",
       "      <td>199784</td>\n",
       "      <td>228.932656</td>\n",
       "      <td>0.448669</td>\n",
       "      <td>19.875009</td>\n",
       "      <td>33.065183</td>\n",
       "      <td>175.123812</td>\n",
       "      <td>66.329755</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-3.218449</td>\n",
       "      <td>20.51</td>\n",
       "      <td>232.553665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30101</th>\n",
       "      <td>199794</td>\n",
       "      <td>236.037700</td>\n",
       "      <td>0.142769</td>\n",
       "      <td>46.447682</td>\n",
       "      <td>120.976059</td>\n",
       "      <td>73.068155</td>\n",
       "      <td>168.104382</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-1.499272</td>\n",
       "      <td>22.86</td>\n",
       "      <td>269.181183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30102</th>\n",
       "      <td>199802</td>\n",
       "      <td>358.289850</td>\n",
       "      <td>0.420164</td>\n",
       "      <td>19.074757</td>\n",
       "      <td>180.637847</td>\n",
       "      <td>97.407677</td>\n",
       "      <td>113.039503</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-6.418015</td>\n",
       "      <td>20.21</td>\n",
       "      <td>457.375732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30103</th>\n",
       "      <td>199807</td>\n",
       "      <td>249.128722</td>\n",
       "      <td>0.083096</td>\n",
       "      <td>70.652506</td>\n",
       "      <td>249.868894</td>\n",
       "      <td>235.491603</td>\n",
       "      <td>356.911784</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-2.031564</td>\n",
       "      <td>21.60</td>\n",
       "      <td>228.462845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30104</th>\n",
       "      <td>199811</td>\n",
       "      <td>168.118502</td>\n",
       "      <td>0.046693</td>\n",
       "      <td>4.208393</td>\n",
       "      <td>305.877694</td>\n",
       "      <td>18.329837</td>\n",
       "      <td>24.041094</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-1.685970</td>\n",
       "      <td>20.46</td>\n",
       "      <td>161.014343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30105</th>\n",
       "      <td>199840</td>\n",
       "      <td>400.213442</td>\n",
       "      <td>0.186230</td>\n",
       "      <td>38.760988</td>\n",
       "      <td>359.740918</td>\n",
       "      <td>60.081088</td>\n",
       "      <td>305.351377</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-3.535834</td>\n",
       "      <td>22.17</td>\n",
       "      <td>367.845551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30106</th>\n",
       "      <td>199841</td>\n",
       "      <td>756.466369</td>\n",
       "      <td>0.520827</td>\n",
       "      <td>13.579187</td>\n",
       "      <td>178.543938</td>\n",
       "      <td>129.214697</td>\n",
       "      <td>16.987685</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-2.637117</td>\n",
       "      <td>23.72</td>\n",
       "      <td>427.644043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30107</th>\n",
       "      <td>199844</td>\n",
       "      <td>422.090076</td>\n",
       "      <td>0.745147</td>\n",
       "      <td>4.801499</td>\n",
       "      <td>14.639142</td>\n",
       "      <td>173.486670</td>\n",
       "      <td>156.674736</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-5.897340</td>\n",
       "      <td>22.76</td>\n",
       "      <td>728.023499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30108</th>\n",
       "      <td>199849</td>\n",
       "      <td>287.425739</td>\n",
       "      <td>0.151372</td>\n",
       "      <td>68.007319</td>\n",
       "      <td>213.497668</td>\n",
       "      <td>195.112141</td>\n",
       "      <td>39.150268</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-3.634985</td>\n",
       "      <td>20.51</td>\n",
       "      <td>256.831116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30109</th>\n",
       "      <td>199870</td>\n",
       "      <td>459.798393</td>\n",
       "      <td>0.074826</td>\n",
       "      <td>2.045908</td>\n",
       "      <td>284.749832</td>\n",
       "      <td>288.480596</td>\n",
       "      <td>107.792390</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-4.704188</td>\n",
       "      <td>22.08</td>\n",
       "      <td>472.560486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30110</th>\n",
       "      <td>199888</td>\n",
       "      <td>687.004799</td>\n",
       "      <td>0.302631</td>\n",
       "      <td>47.216642</td>\n",
       "      <td>331.607005</td>\n",
       "      <td>323.433645</td>\n",
       "      <td>5.392818</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-3.293396</td>\n",
       "      <td>23.56</td>\n",
       "      <td>480.981476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30111</th>\n",
       "      <td>199897</td>\n",
       "      <td>244.250400</td>\n",
       "      <td>0.480042</td>\n",
       "      <td>79.371959</td>\n",
       "      <td>122.203384</td>\n",
       "      <td>334.417541</td>\n",
       "      <td>209.936489</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-1.993197</td>\n",
       "      <td>23.53</td>\n",
       "      <td>354.172272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30112</th>\n",
       "      <td>199911</td>\n",
       "      <td>764.599984</td>\n",
       "      <td>0.400220</td>\n",
       "      <td>52.857432</td>\n",
       "      <td>189.039201</td>\n",
       "      <td>97.834577</td>\n",
       "      <td>39.990335</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-4.035828</td>\n",
       "      <td>23.86</td>\n",
       "      <td>610.722534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30113</th>\n",
       "      <td>199927</td>\n",
       "      <td>774.070088</td>\n",
       "      <td>0.621423</td>\n",
       "      <td>81.562195</td>\n",
       "      <td>189.733174</td>\n",
       "      <td>77.578937</td>\n",
       "      <td>210.568946</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-7.383020</td>\n",
       "      <td>23.53</td>\n",
       "      <td>1228.927856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30114</th>\n",
       "      <td>199930</td>\n",
       "      <td>681.553269</td>\n",
       "      <td>0.116526</td>\n",
       "      <td>10.973148</td>\n",
       "      <td>287.993572</td>\n",
       "      <td>228.262511</td>\n",
       "      <td>139.350951</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-4.020421</td>\n",
       "      <td>24.72</td>\n",
       "      <td>745.259949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30115</th>\n",
       "      <td>199934</td>\n",
       "      <td>236.045643</td>\n",
       "      <td>0.073277</td>\n",
       "      <td>61.408356</td>\n",
       "      <td>171.765148</td>\n",
       "      <td>8.724603</td>\n",
       "      <td>270.246570</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-0.205030</td>\n",
       "      <td>23.60</td>\n",
       "      <td>237.234833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30116</th>\n",
       "      <td>199947</td>\n",
       "      <td>314.354633</td>\n",
       "      <td>0.534003</td>\n",
       "      <td>64.467515</td>\n",
       "      <td>184.821246</td>\n",
       "      <td>101.651075</td>\n",
       "      <td>57.659578</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-1.315186</td>\n",
       "      <td>23.64</td>\n",
       "      <td>309.201813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30117</th>\n",
       "      <td>199949</td>\n",
       "      <td>478.541768</td>\n",
       "      <td>0.490235</td>\n",
       "      <td>73.062817</td>\n",
       "      <td>50.295914</td>\n",
       "      <td>97.700752</td>\n",
       "      <td>237.920205</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-3.597114</td>\n",
       "      <td>24.62</td>\n",
       "      <td>658.512329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30118</th>\n",
       "      <td>199989</td>\n",
       "      <td>374.944172</td>\n",
       "      <td>0.327641</td>\n",
       "      <td>2.211318</td>\n",
       "      <td>14.143906</td>\n",
       "      <td>65.972687</td>\n",
       "      <td>270.135364</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>-5.081730</td>\n",
       "      <td>21.11</td>\n",
       "      <td>412.384308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30119</th>\n",
       "      <td>199997</td>\n",
       "      <td>464.951320</td>\n",
       "      <td>0.497108</td>\n",
       "      <td>76.815628</td>\n",
       "      <td>315.954045</td>\n",
       "      <td>6.535270</td>\n",
       "      <td>337.506024</td>\n",
       "      <td>2456658.5</td>\n",
       "      <td>0.181808</td>\n",
       "      <td>24.88</td>\n",
       "      <td>291.278687</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30120 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       fakeid           a         e          i         lan         aop           M   epoch JD         H    mag     sun_dist\n",
       "0       10005   54.603174  0.030831  70.951523  196.002411  193.558284    3.089665  2456658.5  7.410195  24.80    52.922298\n",
       "1       10009   45.952954  0.009800  18.692049  108.402264  121.216072   74.650853  2456658.5  5.885719  22.64    45.837868\n",
       "2       10010   41.275114  0.128412  68.334839   17.053595  151.336680  203.948441  2456658.5  7.199051  24.02    46.214043\n",
       "3       10011   46.164918  0.057041  11.421039   26.332649   33.323566  246.075320  2456658.5  3.396686  20.29    47.353920\n",
       "4       10012   40.128168  0.010586  84.096966  128.176545   42.591766  117.896197  2456658.5  2.245534  18.47    40.330406\n",
       "5       10022   45.237742  0.089611  74.035231  241.186446   94.726200  130.603474  2456658.5  7.903337  24.84    48.067886\n",
       "6       10024   43.121750  0.168840  20.662362  276.393452  353.776064   45.734849  2456658.5  6.884150  22.97    38.792221\n",
       "7       10028   54.020608  0.243808   9.050033   99.961688  322.487042  253.955759  2456658.5  2.821032  20.72    60.272758\n",
       "8       10029   50.277402  0.151148  67.737131  106.252410  303.505116  315.638477  2456658.5  6.883018  23.65    45.505531\n",
       "9       10034   52.980594  0.149964  46.852903   49.159801   54.750601  257.574750  2456658.5  6.129218  23.75    55.759872\n",
       "10      10038   46.689398  0.088686  37.641367   79.764156  197.667019   35.078121  2456658.5  5.601735  22.16    43.436356\n",
       "11      10046   51.220571  0.161463  31.088543  129.682221  121.111434  101.824794  2456658.5  6.384694  23.88    54.115498\n",
       "12      10050   40.579963  0.099706  17.841340  169.902103   34.308938  129.007486  2456658.5  7.453341  24.01    43.348297\n",
       "13      10052   40.383850  0.089285   6.235992  352.806055   51.359082  296.482493  2456658.5  7.307654  23.42    39.048943\n",
       "14      10053   44.593431  0.123046  39.543000  304.682904  238.730520  139.081138  2456658.5  4.883148  21.95    48.992455\n",
       "15      10057   40.351858  0.047286  28.679043  195.642050  187.531516   43.454090  2456658.5  7.803756  23.82    39.011669\n",
       "16      10060   44.283489  0.124087   0.786626  308.322664  148.899488  222.891098  2456658.5  3.050556  20.07    48.585873\n",
       "17      10066   40.305560  0.029177  49.540530  100.200296   85.346075  229.461941  2456658.5  7.055316  23.31    41.089176\n",
       "18      10067   46.038718  0.047960   1.351583  355.538526  346.132659  330.061487  2456658.5  4.979211  21.59    44.153461\n",
       "19      10073   48.896644  0.164960  50.141373  116.028197  209.658981  354.374812  2456658.5  3.559081  19.85    40.886307\n",
       "20      10074   47.188831  0.223105  15.648236  103.616779  132.647065   51.242260  2456658.5  2.488574  18.92    42.342079\n",
       "21      10088   42.324558  0.155270  59.544534  162.557189  345.375745  191.162623  2456658.5  3.890407  20.95    48.802959\n",
       "22      10091   41.635839  0.081713  64.170315  216.579688  270.858189  340.474946  2456658.5  6.633939  22.63    38.464344\n",
       "23      10099   50.898781  0.280716   6.407607    6.734515  185.240380   73.678854  2456658.5  5.452529  22.64    50.839375\n",
       "24      10103   40.712225  0.004151  34.370100  123.453153  330.159341  307.923543  2456658.5  7.685916  23.92    40.608803\n",
       "25      10109   59.038950  0.282051  83.829887  352.093561  184.633508  145.756347  2456658.5  2.304629  21.12    73.885704\n",
       "26      10115   40.460926  0.134130  67.874417  198.851935   11.891108  228.630404  2456658.5  6.485377  23.11    44.407066\n",
       "27      10120   46.548478  0.075784  32.918228   13.378733   13.094969  310.658360  2456658.5  7.686451  24.35    44.415737\n",
       "28      10121   56.449019  0.218270  62.516514  116.021985   50.999071  286.628642  2456658.5  5.999353  23.54    55.556335\n",
       "29      10130   42.223888  0.038563  39.848275    1.494000  342.627707  357.774777  2456658.5  3.847350  20.13    40.596943\n",
       "...       ...         ...       ...        ...         ...         ...         ...        ...       ...    ...          ...\n",
       "30090  199698  755.506381  0.087058  21.327895   99.864740  352.056401  239.117706  2456658.5 -5.014901  24.00   793.203857\n",
       "30091  199722  606.219261  0.578780  34.978563   70.137189   15.555075  345.351957  2456658.5 -2.202041  22.76   310.081543\n",
       "30092  199726  730.845537  0.138209  82.124621  178.744498  311.337616  280.524887  2456658.5 -4.193813  24.45   726.238525\n",
       "30093  199730  593.315181  0.580618  79.547543  292.381139  128.884903  278.834597  2456658.5 -4.957049  23.65   722.410461\n",
       "30094  199741  587.081124  0.736647  87.577932   86.674145  100.578292  274.168102  2456658.5 -4.309824  24.82   814.527832\n",
       "30095  199750  153.478572  0.098305  78.489855  127.732863  224.281951   18.314222  2456658.5  1.371355  22.90   139.324722\n",
       "30096  199753  134.452806  0.389669  76.472737  109.662859  113.907289  198.286163  2456658.5  0.806425  23.55   185.462524\n",
       "30097  199756  205.818696  0.029061  33.099065  180.764306  186.466931   16.549893  2456658.5 -2.281428  20.80   200.099976\n",
       "30098  199762  210.001785  0.179834  60.223958   86.969555  284.571758   10.089690  2456658.5 -0.345414  22.11   173.100555\n",
       "30099  199766  560.616677  0.402436  69.194299   98.147008  156.878903   24.826455  2456658.5 -1.720267  24.19   386.502777\n",
       "30100  199784  228.932656  0.448669  19.875009   33.065183  175.123812   66.329755  2456658.5 -3.218449  20.51   232.553665\n",
       "30101  199794  236.037700  0.142769  46.447682  120.976059   73.068155  168.104382  2456658.5 -1.499272  22.86   269.181183\n",
       "30102  199802  358.289850  0.420164  19.074757  180.637847   97.407677  113.039503  2456658.5 -6.418015  20.21   457.375732\n",
       "30103  199807  249.128722  0.083096  70.652506  249.868894  235.491603  356.911784  2456658.5 -2.031564  21.60   228.462845\n",
       "30104  199811  168.118502  0.046693   4.208393  305.877694   18.329837   24.041094  2456658.5 -1.685970  20.46   161.014343\n",
       "30105  199840  400.213442  0.186230  38.760988  359.740918   60.081088  305.351377  2456658.5 -3.535834  22.17   367.845551\n",
       "30106  199841  756.466369  0.520827  13.579187  178.543938  129.214697   16.987685  2456658.5 -2.637117  23.72   427.644043\n",
       "30107  199844  422.090076  0.745147   4.801499   14.639142  173.486670  156.674736  2456658.5 -5.897340  22.76   728.023499\n",
       "30108  199849  287.425739  0.151372  68.007319  213.497668  195.112141   39.150268  2456658.5 -3.634985  20.51   256.831116\n",
       "30109  199870  459.798393  0.074826   2.045908  284.749832  288.480596  107.792390  2456658.5 -4.704188  22.08   472.560486\n",
       "30110  199888  687.004799  0.302631  47.216642  331.607005  323.433645    5.392818  2456658.5 -3.293396  23.56   480.981476\n",
       "30111  199897  244.250400  0.480042  79.371959  122.203384  334.417541  209.936489  2456658.5 -1.993197  23.53   354.172272\n",
       "30112  199911  764.599984  0.400220  52.857432  189.039201   97.834577   39.990335  2456658.5 -4.035828  23.86   610.722534\n",
       "30113  199927  774.070088  0.621423  81.562195  189.733174   77.578937  210.568946  2456658.5 -7.383020  23.53  1228.927856\n",
       "30114  199930  681.553269  0.116526  10.973148  287.993572  228.262511  139.350951  2456658.5 -4.020421  24.72   745.259949\n",
       "30115  199934  236.045643  0.073277  61.408356  171.765148    8.724603  270.246570  2456658.5 -0.205030  23.60   237.234833\n",
       "30116  199947  314.354633  0.534003  64.467515  184.821246  101.651075   57.659578  2456658.5 -1.315186  23.64   309.201813\n",
       "30117  199949  478.541768  0.490235  73.062817   50.295914   97.700752  237.920205  2456658.5 -3.597114  24.62   658.512329\n",
       "30118  199989  374.944172  0.327641   2.211318   14.143906   65.972687  270.135364  2456658.5 -5.081730  21.11   412.384308\n",
       "30119  199997  464.951320  0.497108  76.815628  315.954045    6.535270  337.506024  2456658.5  0.181808  24.88   291.278687\n",
       "\n",
       "[30120 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orbits_df = pd.read_csv('TNOfakes_all_SVA1_Y1Y2Y3_gen.csv')\n",
    "orbits_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11c08bac8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDMMALCAYhmEYCRYQDMMwDAD+P/ypwd4td4jMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11be315f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(df['RADEG'], df['DECDEG'])\n",
    "plt.xlabel('RA (deg)')\n",
    "plt.ylabel('Dec (deg)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the JD and equatorial barycentric vectors to the observatory to the exposure data frame\n",
    "df['JD_OBS'] = df['MJD_OBS'] + 2400000.5\n",
    "jd_utc = df['JD_OBS']\n",
    "foo = np.array([Observatories.getObservatoryPosition('W84', jd) for jd in jd_utc])\n",
    "df['xe'] = foo[:,0]\n",
    "df['ye'] = foo[:,1]\n",
    "df['ze'] = foo[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raDec_to_xyz(raDec):\n",
    "    raDeg  = raDec[:,0]\n",
    "    decDeg = raDec[:,1]\n",
    "\n",
    "    x = np.cos(decDeg*np.pi/180.)*np.cos(raDeg*np.pi/180.)\n",
    "    y = np.cos(decDeg*np.pi/180.)*np.sin(raDeg*np.pi/180.)  \n",
    "    z = np.sin(decDeg*np.pi/180.)\n",
    "    \n",
    "    return np.stack((x, y, z), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the equatorial unit vectors to the exposure centers\n",
    "raDec = df[['RADEG', 'DECDEG']].as_matrix()\n",
    "\n",
    "xyz = raDec_to_xyz(raDec)\n",
    "df['xc'] = xyz[:,0]\n",
    "df['yc'] = xyz[:,1]\n",
    "df['zc'] = xyz[:,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EXPNUM</th>\n",
       "      <th>RADEG</th>\n",
       "      <th>DECDEG</th>\n",
       "      <th>DATE_OBS</th>\n",
       "      <th>BAND</th>\n",
       "      <th>EXPTIME</th>\n",
       "      <th>MJD_OBS</th>\n",
       "      <th>JD_OBS</th>\n",
       "      <th>xe</th>\n",
       "      <th>ye</th>\n",
       "      <th>ze</th>\n",
       "      <th>xc</th>\n",
       "      <th>yc</th>\n",
       "      <th>zc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>229683</td>\n",
       "      <td>0.468621</td>\n",
       "      <td>0.377768</td>\n",
       "      <td>2013-09-02T06:10:47.837043</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.257498</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.946924</td>\n",
       "      <td>-0.322540</td>\n",
       "      <td>-0.139930</td>\n",
       "      <td>0.999945</td>\n",
       "      <td>0.008179</td>\n",
       "      <td>0.006593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>229684</td>\n",
       "      <td>2.065533</td>\n",
       "      <td>1.392268</td>\n",
       "      <td>2013-09-02T06:12:46.472481</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.258871</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.946931</td>\n",
       "      <td>-0.322519</td>\n",
       "      <td>-0.139921</td>\n",
       "      <td>0.999055</td>\n",
       "      <td>0.036032</td>\n",
       "      <td>0.024297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>229685</td>\n",
       "      <td>3.659758</td>\n",
       "      <td>2.406768</td>\n",
       "      <td>2013-09-02T06:15:01.886059</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.260439</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.946940</td>\n",
       "      <td>-0.322496</td>\n",
       "      <td>-0.139911</td>\n",
       "      <td>0.997080</td>\n",
       "      <td>0.063775</td>\n",
       "      <td>0.041994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>229688</td>\n",
       "      <td>3.663221</td>\n",
       "      <td>0.377768</td>\n",
       "      <td>2013-09-02T06:21:30.678404</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.264938</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.946966</td>\n",
       "      <td>-0.322428</td>\n",
       "      <td>-0.139882</td>\n",
       "      <td>0.997935</td>\n",
       "      <td>0.063890</td>\n",
       "      <td>0.006593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>229691</td>\n",
       "      <td>0.457838</td>\n",
       "      <td>-1.651232</td>\n",
       "      <td>2013-09-02T06:27:54.655251</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.269383</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.946991</td>\n",
       "      <td>-0.322362</td>\n",
       "      <td>-0.139854</td>\n",
       "      <td>0.999553</td>\n",
       "      <td>0.007987</td>\n",
       "      <td>-0.028815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>229692</td>\n",
       "      <td>3.655137</td>\n",
       "      <td>-1.651232</td>\n",
       "      <td>2013-09-02T06:30:06.609589</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.270910</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.946999</td>\n",
       "      <td>-0.322339</td>\n",
       "      <td>-0.139844</td>\n",
       "      <td>0.997551</td>\n",
       "      <td>0.063724</td>\n",
       "      <td>-0.028815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>229693</td>\n",
       "      <td>5.258321</td>\n",
       "      <td>-0.636732</td>\n",
       "      <td>2013-09-02T06:32:06.124497</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.272293</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947007</td>\n",
       "      <td>-0.322318</td>\n",
       "      <td>-0.139835</td>\n",
       "      <td>0.995730</td>\n",
       "      <td>0.091641</td>\n",
       "      <td>-0.011113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>229694</td>\n",
       "      <td>6.857721</td>\n",
       "      <td>0.377768</td>\n",
       "      <td>2013-09-02T06:34:05.969268</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.273680</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947015</td>\n",
       "      <td>-0.322297</td>\n",
       "      <td>-0.139826</td>\n",
       "      <td>0.992824</td>\n",
       "      <td>0.119402</td>\n",
       "      <td>0.006593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>229695</td>\n",
       "      <td>8.454933</td>\n",
       "      <td>1.392268</td>\n",
       "      <td>2013-09-02T06:36:08.302435</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.275096</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947023</td>\n",
       "      <td>-0.322276</td>\n",
       "      <td>-0.139817</td>\n",
       "      <td>0.988840</td>\n",
       "      <td>0.146988</td>\n",
       "      <td>0.024297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>229696</td>\n",
       "      <td>10.051358</td>\n",
       "      <td>2.406768</td>\n",
       "      <td>2013-09-02T06:38:20.162801</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.276622</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947032</td>\n",
       "      <td>-0.322253</td>\n",
       "      <td>-0.139807</td>\n",
       "      <td>0.983783</td>\n",
       "      <td>0.174377</td>\n",
       "      <td>0.041994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>229697</td>\n",
       "      <td>2.046671</td>\n",
       "      <td>-2.665632</td>\n",
       "      <td>2013-09-02T06:40:45.869068</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.278309</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947041</td>\n",
       "      <td>-0.322228</td>\n",
       "      <td>-0.139796</td>\n",
       "      <td>0.998281</td>\n",
       "      <td>0.035675</td>\n",
       "      <td>-0.046507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>229698</td>\n",
       "      <td>5.246971</td>\n",
       "      <td>-2.665632</td>\n",
       "      <td>2013-09-02T06:42:57.631171</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.279834</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947050</td>\n",
       "      <td>-0.322205</td>\n",
       "      <td>-0.139787</td>\n",
       "      <td>0.994732</td>\n",
       "      <td>0.091350</td>\n",
       "      <td>-0.046507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>229699</td>\n",
       "      <td>0.431417</td>\n",
       "      <td>-3.680132</td>\n",
       "      <td>2013-09-02T06:45:02.443958</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.281278</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947058</td>\n",
       "      <td>-0.322184</td>\n",
       "      <td>-0.139777</td>\n",
       "      <td>0.997910</td>\n",
       "      <td>0.007514</td>\n",
       "      <td>-0.064186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>229700</td>\n",
       "      <td>6.839817</td>\n",
       "      <td>-3.680132</td>\n",
       "      <td>2013-09-02T06:47:16.871230</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.282834</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947067</td>\n",
       "      <td>-0.322160</td>\n",
       "      <td>-0.139768</td>\n",
       "      <td>0.990836</td>\n",
       "      <td>0.118848</td>\n",
       "      <td>-0.064186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>229701</td>\n",
       "      <td>3.635617</td>\n",
       "      <td>-3.680132</td>\n",
       "      <td>2013-09-02T06:49:35.049841</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.284433</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947076</td>\n",
       "      <td>-0.322136</td>\n",
       "      <td>-0.139757</td>\n",
       "      <td>0.995930</td>\n",
       "      <td>0.063280</td>\n",
       "      <td>-0.064186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>229702</td>\n",
       "      <td>6.852538</td>\n",
       "      <td>-1.651232</td>\n",
       "      <td>2013-09-02T06:51:37.853865</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.285855</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947084</td>\n",
       "      <td>-0.322115</td>\n",
       "      <td>-0.139748</td>\n",
       "      <td>0.992444</td>\n",
       "      <td>0.119265</td>\n",
       "      <td>-0.028815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>229703</td>\n",
       "      <td>10.049938</td>\n",
       "      <td>-1.651232</td>\n",
       "      <td>2013-09-02T06:53:48.025210</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.287361</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947092</td>\n",
       "      <td>-0.322092</td>\n",
       "      <td>-0.139739</td>\n",
       "      <td>0.984247</td>\n",
       "      <td>0.174434</td>\n",
       "      <td>-0.028815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>229716</td>\n",
       "      <td>2.016979</td>\n",
       "      <td>-4.694632</td>\n",
       "      <td>2013-09-02T08:06:15.993847</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.337685</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947373</td>\n",
       "      <td>-0.321339</td>\n",
       "      <td>-0.139417</td>\n",
       "      <td>0.996028</td>\n",
       "      <td>0.035078</td>\n",
       "      <td>-0.081845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>229717</td>\n",
       "      <td>5.226079</td>\n",
       "      <td>-4.694632</td>\n",
       "      <td>2013-09-02T08:08:27.501084</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.339207</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947381</td>\n",
       "      <td>-0.321316</td>\n",
       "      <td>-0.139407</td>\n",
       "      <td>0.992502</td>\n",
       "      <td>0.090780</td>\n",
       "      <td>-0.081845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>229718</td>\n",
       "      <td>3.604454</td>\n",
       "      <td>-5.709132</td>\n",
       "      <td>2013-09-02T08:10:25.842980</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.340577</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947389</td>\n",
       "      <td>-0.321296</td>\n",
       "      <td>-0.139398</td>\n",
       "      <td>0.993071</td>\n",
       "      <td>0.062556</td>\n",
       "      <td>-0.099478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>229719</td>\n",
       "      <td>0.389354</td>\n",
       "      <td>-5.709132</td>\n",
       "      <td>2013-09-02T08:12:26.816230</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.341977</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947397</td>\n",
       "      <td>-0.321275</td>\n",
       "      <td>-0.139389</td>\n",
       "      <td>0.995017</td>\n",
       "      <td>0.006762</td>\n",
       "      <td>-0.099478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>229720</td>\n",
       "      <td>1.973450</td>\n",
       "      <td>-6.723632</td>\n",
       "      <td>2013-09-02T08:14:26.176951</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.343359</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947404</td>\n",
       "      <td>-0.321254</td>\n",
       "      <td>-0.139380</td>\n",
       "      <td>0.992533</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>-0.117080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>229721</td>\n",
       "      <td>5.195450</td>\n",
       "      <td>-6.723632</td>\n",
       "      <td>2013-09-02T08:16:28.137979</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.344770</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947412</td>\n",
       "      <td>-0.321233</td>\n",
       "      <td>-0.139371</td>\n",
       "      <td>0.989042</td>\n",
       "      <td>0.089931</td>\n",
       "      <td>-0.117080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>229722</td>\n",
       "      <td>6.819554</td>\n",
       "      <td>-5.709132</td>\n",
       "      <td>2013-09-02T08:18:26.944471</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.346145</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947420</td>\n",
       "      <td>-0.321212</td>\n",
       "      <td>-0.139362</td>\n",
       "      <td>0.988000</td>\n",
       "      <td>0.118154</td>\n",
       "      <td>-0.099478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>229723</td>\n",
       "      <td>8.435179</td>\n",
       "      <td>-4.694632</td>\n",
       "      <td>2013-09-02T08:20:25.216666</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.347514</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947427</td>\n",
       "      <td>-0.321192</td>\n",
       "      <td>-0.139354</td>\n",
       "      <td>0.985864</td>\n",
       "      <td>0.146198</td>\n",
       "      <td>-0.081845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>229724</td>\n",
       "      <td>8.447271</td>\n",
       "      <td>-2.665632</td>\n",
       "      <td>2013-09-02T08:22:25.719124</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.348909</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947435</td>\n",
       "      <td>-0.321171</td>\n",
       "      <td>-0.139345</td>\n",
       "      <td>0.988081</td>\n",
       "      <td>0.146740</td>\n",
       "      <td>-0.046507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>229725</td>\n",
       "      <td>11.647571</td>\n",
       "      <td>-2.665632</td>\n",
       "      <td>2013-09-02T08:24:33.295422</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.350385</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947443</td>\n",
       "      <td>-0.321149</td>\n",
       "      <td>-0.139335</td>\n",
       "      <td>0.978348</td>\n",
       "      <td>0.201673</td>\n",
       "      <td>-0.046507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>229726</td>\n",
       "      <td>8.453821</td>\n",
       "      <td>-0.636732</td>\n",
       "      <td>2013-09-02T08:26:35.074324</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.351795</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947451</td>\n",
       "      <td>-0.321128</td>\n",
       "      <td>-0.139326</td>\n",
       "      <td>0.989074</td>\n",
       "      <td>0.147003</td>\n",
       "      <td>-0.011113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>229727</td>\n",
       "      <td>10.052321</td>\n",
       "      <td>0.377768</td>\n",
       "      <td>2013-09-02T08:28:33.629931</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.353167</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947458</td>\n",
       "      <td>-0.321107</td>\n",
       "      <td>-0.139317</td>\n",
       "      <td>0.984627</td>\n",
       "      <td>0.174544</td>\n",
       "      <td>0.006593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>229728</td>\n",
       "      <td>11.649633</td>\n",
       "      <td>1.392268</td>\n",
       "      <td>2013-09-02T08:30:31.974521</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>56537.354537</td>\n",
       "      <td>2.456538e+06</td>\n",
       "      <td>0.947466</td>\n",
       "      <td>-0.321087</td>\n",
       "      <td>-0.139309</td>\n",
       "      <td>0.979112</td>\n",
       "      <td>0.201867</td>\n",
       "      <td>0.024297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>599349</td>\n",
       "      <td>6.142708</td>\n",
       "      <td>-3.037608</td>\n",
       "      <td>2016-12-04T02:29:22.832617</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57726.103737</td>\n",
       "      <td>2.457727e+06</td>\n",
       "      <td>0.305744</td>\n",
       "      <td>0.864230</td>\n",
       "      <td>0.374445</td>\n",
       "      <td>0.992861</td>\n",
       "      <td>0.106855</td>\n",
       "      <td>-0.052991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>599351</td>\n",
       "      <td>6.152925</td>\n",
       "      <td>-1.008708</td>\n",
       "      <td>2016-12-04T02:33:19.111757</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57726.106471</td>\n",
       "      <td>2.457727e+06</td>\n",
       "      <td>0.305698</td>\n",
       "      <td>0.864244</td>\n",
       "      <td>0.374450</td>\n",
       "      <td>0.994085</td>\n",
       "      <td>0.107166</td>\n",
       "      <td>-0.017604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>599697</td>\n",
       "      <td>3.148275</td>\n",
       "      <td>2.119332</td>\n",
       "      <td>2016-12-05T01:23:12.695404</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.057786</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289810</td>\n",
       "      <td>0.868665</td>\n",
       "      <td>0.376371</td>\n",
       "      <td>0.997808</td>\n",
       "      <td>0.054883</td>\n",
       "      <td>0.036981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>599699</td>\n",
       "      <td>3.132283</td>\n",
       "      <td>4.148232</td>\n",
       "      <td>2016-12-05T01:26:24.625545</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.060007</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289773</td>\n",
       "      <td>0.868676</td>\n",
       "      <td>0.376376</td>\n",
       "      <td>0.995890</td>\n",
       "      <td>0.054498</td>\n",
       "      <td>0.072337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>599700</td>\n",
       "      <td>3.132283</td>\n",
       "      <td>4.148232</td>\n",
       "      <td>2016-12-05T01:28:24.022244</td>\n",
       "      <td>i</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.061389</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289749</td>\n",
       "      <td>0.868682</td>\n",
       "      <td>0.376379</td>\n",
       "      <td>0.995890</td>\n",
       "      <td>0.054498</td>\n",
       "      <td>0.072337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>599707</td>\n",
       "      <td>2.957521</td>\n",
       "      <td>-1.008708</td>\n",
       "      <td>2016-12-05T01:41:48.338083</td>\n",
       "      <td>r</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.070698</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289592</td>\n",
       "      <td>0.868726</td>\n",
       "      <td>0.376397</td>\n",
       "      <td>0.998513</td>\n",
       "      <td>0.051588</td>\n",
       "      <td>-0.017604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>599708</td>\n",
       "      <td>1.554317</td>\n",
       "      <td>1.104832</td>\n",
       "      <td>2016-12-05T01:43:45.865633</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.072059</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289569</td>\n",
       "      <td>0.868733</td>\n",
       "      <td>0.376399</td>\n",
       "      <td>0.999446</td>\n",
       "      <td>0.027120</td>\n",
       "      <td>0.019282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>599710</td>\n",
       "      <td>1.542900</td>\n",
       "      <td>3.133732</td>\n",
       "      <td>2016-12-05T01:46:56.902576</td>\n",
       "      <td>i</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.074270</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289532</td>\n",
       "      <td>0.868743</td>\n",
       "      <td>0.376404</td>\n",
       "      <td>0.998143</td>\n",
       "      <td>0.026885</td>\n",
       "      <td>0.054667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>599711</td>\n",
       "      <td>1.542900</td>\n",
       "      <td>3.133732</td>\n",
       "      <td>2016-12-05T01:48:55.348865</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.075641</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289509</td>\n",
       "      <td>0.868749</td>\n",
       "      <td>0.376406</td>\n",
       "      <td>0.998143</td>\n",
       "      <td>0.026885</td>\n",
       "      <td>0.054667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>599712</td>\n",
       "      <td>1.542900</td>\n",
       "      <td>3.133732</td>\n",
       "      <td>2016-12-05T01:50:53.846887</td>\n",
       "      <td>r</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.077012</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289486</td>\n",
       "      <td>0.868756</td>\n",
       "      <td>0.376409</td>\n",
       "      <td>0.998143</td>\n",
       "      <td>0.026885</td>\n",
       "      <td>0.054667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>599714</td>\n",
       "      <td>3.132283</td>\n",
       "      <td>4.148232</td>\n",
       "      <td>2016-12-05T01:54:07.163708</td>\n",
       "      <td>r</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.079250</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289448</td>\n",
       "      <td>0.868767</td>\n",
       "      <td>0.376414</td>\n",
       "      <td>0.995890</td>\n",
       "      <td>0.054498</td>\n",
       "      <td>0.072337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>599715</td>\n",
       "      <td>6.333375</td>\n",
       "      <td>4.148232</td>\n",
       "      <td>2016-12-05T01:56:06.716377</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.080633</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289425</td>\n",
       "      <td>0.868773</td>\n",
       "      <td>0.376416</td>\n",
       "      <td>0.991293</td>\n",
       "      <td>0.110024</td>\n",
       "      <td>0.072337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>599718</td>\n",
       "      <td>1.363079</td>\n",
       "      <td>0.005792</td>\n",
       "      <td>2016-12-05T02:01:24.124961</td>\n",
       "      <td>r</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.084307</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289363</td>\n",
       "      <td>0.868790</td>\n",
       "      <td>0.376423</td>\n",
       "      <td>0.999717</td>\n",
       "      <td>0.023788</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>599719</td>\n",
       "      <td>2.960129</td>\n",
       "      <td>1.020292</td>\n",
       "      <td>2016-12-05T02:03:21.991212</td>\n",
       "      <td>r</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.085671</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289340</td>\n",
       "      <td>0.868797</td>\n",
       "      <td>0.376426</td>\n",
       "      <td>0.998507</td>\n",
       "      <td>0.051633</td>\n",
       "      <td>0.017807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>599720</td>\n",
       "      <td>2.960129</td>\n",
       "      <td>1.020292</td>\n",
       "      <td>2016-12-05T02:05:22.311540</td>\n",
       "      <td>i</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.087064</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289316</td>\n",
       "      <td>0.868803</td>\n",
       "      <td>0.376429</td>\n",
       "      <td>0.998507</td>\n",
       "      <td>0.051633</td>\n",
       "      <td>0.017807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>599721</td>\n",
       "      <td>4.554767</td>\n",
       "      <td>2.034792</td>\n",
       "      <td>2016-12-05T02:07:20.795405</td>\n",
       "      <td>r</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.088435</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289293</td>\n",
       "      <td>0.868810</td>\n",
       "      <td>0.376432</td>\n",
       "      <td>0.996213</td>\n",
       "      <td>0.079362</td>\n",
       "      <td>0.035506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>599722</td>\n",
       "      <td>6.154829</td>\n",
       "      <td>1.020292</td>\n",
       "      <td>2016-12-05T02:09:19.565549</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.089810</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289270</td>\n",
       "      <td>0.868816</td>\n",
       "      <td>0.376434</td>\n",
       "      <td>0.994078</td>\n",
       "      <td>0.107199</td>\n",
       "      <td>0.017807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>599724</td>\n",
       "      <td>7.750575</td>\n",
       "      <td>2.034792</td>\n",
       "      <td>2016-12-05T02:13:15.604826</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.092542</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289224</td>\n",
       "      <td>0.868829</td>\n",
       "      <td>0.376440</td>\n",
       "      <td>0.990240</td>\n",
       "      <td>0.134776</td>\n",
       "      <td>0.035506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>599726</td>\n",
       "      <td>4.557675</td>\n",
       "      <td>0.005792</td>\n",
       "      <td>2016-12-05T02:17:15.486513</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.095318</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289177</td>\n",
       "      <td>0.868842</td>\n",
       "      <td>0.376445</td>\n",
       "      <td>0.996838</td>\n",
       "      <td>0.079463</td>\n",
       "      <td>0.000101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>599728</td>\n",
       "      <td>4.550158</td>\n",
       "      <td>-2.023208</td>\n",
       "      <td>2016-12-05T02:21:12.132313</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.098057</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289131</td>\n",
       "      <td>0.868855</td>\n",
       "      <td>0.376450</td>\n",
       "      <td>0.996227</td>\n",
       "      <td>0.079282</td>\n",
       "      <td>-0.035304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>599730</td>\n",
       "      <td>4.532342</td>\n",
       "      <td>-4.052108</td>\n",
       "      <td>2016-12-05T02:25:09.216850</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57727.100801</td>\n",
       "      <td>2.457728e+06</td>\n",
       "      <td>0.289084</td>\n",
       "      <td>0.868868</td>\n",
       "      <td>0.376456</td>\n",
       "      <td>0.994381</td>\n",
       "      <td>0.078824</td>\n",
       "      <td>-0.070664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>600111</td>\n",
       "      <td>2.914979</td>\n",
       "      <td>-5.066608</td>\n",
       "      <td>2016-12-06T02:29:36.325825</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57728.103893</td>\n",
       "      <td>2.457729e+06</td>\n",
       "      <td>0.272235</td>\n",
       "      <td>0.873265</td>\n",
       "      <td>0.378362</td>\n",
       "      <td>0.994804</td>\n",
       "      <td>0.050655</td>\n",
       "      <td>-0.088314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>602137</td>\n",
       "      <td>1.328129</td>\n",
       "      <td>-4.052108</td>\n",
       "      <td>2016-12-19T01:22:40.980411</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57741.057419</td>\n",
       "      <td>2.457742e+06</td>\n",
       "      <td>0.049153</td>\n",
       "      <td>0.905345</td>\n",
       "      <td>0.392273</td>\n",
       "      <td>0.997232</td>\n",
       "      <td>0.023120</td>\n",
       "      <td>-0.070664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>602138</td>\n",
       "      <td>2.957521</td>\n",
       "      <td>-1.008708</td>\n",
       "      <td>2016-12-19T01:24:40.623946</td>\n",
       "      <td>i</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57741.058803</td>\n",
       "      <td>2.457742e+06</td>\n",
       "      <td>0.049129</td>\n",
       "      <td>0.905347</td>\n",
       "      <td>0.392273</td>\n",
       "      <td>0.998513</td>\n",
       "      <td>0.051588</td>\n",
       "      <td>-0.017604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>602139</td>\n",
       "      <td>2.942400</td>\n",
       "      <td>-3.037608</td>\n",
       "      <td>2016-12-19T01:26:39.578978</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57741.060180</td>\n",
       "      <td>2.457742e+06</td>\n",
       "      <td>0.049104</td>\n",
       "      <td>0.905348</td>\n",
       "      <td>0.392274</td>\n",
       "      <td>0.997278</td>\n",
       "      <td>0.051260</td>\n",
       "      <td>-0.052991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>602140</td>\n",
       "      <td>1.328129</td>\n",
       "      <td>-4.052108</td>\n",
       "      <td>2016-12-19T01:28:37.923112</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57741.061550</td>\n",
       "      <td>2.457742e+06</td>\n",
       "      <td>0.049080</td>\n",
       "      <td>0.905349</td>\n",
       "      <td>0.392274</td>\n",
       "      <td>0.997232</td>\n",
       "      <td>0.023120</td>\n",
       "      <td>-0.070664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>602526</td>\n",
       "      <td>1.551733</td>\n",
       "      <td>-0.924168</td>\n",
       "      <td>2016-12-20T01:00:36.607920</td>\n",
       "      <td>r</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57742.042090</td>\n",
       "      <td>2.457743e+06</td>\n",
       "      <td>0.031951</td>\n",
       "      <td>0.905883</td>\n",
       "      <td>0.392506</td>\n",
       "      <td>0.999503</td>\n",
       "      <td>0.027076</td>\n",
       "      <td>-0.016129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>602527</td>\n",
       "      <td>1.542900</td>\n",
       "      <td>3.133732</td>\n",
       "      <td>2016-12-20T01:02:39.390833</td>\n",
       "      <td>g</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57742.043512</td>\n",
       "      <td>2.457743e+06</td>\n",
       "      <td>0.031926</td>\n",
       "      <td>0.905884</td>\n",
       "      <td>0.392507</td>\n",
       "      <td>0.998143</td>\n",
       "      <td>0.026885</td>\n",
       "      <td>0.054667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>602913</td>\n",
       "      <td>1.352867</td>\n",
       "      <td>-2.023208</td>\n",
       "      <td>2016-12-21T01:12:06.179235</td>\n",
       "      <td>i</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57743.050072</td>\n",
       "      <td>2.457744e+06</td>\n",
       "      <td>0.014327</td>\n",
       "      <td>0.906157</td>\n",
       "      <td>0.392623</td>\n",
       "      <td>0.999098</td>\n",
       "      <td>0.023595</td>\n",
       "      <td>-0.035304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>602914</td>\n",
       "      <td>2.957521</td>\n",
       "      <td>-1.008708</td>\n",
       "      <td>2016-12-21T01:14:05.211018</td>\n",
       "      <td>z</td>\n",
       "      <td>90.0</td>\n",
       "      <td>57743.051449</td>\n",
       "      <td>2.457744e+06</td>\n",
       "      <td>0.014303</td>\n",
       "      <td>0.906157</td>\n",
       "      <td>0.392624</td>\n",
       "      <td>0.998513</td>\n",
       "      <td>0.051588</td>\n",
       "      <td>-0.017604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1970 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      EXPNUM      RADEG    DECDEG                    DATE_OBS BAND  EXPTIME       MJD_OBS        JD_OBS        xe        ye        ze        xc        yc        zc\n",
       "0     229683   0.468621  0.377768  2013-09-02T06:10:47.837043    g     90.0  56537.257498  2.456538e+06  0.946924 -0.322540 -0.139930  0.999945  0.008179  0.006593\n",
       "1     229684   2.065533  1.392268  2013-09-02T06:12:46.472481    g     90.0  56537.258871  2.456538e+06  0.946931 -0.322519 -0.139921  0.999055  0.036032  0.024297\n",
       "2     229685   3.659758  2.406768  2013-09-02T06:15:01.886059    g     90.0  56537.260439  2.456538e+06  0.946940 -0.322496 -0.139911  0.997080  0.063775  0.041994\n",
       "3     229688   3.663221  0.377768  2013-09-02T06:21:30.678404    g     90.0  56537.264938  2.456538e+06  0.946966 -0.322428 -0.139882  0.997935  0.063890  0.006593\n",
       "4     229691   0.457838 -1.651232  2013-09-02T06:27:54.655251    g     90.0  56537.269383  2.456538e+06  0.946991 -0.322362 -0.139854  0.999553  0.007987 -0.028815\n",
       "5     229692   3.655137 -1.651232  2013-09-02T06:30:06.609589    g     90.0  56537.270910  2.456538e+06  0.946999 -0.322339 -0.139844  0.997551  0.063724 -0.028815\n",
       "6     229693   5.258321 -0.636732  2013-09-02T06:32:06.124497    g     90.0  56537.272293  2.456538e+06  0.947007 -0.322318 -0.139835  0.995730  0.091641 -0.011113\n",
       "7     229694   6.857721  0.377768  2013-09-02T06:34:05.969268    g     90.0  56537.273680  2.456538e+06  0.947015 -0.322297 -0.139826  0.992824  0.119402  0.006593\n",
       "8     229695   8.454933  1.392268  2013-09-02T06:36:08.302435    g     90.0  56537.275096  2.456538e+06  0.947023 -0.322276 -0.139817  0.988840  0.146988  0.024297\n",
       "9     229696  10.051358  2.406768  2013-09-02T06:38:20.162801    g     90.0  56537.276622  2.456538e+06  0.947032 -0.322253 -0.139807  0.983783  0.174377  0.041994\n",
       "10    229697   2.046671 -2.665632  2013-09-02T06:40:45.869068    g     90.0  56537.278309  2.456538e+06  0.947041 -0.322228 -0.139796  0.998281  0.035675 -0.046507\n",
       "11    229698   5.246971 -2.665632  2013-09-02T06:42:57.631171    g     90.0  56537.279834  2.456538e+06  0.947050 -0.322205 -0.139787  0.994732  0.091350 -0.046507\n",
       "12    229699   0.431417 -3.680132  2013-09-02T06:45:02.443958    g     90.0  56537.281278  2.456538e+06  0.947058 -0.322184 -0.139777  0.997910  0.007514 -0.064186\n",
       "13    229700   6.839817 -3.680132  2013-09-02T06:47:16.871230    g     90.0  56537.282834  2.456538e+06  0.947067 -0.322160 -0.139768  0.990836  0.118848 -0.064186\n",
       "14    229701   3.635617 -3.680132  2013-09-02T06:49:35.049841    g     90.0  56537.284433  2.456538e+06  0.947076 -0.322136 -0.139757  0.995930  0.063280 -0.064186\n",
       "15    229702   6.852538 -1.651232  2013-09-02T06:51:37.853865    g     90.0  56537.285855  2.456538e+06  0.947084 -0.322115 -0.139748  0.992444  0.119265 -0.028815\n",
       "16    229703  10.049938 -1.651232  2013-09-02T06:53:48.025210    g     90.0  56537.287361  2.456538e+06  0.947092 -0.322092 -0.139739  0.984247  0.174434 -0.028815\n",
       "17    229716   2.016979 -4.694632  2013-09-02T08:06:15.993847    g     90.0  56537.337685  2.456538e+06  0.947373 -0.321339 -0.139417  0.996028  0.035078 -0.081845\n",
       "18    229717   5.226079 -4.694632  2013-09-02T08:08:27.501084    g     90.0  56537.339207  2.456538e+06  0.947381 -0.321316 -0.139407  0.992502  0.090780 -0.081845\n",
       "19    229718   3.604454 -5.709132  2013-09-02T08:10:25.842980    g     90.0  56537.340577  2.456538e+06  0.947389 -0.321296 -0.139398  0.993071  0.062556 -0.099478\n",
       "20    229719   0.389354 -5.709132  2013-09-02T08:12:26.816230    g     90.0  56537.341977  2.456538e+06  0.947397 -0.321275 -0.139389  0.995017  0.006762 -0.099478\n",
       "21    229720   1.973450 -6.723632  2013-09-02T08:14:26.176951    g     90.0  56537.343359  2.456538e+06  0.947404 -0.321254 -0.139380  0.992533  0.034200 -0.117080\n",
       "22    229721   5.195450 -6.723632  2013-09-02T08:16:28.137979    g     90.0  56537.344770  2.456538e+06  0.947412 -0.321233 -0.139371  0.989042  0.089931 -0.117080\n",
       "23    229722   6.819554 -5.709132  2013-09-02T08:18:26.944471    g     90.0  56537.346145  2.456538e+06  0.947420 -0.321212 -0.139362  0.988000  0.118154 -0.099478\n",
       "24    229723   8.435179 -4.694632  2013-09-02T08:20:25.216666    g     90.0  56537.347514  2.456538e+06  0.947427 -0.321192 -0.139354  0.985864  0.146198 -0.081845\n",
       "25    229724   8.447271 -2.665632  2013-09-02T08:22:25.719124    g     90.0  56537.348909  2.456538e+06  0.947435 -0.321171 -0.139345  0.988081  0.146740 -0.046507\n",
       "26    229725  11.647571 -2.665632  2013-09-02T08:24:33.295422    g     90.0  56537.350385  2.456538e+06  0.947443 -0.321149 -0.139335  0.978348  0.201673 -0.046507\n",
       "27    229726   8.453821 -0.636732  2013-09-02T08:26:35.074324    g     90.0  56537.351795  2.456538e+06  0.947451 -0.321128 -0.139326  0.989074  0.147003 -0.011113\n",
       "28    229727  10.052321  0.377768  2013-09-02T08:28:33.629931    g     90.0  56537.353167  2.456538e+06  0.947458 -0.321107 -0.139317  0.984627  0.174544  0.006593\n",
       "29    229728  11.649633  1.392268  2013-09-02T08:30:31.974521    g     90.0  56537.354537  2.456538e+06  0.947466 -0.321087 -0.139309  0.979112  0.201867  0.024297\n",
       "...      ...        ...       ...                         ...  ...      ...           ...           ...       ...       ...       ...       ...       ...       ...\n",
       "1940  599349   6.142708 -3.037608  2016-12-04T02:29:22.832617    z     90.0  57726.103737  2.457727e+06  0.305744  0.864230  0.374445  0.992861  0.106855 -0.052991\n",
       "1941  599351   6.152925 -1.008708  2016-12-04T02:33:19.111757    z     90.0  57726.106471  2.457727e+06  0.305698  0.864244  0.374450  0.994085  0.107166 -0.017604\n",
       "1942  599697   3.148275  2.119332  2016-12-05T01:23:12.695404    z     90.0  57727.057786  2.457728e+06  0.289810  0.868665  0.376371  0.997808  0.054883  0.036981\n",
       "1943  599699   3.132283  4.148232  2016-12-05T01:26:24.625545    z     90.0  57727.060007  2.457728e+06  0.289773  0.868676  0.376376  0.995890  0.054498  0.072337\n",
       "1944  599700   3.132283  4.148232  2016-12-05T01:28:24.022244    i     90.0  57727.061389  2.457728e+06  0.289749  0.868682  0.376379  0.995890  0.054498  0.072337\n",
       "1945  599707   2.957521 -1.008708  2016-12-05T01:41:48.338083    r     90.0  57727.070698  2.457728e+06  0.289592  0.868726  0.376397  0.998513  0.051588 -0.017604\n",
       "1946  599708   1.554317  1.104832  2016-12-05T01:43:45.865633    z     90.0  57727.072059  2.457728e+06  0.289569  0.868733  0.376399  0.999446  0.027120  0.019282\n",
       "1947  599710   1.542900  3.133732  2016-12-05T01:46:56.902576    i     90.0  57727.074270  2.457728e+06  0.289532  0.868743  0.376404  0.998143  0.026885  0.054667\n",
       "1948  599711   1.542900  3.133732  2016-12-05T01:48:55.348865    z     90.0  57727.075641  2.457728e+06  0.289509  0.868749  0.376406  0.998143  0.026885  0.054667\n",
       "1949  599712   1.542900  3.133732  2016-12-05T01:50:53.846887    r     90.0  57727.077012  2.457728e+06  0.289486  0.868756  0.376409  0.998143  0.026885  0.054667\n",
       "1950  599714   3.132283  4.148232  2016-12-05T01:54:07.163708    r     90.0  57727.079250  2.457728e+06  0.289448  0.868767  0.376414  0.995890  0.054498  0.072337\n",
       "1951  599715   6.333375  4.148232  2016-12-05T01:56:06.716377    z     90.0  57727.080633  2.457728e+06  0.289425  0.868773  0.376416  0.991293  0.110024  0.072337\n",
       "1952  599718   1.363079  0.005792  2016-12-05T02:01:24.124961    r     90.0  57727.084307  2.457728e+06  0.289363  0.868790  0.376423  0.999717  0.023788  0.000101\n",
       "1953  599719   2.960129  1.020292  2016-12-05T02:03:21.991212    r     90.0  57727.085671  2.457728e+06  0.289340  0.868797  0.376426  0.998507  0.051633  0.017807\n",
       "1954  599720   2.960129  1.020292  2016-12-05T02:05:22.311540    i     90.0  57727.087064  2.457728e+06  0.289316  0.868803  0.376429  0.998507  0.051633  0.017807\n",
       "1955  599721   4.554767  2.034792  2016-12-05T02:07:20.795405    r     90.0  57727.088435  2.457728e+06  0.289293  0.868810  0.376432  0.996213  0.079362  0.035506\n",
       "1956  599722   6.154829  1.020292  2016-12-05T02:09:19.565549    z     90.0  57727.089810  2.457728e+06  0.289270  0.868816  0.376434  0.994078  0.107199  0.017807\n",
       "1957  599724   7.750575  2.034792  2016-12-05T02:13:15.604826    z     90.0  57727.092542  2.457728e+06  0.289224  0.868829  0.376440  0.990240  0.134776  0.035506\n",
       "1958  599726   4.557675  0.005792  2016-12-05T02:17:15.486513    z     90.0  57727.095318  2.457728e+06  0.289177  0.868842  0.376445  0.996838  0.079463  0.000101\n",
       "1959  599728   4.550158 -2.023208  2016-12-05T02:21:12.132313    z     90.0  57727.098057  2.457728e+06  0.289131  0.868855  0.376450  0.996227  0.079282 -0.035304\n",
       "1960  599730   4.532342 -4.052108  2016-12-05T02:25:09.216850    z     90.0  57727.100801  2.457728e+06  0.289084  0.868868  0.376456  0.994381  0.078824 -0.070664\n",
       "1961  600111   2.914979 -5.066608  2016-12-06T02:29:36.325825    z     90.0  57728.103893  2.457729e+06  0.272235  0.873265  0.378362  0.994804  0.050655 -0.088314\n",
       "1962  602137   1.328129 -4.052108  2016-12-19T01:22:40.980411    g     90.0  57741.057419  2.457742e+06  0.049153  0.905345  0.392273  0.997232  0.023120 -0.070664\n",
       "1963  602138   2.957521 -1.008708  2016-12-19T01:24:40.623946    i     90.0  57741.058803  2.457742e+06  0.049129  0.905347  0.392273  0.998513  0.051588 -0.017604\n",
       "1964  602139   2.942400 -3.037608  2016-12-19T01:26:39.578978    z     90.0  57741.060180  2.457742e+06  0.049104  0.905348  0.392274  0.997278  0.051260 -0.052991\n",
       "1965  602140   1.328129 -4.052108  2016-12-19T01:28:37.923112    z     90.0  57741.061550  2.457742e+06  0.049080  0.905349  0.392274  0.997232  0.023120 -0.070664\n",
       "1966  602526   1.551733 -0.924168  2016-12-20T01:00:36.607920    r     90.0  57742.042090  2.457743e+06  0.031951  0.905883  0.392506  0.999503  0.027076 -0.016129\n",
       "1967  602527   1.542900  3.133732  2016-12-20T01:02:39.390833    g     90.0  57742.043512  2.457743e+06  0.031926  0.905884  0.392507  0.998143  0.026885  0.054667\n",
       "1968  602913   1.352867 -2.023208  2016-12-21T01:12:06.179235    i     90.0  57743.050072  2.457744e+06  0.014327  0.906157  0.392623  0.999098  0.023595 -0.035304\n",
       "1969  602914   2.957521 -1.008708  2016-12-21T01:14:05.211018    z     90.0  57743.051449  2.457744e+06  0.014303  0.906157  0.392624  0.998513  0.051588 -0.017604\n",
       "\n",
       "[1970 rows x 14 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This rotation is taking things from equatorial to ecliptic\n",
    "rot_mat = MPC_library.rotate_matrix(-MPC_library.Constants.ecl)\n",
    "def equatorial_to_ecliptic(v, rot_mat=MPC_library.rotate_matrix(-MPC_library.Constants.ecl)):\n",
    "    return np.dot(rot_mat, v.reshape(-1, 1)).flatten()\n",
    "\n",
    "def equatorial_to_ecliptic_v2(v, rot_mat=MPC_library.rotate_matrix(-MPC_library.Constants.ecl)):\n",
    "    return np.matmul(v, rot_mat.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the vector to the observatory and unit vector to the exposure center to ecliptic coordinates\n",
    "\n",
    "ree = equatorial_to_ecliptic_v2(df[['xe', 'ye', 'ze']].as_matrix())\n",
    "df['xe_ec'] = ree[:,0]\n",
    "df['ye_ec'] = ree[:,1]\n",
    "df['ze_ec'] = ree[:,2]\n",
    "\n",
    "rce = equatorial_to_ecliptic_v2(df[['xc', 'yc', 'zc']].as_matrix())\n",
    "df['xc_ec'] = rce[:,0]\n",
    "df['yc_ec'] = rce[:,1]\n",
    "df['zc_ec'] = rce[:,2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This returns the topocentric distances and new heliocentric\n",
    "# position vectors to the target, given the assumed distance\n",
    "# r and the position vector of the observatory re.\n",
    "def adjust_position(r, rho_target, re):\n",
    "    rho_x, rho_y, rho_z = rho_target\n",
    "    xe, ye, ze = re\n",
    "    Robs = np.sqrt(xe * xe + ye * ye + ze * ze)\n",
    "    cos_phi = -(rho_x * xe + rho_y * ye + rho_z * ze) / Robs\n",
    "    phi = np.arccos(cos_phi)\n",
    "    sin_phi = np.sin(phi)\n",
    "\n",
    "    xx2 = r*r - Robs*sin_phi * Robs*sin_phi\n",
    "    \n",
    "    if xx2 < 0:\n",
    "        None, None\n",
    "\n",
    "    xx = np.sqrt(xx2)\n",
    "    yy = Robs * cos_phi\n",
    "    \n",
    "    rho_p = yy + xx\n",
    "\n",
    "    # This could be done with numpy arrays\n",
    "    x_p = xe + rho_p*rho_x\n",
    "    y_p = ye + rho_p*rho_y\n",
    "    z_p = ze + rho_p*rho_z\n",
    "    \n",
    "    rho_m = yy - xx\n",
    "    \n",
    "    # This could be done with numpy arrays    \n",
    "    x_m = xe + rho_m*rho_x\n",
    "    y_m = ye + rho_m*rho_y\n",
    "    z_m = ze + rho_m*rho_z\n",
    "        \n",
    "    return (rho_p, (x_p, y_p, z_p)), (rho_m, (x_m, y_m, z_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the reference vector, which will be the average of the vectors from the barycenter to the exposure centers, assuming a distance of 40 AU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = df[['xe_ec', 'ye_ec', 'ze_ec']].as_matrix()\n",
    "rc = df[['xc_ec', 'yc_ec', 'zc_ec']].as_matrix()\n",
    "\n",
    "# Doing this as a loop, but it's not compute-intensive\n",
    "rp = np.zeros((rc.shape))\n",
    "for i, (rcp, rep) in enumerate(zip(rc, re)):\n",
    "     ((_, tmp), (_, _)) = adjust_position(40.0, rcp, rep)\n",
    "     rp[i] = np.array(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.98323679,  0.16221222, -0.08326226])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_vec = np.mean(rp, axis=0)\n",
    "ref_vec /= np.linalg.norm(ref_vec)\n",
    "ref_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check again\n",
    "# This routine returns the 3-D rotation matrix for the \n",
    "# given reference vector.\n",
    "def xyz_to_proj_matrix(r_ref):\n",
    "    x_ref, y_ref, z_ref = r_ref\n",
    "    r = np.sqrt(x_ref*x_ref + y_ref*y_ref + z_ref*z_ref)\n",
    "    lon0 = np.arctan2(y_ref, x_ref)\n",
    "    lat0 = np.arcsin(z_ref/r)\n",
    "    slon0 = np.sin(lon0)\n",
    "    clon0 = np.cos(lon0)\n",
    "    slat0 = np.sin(lat0)\n",
    "    clat0 = np.cos(lat0)\n",
    "\n",
    "    mat = np.array([[-slon0, clon0, 0], \n",
    "                    [-clon0*slat0, -slon0*slat0, clat0], \n",
    "                    [clon0*clat0, slon0*clat0, slat0 ]])\n",
    "    \n",
    "    return mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the reference direction has been established, the next step is to transform the xe, ye, ze observatory postions to tangent plane coordinates and RA/Dec positions in each exposure to ecliptic coordinates and then to tangent plane coordinates.  This will result in theta_x, theta_y values for each position, which will be the primary input to the later steps of the algorithm.\n",
    "\n",
    "We also establish the reference time as the mean of the exposure times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = xyz_to_proj_matrix(ref_vec)\n",
    "\n",
    "re_proj = np.matmul(re, mat.T)\n",
    "\n",
    "df['xe_p'] = re_proj[:,0]\n",
    "df['ye_p'] = re_proj[:,1]\n",
    "df['ze_p'] = re_proj[:,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = MPC_library.Constants.speed_of_light\n",
    "GM = MPC_library.Constants.GMsun"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The stuff below here depends upon gamma and gamma-dot\n",
    "gamma = 0.025\n",
    "gamma_dot = 0.0\n",
    "\n",
    "t0 = df['JD_OBS'].mean()\n",
    "\n",
    "\n",
    "tp = df['JD_OBS'] - t0 - 1.0/(gamma*c)\n",
    "\n",
    "df['tp'] = tp\n",
    "df['tp2'] = tp*tp\n",
    "\n",
    "f = 1.0 + GM * gamma**3 * df['tp2'] - gamma * df['ze_p'] + gamma_dot * df['tp']\n",
    "\n",
    "df['f'] = f\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mjd': '56544.189986000005', 'mag': '21.226', 'objid': '227594796', 'HPIX_1024': '4456041', 'exptime': '90.0', 'HPIX_16': '1087', 'HPIX_32': '4351', 'fakeid': '180061594', 'ccd': '26', 'expnum': '231922', 'HPIX_512': '1114010', 'HPIX_256': '278502', 'mag_from_limit': '3.372', 'band': 'g', 'dec': '-1.1987762826140012', 'HPIX_128': '69625', 'nite': '20130908', 'date': '2013/9/9 04:33:35', 'HPIX_64': '17406', 'ml_score': '0.9670000000000001', 'ra': '359.4563006020627', 'snr': '36.685'}\n",
      "{'mjd': '56958.183438199994', 'mag': '22.813000000000002', 'objid': '231859269', 'HPIX_1024': '4805892', 'exptime': '90.0', 'HPIX_16': '1173', 'HPIX_32': '4693', 'fakeid': '180156619', 'ccd': '32', 'expnum': '372120', 'HPIX_512': '1201473', 'HPIX_256': '300368', 'mag_from_limit': '2.1069999999999998', 'band': 'r', 'dec': '-0.4781901968101665', 'HPIX_128': '75092', 'nite': '20141027', 'date': '2014/10/28 04:24:09', 'HPIX_64': '18773', 'ml_score': '0.93', 'ra': '359.36581553365045', 'snr': '8.792'}\n",
      "{'mjd': '57333.0698487', 'mag': '19.503', 'objid': '233735979', 'HPIX_1024': '4983952', 'exptime': '90.0', 'HPIX_16': '1216', 'HPIX_32': '4867', 'fakeid': '180096500', 'ccd': '13', 'expnum': '490624', 'HPIX_512': '1245988', 'HPIX_256': '311497', 'mag_from_limit': '5.056', 'band': 'r', 'dec': '2.8696971901983295', 'HPIX_128': '77874', 'nite': '20151106', 'date': '2015/11/7 01:40:35', 'HPIX_64': '19468', 'ml_score': '0.98', 'ra': '359.8182242565234', 'snr': '168.051'}\n",
      "{'mjd': '57626.2073244', 'mag': '21.809', 'objid': '234768625', 'HPIX_1024': '4806133', 'exptime': '90.0', 'HPIX_16': '1173', 'HPIX_32': '4693', 'fakeid': '180076034', 'ccd': '25', 'expnum': '567838', 'HPIX_512': '1201533', 'HPIX_256': '300383', 'mag_from_limit': '2.7880000000000003', 'band': 'r', 'dec': '0.4492152643226641', 'HPIX_128': '75095', 'nite': '20160825', 'date': '2016/8/26 04:58:33', 'HPIX_64': '18773', 'ml_score': '0.94', 'ra': '359.41514511985895', 'snr': '25.395'}\n"
     ]
    }
   ],
   "source": [
    "# First, do this for just a single object to test the results.\n",
    "\n",
    "raDecDict = defaultdict(list)\n",
    "magIDDict = defaultdict(list)\n",
    "for filename in filelist:\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        spamreader = csv.DictReader(csvfile)\n",
    "        for i, row in enumerate(spamreader):\n",
    "            if i==0:\n",
    "                print(row)\n",
    "            key = int(row['expnum'])\n",
    "#            if int(row['fakeid'])==180109419:\n",
    "#            if int(row['fakeid'])==180159275:\n",
    "            if int(row['fakeid'])==180147080:\n",
    "                magIDDict[key].append((float(row['mag']), int(row['objid']), int(row['fakeid'])))\n",
    "                raDecDict[key].append((float(row['ra']), float(row['dec'])))\n",
    "\n",
    "justThese=magIDDict.keys()\n",
    "\n",
    "xyz_proj = defaultdict(np.array)\n",
    "#for expnum in df['EXPNUM']:\n",
    "for expnum in justThese:\n",
    "    raDec = np.array(raDecDict[expnum])\n",
    "    if raDec.shape[0]>0:\n",
    "        xyz = raDec_to_xyz(raDec)\n",
    "        xyz_ec = equatorial_to_ecliptic_v2(xyz)\n",
    "        xyz_proj[expnum] = np.matmul(xyz_ec, mat.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24825012862e-07\n",
      "4.24825012862e-07 0.000848034701 2.41858516e-07 -2.1560975e-07 2.74588074e-07\n",
      "CPU times: user 5.1 ms, sys: 272 µs, total: 5.37 ms\n",
      "Wall time: 5.27 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "thresh = 5\n",
    "rad = (np.pi/180.)*(0.01/3600.)\n",
    "\n",
    "# With reference direction from first observation\n",
    "# 0.00165551  0.00073627 -0.00010067 -0.00024954  0.00313782  0.00071216\n",
    "\n",
    "#alpha_dot = 0.00073627/365.25\n",
    "#beta_dot = -0.00024954/365.25\n",
    "\n",
    "#gamma = 0.00313782  \n",
    "#gamma_dot = 0.00071216/365.25\n",
    "\n",
    "# After adjusting reference direction\n",
    "# 0.05283919  0.00077423  0.06707400 -0.00020518  0.00314923  0.00069145\n",
    "\n",
    "# After sorting the times\n",
    "# 0.05283919  0.00077411  0.06707400 -0.00020534  0.00314923  0.00068910\n",
    "\n",
    "alpha_dot = 0.00077411/365.25\n",
    "beta_dot = -0.00020534/365.25\n",
    "\n",
    "gamma = 0.00314923 \n",
    "gamma_dot = 0.00068910/365.25\n",
    "\n",
    "alpha = 1.26587450e-01     \n",
    "alpha_dot = 2.41858516e-07 \n",
    "beta = -7.61819057e-02    \n",
    "beta_dot = -2.15609750e-07\n",
    "gamma = 8.48034701e-04\n",
    "gamma_dot = 2.74588074e-07   \n",
    "\n",
    "\n",
    "per = 365.25*np.power(gamma, -1.5)\n",
    "n = 2.0*np.pi/per\n",
    "\n",
    "print(n)\n",
    "\n",
    "print(n, gamma, alpha_dot, beta_dot, gamma_dot)\n",
    "\n",
    "\n",
    "alpha_p = []\n",
    "beta_p = []\n",
    "fakeIDs = []\n",
    "tps = []\n",
    "xs = []\n",
    "ys = []\n",
    "xes = []\n",
    "yes = []\n",
    "zes = []\n",
    "\n",
    "# choose the range of alpha_dot and beta_dot based on gamma\n",
    "\n",
    "# choose gamma\n",
    "# compute tp\n",
    "\n",
    "t0 = df['JD_OBS'].mean()\n",
    "\n",
    "tp = df['JD_OBS'] - t0 - 1.0/(gamma*c)\n",
    "\n",
    "df['tp'] = tp\n",
    "df['tp2'] = tp*tp\n",
    "\n",
    "# choose gamma_dot\n",
    "# compute f\n",
    "\n",
    "f = 1.0 - 0.5*GM * gamma**3 * df['tp2'] - gamma * df['ze_p'] + gamma_dot * df['tp']\n",
    "\n",
    "h  = 1.0 - 0.5*GM * gamma**3 * df['tp2']\n",
    "\n",
    "df['f'] = f\n",
    "df['h'] = h\n",
    "\n",
    "# compute the theta_x*f + gamma*xe and theta_y*f + gamma*ye parts\n",
    "# those are the basic exposure arrays\n",
    "for expnum, tp, f, xe, ye, ze, h in zip(df['EXPNUM'], df['tp'], df['f'], df['xe_p'], df['ye_p'], df['ze_p'], df['h']):\n",
    "    if expnum in justThese:\n",
    "        for row, row2 in zip(xyz_proj[expnum], magIDDict[expnum]):\n",
    "            xp, yp, zp = row\n",
    "            theta_x = xp/zp\n",
    "            theta_y = yp/zp\n",
    "            _, _, fakeID = row2\n",
    "            alpha_p.append((theta_x*f + gamma*xe - alpha_dot*tp)/h)\n",
    "            beta_p.append((theta_y*f + gamma*ye - beta_dot*tp)/h)\n",
    "            fakeIDs.append(fakeID)\n",
    "            tps.append(tp)\n",
    "            xs.append(theta_x)\n",
    "            ys.append(theta_y)\n",
    "            xes.append(xe)\n",
    "            yes.append(ye)\n",
    "            zes.append(ze)\n",
    "\n",
    "# Keep the unshifted arrays.  Then carry out a series of alpha_dot, beta_dot shifts,\n",
    "# building a KD-tree and searching for clusters after each shift.\n",
    "\n",
    "\n",
    "alpha_p = np.array(alpha_p)\n",
    "beta_p = np.array(beta_p)\n",
    "times = np.array(tps)\n",
    "xs = np.array(xs)\n",
    "ys = np.array(ys)\n",
    "\n",
    "xes = np.array(xes)\n",
    "yes = np.array(yes)\n",
    "zes = np.array(zes)\n",
    "\n",
    "points = np.vstack((alpha_p, beta_p)).T\n",
    "\n",
    "cluster_set = set()\n",
    "\n",
    "\n",
    "tree = scipy.spatial.cKDTree(points)\n",
    "matches = tree.query_ball_tree(tree, rad)\n",
    "for j, match in enumerate(matches):\n",
    "    if len(match)>=thresh:\n",
    "        cluster_set.update({tuple(match)})\n",
    "\n",
    "cluster_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 180147080 -245.752079984 [ 0.12658523 -0.07618398]\n",
      "1 1 180147080 -220.820426084 [ 0.12658528 -0.07618398]\n",
      "2 2 180147080 446.345941716 [ 0.12658526 -0.07618398]\n",
      "3 3 180147080 505.111249916 [ 0.12658525 -0.07618398]\n",
      "4 4 180147080 511.163879216 [ 0.12658525 -0.07618398]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cluster in cluster_set:\n",
    "    for i, elt in enumerate(cluster):\n",
    "        print(i, elt, fakeIDs[elt], tps[elt], points[elt])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x12064c198>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAENCAYAAADQTGjcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ced79b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter((alpha_p-alpha_p.mean())*206265, (beta_p-beta_p.mean())*206265, c=times)\n",
    "plt.xlabel(r'$\\theta_x$ (arcsec)')\n",
    "plt.ylabel(r'$\\theta_y$ (arcsec)')\n",
    "#plt.xlim(-0.005, 0.005)\n",
    "#plt.ylim(-0.005, 0.005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12658522702903707,\n",
       " 0.12658528031658856,\n",
       " -0.076183979618748385,\n",
       " -0.076183978338626057)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[:, 0].min(), points[:, 0].max(), points[:, 1].min(), points[:, 1].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to output the detections for processing with the modified Berstein code, to be able to see the fitted orbital parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the theta_x*f + gamma*xe and theta_y*f + gamma*ye parts\n",
    "# those are the basic exposure arrays\n",
    "with open('DES_detections.txt', 'w') as file:\n",
    "    for expnum, t in zip(df['EXPNUM'], df['JD_OBS']):\n",
    "        for row, row2 in zip(raDecDict[expnum], magIDDict[expnum]):\n",
    "            ra, dec = row\n",
    "            _, _, fakeID = row2\n",
    "            outstring = \"%d %.16lf %.16lf %.16lf 0.2 807\\n\" % (fakeID, t, ra, dec)\n",
    "            file.write(outstring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the observations for all the synthetic objects to calculate their fitted orbital parameters.\n",
    "\n",
    "mpc_output_dict = defaultdict(list)\n",
    "for filename in filelist:\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        spamreader = csv.DictReader(csvfile)\n",
    "        for i, row in enumerate(spamreader):\n",
    "            fakeid = int(row['fakeid'])\n",
    "            outstring = \"%d %.16lf %.16lf %.16lf %.1lf %d\" % (int(row['fakeid']), float(row['mjd']) + 2400000.5, float(row['ra']), float(row['dec']), 0.2, 807)\n",
    "            mpc_output_dict[fakeid].append(outstring)\n",
    "\n",
    "\n",
    "outfile = open('DES_parameters.out', 'w')\n",
    "outfile.close()\n",
    "for key in sorted(mpc_output_dict.keys()):\n",
    "    val = mpc_output_dict[key]\n",
    "    if len(val)>=5:\n",
    "        sorted_string = sorted(val, key=lambda x: x[10:34])\n",
    "        instring = '\\n'.join(sorted_string)\n",
    "        infile = open('DES_'+str(key), 'w')\n",
    "        infile.write(instring)\n",
    "        infile.close()\n",
    "        command='/Users/mholman/Dropbox/planet_x/orbfit.v2/fit_radec_des 2457172.751213126 0.98323697 0.16221004 -0.08326437 <' + 'DES_' + str(key) + ' >> DES_parameters.out'\n",
    "        print(command)\n",
    "        os.system(command)\n",
    "    else:\n",
    "        outfile = open('DES_parameters.out', 'a')\n",
    "        outfile.write(str(key)+'\\n')\n",
    "        outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict = {}\n",
    "for line in open('DES_parameters.out', 'r'):\n",
    "    vals = line.strip().split()\n",
    "    key = int(vals[0])\n",
    "    if len(vals) > 1:\n",
    "        nobs = int(vals[1])\n",
    "        alpha = float(vals[2])\n",
    "        alpha_dot = float(vals[3])        \n",
    "        beta = float(vals[4])        \n",
    "        beta_dot = float(vals[5])\n",
    "        gamma = float(vals[6])\n",
    "        gamma_dot = float(vals[7])\n",
    "        a = float(vals[8])\n",
    "        e = float(vals[9])\n",
    "        incl = float(vals[10])\n",
    "        dur = float(vals[11])\n",
    "        parameters_dict[key] = {'nobs': nobs, \n",
    "                                'alpha': alpha, 'alpha_dot': alpha_dot, \n",
    "                                'beta': beta, 'beta_dot': beta_dot,\n",
    "                                'gamma': gamma, 'gamma_dot': gamma_dot,\n",
    "                                'a': a, 'e': e, 'incl': incl, 'dur': dur}\n",
    "    else:\n",
    "        parameters_dict[key] = None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I am going to collect the data for all the objects, to do search runs.\n",
    "\n",
    "raDecDict = defaultdict(list)\n",
    "magIDDict = defaultdict(list)\n",
    "#for filename in filelist:\n",
    "for filename in scrambledlist:\n",
    "    with open(filename, newline='') as csvfile:\n",
    "        spamreader = csv.DictReader(csvfile)\n",
    "        for i, row in enumerate(spamreader):\n",
    "            if i==0:\n",
    "                print(row)\n",
    "            key = int(row['expnum'])\n",
    "            #if int(row['fakeid'])==180109419:\n",
    "            magIDDict[key].append((float(row['mag']), int(row['objid']), int(row['fakeid'])))\n",
    "            raDecDict[key].append((float(row['ra']), float(row['dec'])))\n",
    "\n",
    "justThese=magIDDict.keys()\n",
    "\n",
    "xyz_proj = defaultdict(np.array)\n",
    "#for expnum in df['EXPNUM']:\n",
    "for expnum in justThese:\n",
    "    raDec = np.array(raDecDict[expnum])\n",
    "    if raDec.shape[0]>0:\n",
    "        xyz = raDec_to_xyz(raDec)\n",
    "        xyz_ec = equatorial_to_ecliptic_v2(xyz)\n",
    "        xyz_proj[expnum] = np.matmul(xyz_ec, mat.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "thresh = 5\n",
    "rad = (np.pi/180.)*(0.01/3600.)\n",
    "\n",
    "# With reference direction from first observation\n",
    "# 0.00165551  0.00073627 -0.00010067 -0.00024954  0.00313782  0.00071216\n",
    "\n",
    "#alpha_dot = 0.00073627/365.25\n",
    "#beta_dot = -0.00024954/365.25\n",
    "\n",
    "#gamma = 0.00313782  \n",
    "#gamma_dot = 0.00071216/365.25\n",
    "\n",
    "# After adjusting reference direction\n",
    "# 0.05283919  0.00077423  0.06707400 -0.00020518  0.00314923  0.00069145\n",
    "\n",
    "# After sorting the times\n",
    "# 0.05283919  0.00077411  0.06707400 -0.00020534  0.00314923  0.00068910\n",
    "\n",
    "alpha_dot = 0.00077411/365.25\n",
    "beta_dot = -0.00020534/365.25\n",
    "\n",
    "gamma = 0.00314923 \n",
    "gamma_dot = 0.00068910/365.25\n",
    "\n",
    "alpha = 1.26587450e-01     \n",
    "alpha_dot = 2.41858516e-07 \n",
    "beta = -7.61819057e-02    \n",
    "beta_dot = -2.15609750e-07\n",
    "gamma = 8.48034701e-04\n",
    "gamma_dot = 2.74588074e-07   \n",
    "\n",
    "\n",
    "per = 365.25*np.power(gamma, -1.5)\n",
    "n = 2.0*np.pi/per\n",
    "\n",
    "print(n)\n",
    "\n",
    "print(n, gamma, alpha_dot, beta_dot, gamma_dot)\n",
    "\n",
    "\n",
    "alpha_p = []\n",
    "beta_p = []\n",
    "fakeIDs = []\n",
    "tps = []\n",
    "xs = []\n",
    "ys = []\n",
    "xes = []\n",
    "yes = []\n",
    "zes = []\n",
    "\n",
    "# choose the range of alpha_dot and beta_dot based on gamma\n",
    "\n",
    "# choose gamma\n",
    "# compute tp\n",
    "\n",
    "t0 = df['JD_OBS'].mean()\n",
    "\n",
    "tp = df['JD_OBS'] - t0 - 1.0/(gamma*c)\n",
    "\n",
    "df['tp'] = tp\n",
    "df['tp2'] = tp*tp\n",
    "\n",
    "# choose gamma_dot\n",
    "# compute f\n",
    "\n",
    "f = 1.0 - 0.5*GM * gamma**3 * df['tp2'] - gamma * df['ze_p'] + gamma_dot * df['tp']\n",
    "\n",
    "h  = 1.0 - 0.5*GM * gamma**3 * df['tp2']\n",
    "\n",
    "df['f'] = f\n",
    "df['h'] = h\n",
    "\n",
    "# compute the theta_x*f + gamma*xe and theta_y*f + gamma*ye parts\n",
    "# those are the basic exposure arrays\n",
    "for expnum, tp, f, xe, ye, ze, h in zip(df['EXPNUM'], df['tp'], df['f'], df['xe_p'], df['ye_p'], df['ze_p'], df['h']):\n",
    "    if expnum in justThese:\n",
    "        for row, row2 in zip(xyz_proj[expnum], magIDDict[expnum]):\n",
    "            xp, yp, zp = row\n",
    "            theta_x = xp/zp\n",
    "            theta_y = yp/zp\n",
    "            _, _, fakeID = row2\n",
    "            alpha_p.append((theta_x*f + gamma*xe - alpha_dot*tp)/h)\n",
    "            beta_p.append((theta_y*f + gamma*ye - beta_dot*tp)/h)\n",
    "            fakeIDs.append(fakeID)\n",
    "            tps.append(tp)\n",
    "            xs.append(theta_x)\n",
    "            ys.append(theta_y)\n",
    "            xes.append(xe)\n",
    "            yes.append(ye)\n",
    "            zes.append(ze)\n",
    "\n",
    "# Keep the unshifted arrays.  Then carry out a series of alpha_dot, beta_dot shifts,\n",
    "# building a KD-tree and searching for clusters after each shift.\n",
    "\n",
    "\n",
    "alpha_p = np.array(alpha_p)\n",
    "beta_p = np.array(beta_p)\n",
    "times = np.array(tps)\n",
    "xs = np.array(xs)\n",
    "ys = np.array(ys)\n",
    "\n",
    "xes = np.array(xes)\n",
    "yes = np.array(yes)\n",
    "zes = np.array(zes)\n",
    "\n",
    "points = np.vstack((alpha_p, beta_p)).T\n",
    "\n",
    "cluster_set = set()\n",
    "\n",
    "\n",
    "tree = scipy.spatial.cKDTree(points)\n",
    "matches = tree.query_ball_tree(tree, rad)\n",
    "for j, match in enumerate(matches):\n",
    "    if len(match)>=thresh:\n",
    "        cluster_set.update({tuple(match)})\n",
    "\n",
    "cluster_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "points_s = points[points[:,0].argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points[:,0].argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dy = 0.0001\n",
    "args = np.floor(points[:,1]//dy).astype(int)\n",
    "idx = np.lexsort((points[:,0], args))\n",
    "points_s=points[idx]\n",
    "\n",
    "trans=np.argwhere((points_s[1:,0]-points_s[:-1, 0])<0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000, 20000):\n",
    "    print(points_s[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans[1000:1010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_s[995032:995035]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(trans):\n",
    "    print(i, points_s[t-1:t+2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to write a c routine with a python wrapper that will do the strip-based neighbor calculation.  It needs to accept a sorted points array, with a dy value, an r value, a cluster membership threshold, and an array of start indices.\n",
    "\n",
    "The routine then marches down each strip, keeping track of the last near index, looking for values in that strip and the neighboring strips that are closer than r.  The returned values are the clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(points[:,0], points[:,1], c=args)\n",
    "plt.xlim(-0.3,0.3)\n",
    "plt.ylim(-0.3,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(points_s[:,0], points_s[:,1])\n",
    "plt.xlim(-0.3,0.3)\n",
    "plt.ylim(-0.3,0.3)\n",
    "plt.savefig('shifted.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xs, ys)\n",
    "plt.xlim(-0.3,0.3)\n",
    "plt.ylim(-0.3,0.3)\n",
    "plt.savefig('unshifted.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_s[:, 1].min(), points_s[:, 1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to alter this so that it does the full range of alpha_dot and beta_dot for a given value or values of gamma\n",
    "# and a given step size in alpha_dot and beta_dot.\n",
    "\n",
    "thresh = 6\n",
    "rad = (np.pi/180.)*(20./3600.)\n",
    "\n",
    "\n",
    "gamma = 0.02\n",
    "gamma_dot = 0.0\n",
    "\n",
    "per = 365.25*np.power(gamma, -1.5)\n",
    "n = 2.0*np.pi/per\n",
    "\n",
    "alpha_dot = n\n",
    "beta_dot = 0.2*n\n",
    "\n",
    "fakeIDs = []\n",
    "\n",
    "def single_detection_clusters(df, xyz_proj, magIDDict, \n",
    "                              gamma, gamma_dot, alpha_dot_beta_dot_pairs,\n",
    "                              thresh, rad, GM):\n",
    "    \n",
    "    ntotal = np.sum([v.shape[0] for key, v in xyz_proj.items()])\n",
    "        \n",
    "    theta  = np.zeros((ntotal, 2))\n",
    "    re     = np.zeros((ntotal, 3))\n",
    "    points = np.zeros((ntotal, 2))\n",
    "    points0 = np.zeros((ntotal, 2))    \n",
    "    fs     = np.zeros(ntotal)\n",
    "    \n",
    "    #fakeIDs = []\n",
    "    #tps = []\n",
    "\n",
    "    # choose the range of alpha_dot and beta_dot based on gamma\n",
    "    \n",
    "    # choose gamma\n",
    "    # compute tp\n",
    "\n",
    "    # This can be done up front.\n",
    "    t0 = df['JD_OBS'].mean()\n",
    "\n",
    "    #tp_arr = df['JD_OBS'] - t0 - 1.0/(gamma*c)\n",
    "    #tp2_arr = tp_arr*tp_arr\n",
    "    \n",
    "    ts = np.zeros(ntotal)\n",
    "\n",
    "    # choose gamma_dot\n",
    "    # compute f\n",
    "\n",
    "    # Iterate over the exposures to set up the data\n",
    "    i=0\n",
    "    for expnum, t, xe, ye, ze in zip(df['EXPNUM'], df['JD_OBS'], df['xe_p'], df['ye_p'], df['ze_p']):\n",
    "        \n",
    "        # Iterate over the detections in each exposure, filling in some arrays\n",
    "        for row, row2 in zip(xyz_proj[expnum], magIDDict[expnum]):\n",
    "            xp, yp, zp = row\n",
    "            theta_x = xp/zp\n",
    "            theta_y = yp/zp\n",
    "            _, _, fakeID = row2\n",
    "            \n",
    "            ts[i] = t-t0\n",
    "\n",
    "            theta[i][0] = theta_x\n",
    "            theta[i][1] = theta_y\n",
    "            \n",
    "            re[i][0] = xe\n",
    "            re[i][1] = ye\n",
    "            re[i][2] = ze\n",
    "            \n",
    "            fakeIDs.append(fakeID)\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "    # Now we have arrays of theta, re, and ts, but those could actually be the input \n",
    "    # to the routine.\n",
    "    \n",
    "    # The rest of this is very simple.\n",
    "    \n",
    "    # The gamma loop starts here:\n",
    "    \n",
    "    tp = ts-1.0/(gamma*c)\n",
    "    \n",
    "\n",
    "    # The gamma_dot loop starts here.\n",
    "    # The range of gamma_dot depends upon the value of gamma\n",
    "    \n",
    "    f_arr = 1.0 - 0.5*GM * gamma**3 * tp*tp - gamma * re[:, 2] + gamma_dot * tp\n",
    "    h_arr = 1.0 - 0.5*GM * gamma**3 * tp*tp\n",
    "    \n",
    "    points0 = theta*f_arr[:, np.newaxis] + gamma*re[:, 0:2]\n",
    "    \n",
    "    # The unshifted array points0 is now established for the given gamma, gamma_dot.  \n",
    "    \n",
    "    # Now carry out a series of alpha_dot, beta_dot shifts,\n",
    "    # building a KD-tree and searching for clusters after each shift.\n",
    "    # The range of alpha_dot and beta_dot depends upon the value of gamma.\n",
    "\n",
    "    cluster_set_dict=defaultdict(set)\n",
    "    \n",
    "    for alpha_dot, beta_dot in alpha_dot_beta_dot_pairs:\n",
    "        points[:, 0] = (points0[:, 0] - alpha_dot*tp)/h_arr\n",
    "        points[:, 1] = (points0[:, 1] -  beta_dot*tp)/h_arr\n",
    "    \n",
    "        #cluster_set = set()\n",
    "\n",
    "        tree = scipy.spatial.cKDTree(points)\n",
    "        matches = tree.query_ball_tree(tree, rad)\n",
    "        for j, match in enumerate(matches):\n",
    "            if len(match)>=thresh:\n",
    "                cluster_set_dict[tuple(match)].update({(alpha_dot, beta_dot)})\n",
    "\n",
    "        #cluster_set_dict[(alpha_dot, beta_dot)] = cluster_set\n",
    "\n",
    "    return cluster_set_dict, tp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "thresh = 12\n",
    "rad = (np.pi/180.)*(20./3600.)\n",
    "\n",
    "alpha_dot = 0.00077411/365.25\n",
    "beta_dot = -0.00020534/365.25\n",
    "\n",
    "gamma = 0.00314923 \n",
    "gamma_dot = 0.00068910/365.25\n",
    "\n",
    "#gamma = 0.011\n",
    "#gamma_dot = 0.0\n",
    "\n",
    "per = 365.25*np.power(gamma, -1.5)\n",
    "print(per/365.25)\n",
    "n = 1.5*np.pi/per\n",
    "\n",
    "#alpha_dot = n\n",
    "#beta_dot = 0.0*n\n",
    "\n",
    "apts, bpts = np.meshgrid(np.linspace(0.9, 1.1, 5)*alpha_dot, np.linspace(0.9, 1.1, 5)*beta_dot)\n",
    "alpha_dot_beta_dot_pairs = list(zip(apts.flatten(), bpts.flatten()))\n",
    "\n",
    "#alpha_dot_beta_dot_pairs = zip(np.linspace(-1, 1, 101)*alpha_dot, np.linspace(1, 1, 101)*beta_dot)\n",
    "\n",
    "cluster_set_dict, tps = single_detection_clusters(df, xyz_proj, magIDDict, \n",
    "                              gamma, gamma_dot, alpha_dot_beta_dot_pairs,\n",
    "                              thresh, rad, GM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I want to alter this so that it does the full range of alpha_dot and beta_dot for a given value or values of gamma\n",
    "# and a given step size in alpha_dot and beta_dot.\n",
    "\n",
    "def single_detection_clusters(df, xyz_proj, magIDDict, \n",
    "                              gamma, gamma_dot, delta_alpha_dot, delta_beta_dot,\n",
    "                              thresh, rad, GM):\n",
    "    \n",
    "    # Given gamma, figure out the maximum values of alpha_dot and beta_dot\n",
    "    \n",
    "    ntotal = np.sum([v.shape[0] for key, v in xyz_proj.items()])\n",
    "        \n",
    "    theta  = np.zeros((ntotal, 2))\n",
    "    re     = np.zeros((ntotal, 3))\n",
    "    points = np.zeros((ntotal, 2))\n",
    "    points0 = np.zeros((ntotal, 2))    \n",
    "    fs     = np.zeros(ntotal)\n",
    "    \n",
    "    fakeIDs = []\n",
    "\n",
    "    # choose the range of alpha_dot and beta_dot based on gamma\n",
    "    \n",
    "    # choose gamma\n",
    "    # compute tp\n",
    "\n",
    "    # This can be done up front.\n",
    "    t0 = df['JD_OBS'].mean()\n",
    "\n",
    "    ts = np.zeros(ntotal)\n",
    "\n",
    "    # Iterate over the exposures to set up the data\n",
    "    i=0\n",
    "    for expnum, t, xe, ye, ze in zip(df['EXPNUM'], df['JD_OBS'], df['xe_p'], df['ye_p'], df['ze_p']):\n",
    "        \n",
    "        # Iterate over the detections in each exposure, filling in some arrays\n",
    "        for row, row2 in zip(xyz_proj[expnum], magIDDict[expnum]):\n",
    "            xp, yp, zp = row\n",
    "            theta_x = xp/zp\n",
    "            theta_y = yp/zp\n",
    "            _, _, fakeID = row2\n",
    "            \n",
    "            ts[i] = t-t0\n",
    "\n",
    "            theta[i][0] = theta_x\n",
    "            theta[i][1] = theta_y\n",
    "            \n",
    "            re[i][0] = xe\n",
    "            re[i][1] = ye\n",
    "            re[i][2] = ze\n",
    "            \n",
    "            fakeIDs.append(fakeID)\n",
    "            \n",
    "            i += 1\n",
    "\n",
    "    # Now we have arrays of theta, re, and ts, but those could actually be the input \n",
    "    # to the routine.\n",
    "    \n",
    "    # The gamma loop starts here:\n",
    "    \n",
    "    angular_rate_max = np.sqrt(2.0 * GM * gamma**3)\n",
    "    \n",
    "    na = ((angular_rate_max // delta_alpha_dot) + 1)\n",
    "    alpha_dot_max = na * delta_alpha_dot\n",
    "    \n",
    "    nb = ((angular_rate_max // delta_beta_dot) + 1)\n",
    "    beta_dot_max = nb * delta_beta_dot\n",
    "    \n",
    "    alpha_dots = np.linspace(-alpha_dot_max, alpha_dot_max, 2*na - 1)\n",
    "    beta_dots  = np.linspace(-beta_dot_max, beta_dot_max, 2*nb - 1)\n",
    "    \n",
    "    apts, bpts = np.meshgrid(alpha_dots, beta_dots)\n",
    "    alpha_dot_beta_dot_pairs = list(zip(apts.flatten(), bpts.flatten()))\n",
    "    \n",
    "    print(len(alpha_dots), len(beta_dots), len(alpha_dot_beta_dot_pairs))\n",
    "    \n",
    "    tp = ts-1.0/(gamma*c)\n",
    "    \n",
    "\n",
    "    # The gamma_dot loop starts here.\n",
    "    # The range of gamma_dot depends upon the value of gamma\n",
    "    \n",
    "    f_arr = 1.0 - 0.5*GM * gamma**3 * tp*tp - gamma * re[:, 2] + gamma_dot * tp\n",
    "    h_arr = 1.0 - 0.5*GM * gamma**3 * tp*tp\n",
    "    \n",
    "    points0 = theta*f_arr[:, np.newaxis] + gamma*re[:, 0:2]\n",
    "    \n",
    "    # The unshifted array points0 is now established for the given gamma, gamma_dot.  \n",
    "    \n",
    "    # Now carry out a series of alpha_dot, beta_dot shifts,\n",
    "    # building a KD-tree and searching for clusters after each shift.\n",
    "    # The range of alpha_dot and beta_dot depends upon the value of gamma.\n",
    "\n",
    "    cluster_set_dict=defaultdict(set)\n",
    "    \n",
    "    for alpha_dot, beta_dot in alpha_dot_beta_dot_pairs:\n",
    "        points[:, 0] = (points0[:, 0] - alpha_dot*tp)/h_arr\n",
    "        points[:, 1] = (points0[:, 1] -  beta_dot*tp)/h_arr\n",
    "    \n",
    "        #cluster_set = set()\n",
    "\n",
    "        tree = scipy.spatial.cKDTree(points)\n",
    "        matches = tree.query_ball_tree(tree, rad)\n",
    "        for j, match in enumerate(matches):\n",
    "            if len(match)>=thresh:\n",
    "                cluster_set_dict[tuple(match)].update({(alpha_dot, beta_dot)})\n",
    "\n",
    "        #cluster_set_dict[(alpha_dot, beta_dot)] = cluster_set\n",
    "\n",
    "    return cluster_set_dict, tp, fakeIDs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "thresh = 6\n",
    "rad = (np.pi/180.)*(20./3600.)\n",
    "\n",
    "gamma = 0.00314923 \n",
    "gamma_dot = 0.00068910/365.25\n",
    "\n",
    "gamma = 0.001\n",
    "gamma_dot = 0.0\n",
    "\n",
    "delta_alpha_dot = 1.2e-7\n",
    "delta_beta_dot  = 1.2e-7\n",
    "\n",
    "cluster_set_dict, tps, fakeIDs = single_detection_clusters(df, xyz_proj, magIDDict, \n",
    "                              gamma, gamma_dot, delta_alpha_dot, delta_beta_dot,\n",
    "                              thresh, rad, GM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now alter this to do a set of gamma and gamma_dot pairs. \n",
    "\n",
    "def single_detection_clusters(df, xyz_proj, magIDDict, \n",
    "                              gamma_gamma_dot_pairs, delta_alpha_dot, delta_beta_dot,\n",
    "                              thresh, rad, GM):\n",
    "    \n",
    "    # Given gamma, figure out the maximum values of alpha_dot and beta_dot\n",
    "    \n",
    "    ntotal = np.sum([v.shape[0] for key, v in xyz_proj.items()])\n",
    "        \n",
    "    theta  = np.zeros((ntotal, 2))\n",
    "    re     = np.zeros((ntotal, 3))\n",
    "    points = np.zeros((ntotal, 2))\n",
    "    points0 = np.zeros((ntotal, 2))    \n",
    "    fs     = np.zeros(ntotal)\n",
    "    \n",
    "    fakeIDs = []\n",
    "\n",
    "    # choose the range of alpha_dot and beta_dot based on gamma\n",
    "    \n",
    "    # choose gamma\n",
    "    # compute tp\n",
    "\n",
    "    # This can be done up front.\n",
    "    t0 = df['JD_OBS'].mean()\n",
    "    #t0 = df['JD_OBS'].mean() - 100.0\n",
    "\n",
    "    ts = np.zeros(ntotal)\n",
    "\n",
    "    # Iterate over the exposures to set up the data\n",
    "    i=0\n",
    "    for expnum, t, xe, ye, ze in zip(df['EXPNUM'], df['JD_OBS'], df['xe_p'], df['ye_p'], df['ze_p']):\n",
    "        \n",
    "        # Iterate over the detections in each exposure, filling in some arrays\n",
    "        for row, row2 in zip(xyz_proj[expnum], magIDDict[expnum]):\n",
    "            xp, yp, zp = row\n",
    "            theta_x = xp/zp\n",
    "            theta_y = yp/zp\n",
    "            _, _, fakeID = row2\n",
    "            \n",
    "            ts[i] = t-t0\n",
    "\n",
    "            theta[i][0] = theta_x\n",
    "            theta[i][1] = theta_y\n",
    "            \n",
    "            re[i][0] = xe\n",
    "            re[i][1] = ye\n",
    "            re[i][2] = ze\n",
    "            \n",
    "            fakeIDs.append(fakeID)\n",
    "            \n",
    "            i += 1\n",
    "    \n",
    "    print(theta[:, 0].mean(), theta[:, 0].min(), theta[:, 0].max(), theta[:, 1].mean(), theta[:, 1].min(), theta[:, 1].max())\n",
    "\n",
    "    # Now we have arrays of theta, re, and ts, but those could actually be the input \n",
    "    # to the routine.\n",
    "    \n",
    "    for gamma, gamma_dot in gamma_gamma_dot_pairs:\n",
    "        \n",
    "        tp = ts-1.0/(gamma*c)\n",
    "\n",
    "        # I don't think the signs in front of the GM terms are correct as minus.  Trying plus.\n",
    "        f_arr = 1.0 - 0.5*GM * gamma**3 * tp*tp - gamma * re[:, 2] + gamma_dot * tp\n",
    "        h_arr = 1.0 - 0.5*GM * gamma**3 * tp*tp\n",
    "    \n",
    "        points0 = theta*f_arr[:, np.newaxis] + gamma*re[:, 0:2]\n",
    "        \n",
    "        # The unshifted array points0 is now established for the given gamma, gamma_dot.  \n",
    "    \n",
    "        # Now carry out a series of alpha_dot, beta_dot shifts,\n",
    "        # building a KD-tree and searching for clusters after each shift.\n",
    "        # The range of alpha_dot and beta_dot depends upon the value of gamma.\n",
    "    \n",
    "        angular_rate_max = np.sqrt(2.0 * GM * gamma**3)\n",
    "    \n",
    "        na = ((angular_rate_max // delta_alpha_dot) + 1)\n",
    "        alpha_dot_max = na * delta_alpha_dot\n",
    "    \n",
    "        nb = ((angular_rate_max // delta_beta_dot) + 1)\n",
    "        beta_dot_max = nb * delta_beta_dot\n",
    "    \n",
    "        alpha_dots = np.linspace(-alpha_dot_max, alpha_dot_max, 2*na - 1)\n",
    "        beta_dots  = np.linspace(-beta_dot_max, beta_dot_max, 2*nb - 1)\n",
    "    \n",
    "        apts, bpts = np.meshgrid(alpha_dots, beta_dots)\n",
    "        alpha_dot_beta_dot_pairs = list(zip(apts.flatten(), bpts.flatten()))\n",
    "        \n",
    "        print(gamma, na, nb, len(alpha_dot_beta_dot_pairs))\n",
    "    \n",
    "        #cluster_set_dict=defaultdict(set)\n",
    "        cluster_set_dict=defaultdict(int)\n",
    "    \n",
    "        for alpha_dot, beta_dot in alpha_dot_beta_dot_pairs:\n",
    "            points[:, 0] = (points0[:, 0] - alpha_dot*tp)/h_arr\n",
    "            points[:, 1] = (points0[:, 1] -  beta_dot*tp)/h_arr\n",
    "\n",
    "            #print(points[:, 0].min(), points[:, 0].max(), points[:, 1].min(), points[:, 1].max())\n",
    "            \n",
    "            tree = scipy.spatial.cKDTree(points)\n",
    "            \n",
    "            matches = tree.query_ball_tree(tree, rad)\n",
    "            for j, match in enumerate(matches):\n",
    "                if len(match)>=thresh:\n",
    "                    #cluster_set_dict[tuple(match)].update({(gamma, gamma_dot, alpha_dot, beta_dot)})\n",
    "                    cluster_set_dict[tuple(match)] += 1\n",
    "\n",
    "    return cluster_set_dict, tp, fakeIDs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "thresh = 5\n",
    "rad = (np.pi/180.)*(50./3600.)\n",
    "\n",
    "#gamma = 0.00314923 \n",
    "#gamma_dot = 0.00068910/365.25\n",
    "\n",
    "#gamma_dot = 0.0\n",
    "\n",
    "#gamma_dot = 2.74588074e-07 \n",
    "\n",
    "#alpha = 1.26587450e-01     \n",
    "#alpha_dot = 2.41858516e-07 \n",
    "#beta = -7.61819057e-02    \n",
    "#beta_dot = -2.15609750e-07\n",
    "#gamma = 8.48034701e-04\n",
    "#gamma_dot = 2.74588074e-07   \n",
    "\n",
    "gamma = 0.0\n",
    "gamma_dot = 0.0 \n",
    "\n",
    "gamma_gamma_dot_pairs = list(map(lambda x: (x + gamma, gamma_dot), np.arange(0e-4, 21e-4, 2.0e-4)))\n",
    "\n",
    "#gamma_gamma_dot_pairs = [(gamma, gamma_dot)]\n",
    "\n",
    "delta_alpha_dot = 2.0e-7\n",
    "delta_beta_dot  = 2.0e-7\n",
    "\n",
    "cluster_set_dict, tps, fakeIDs = single_detection_clusters(df, xyz_proj, magIDDict, \n",
    "                              gamma_gamma_dot_pairs, delta_alpha_dot, delta_beta_dot,\n",
    "                              thresh, rad, GM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpts, ypts = np.meshgrid(np.linspace(-0.15, 0.15, 101), np.linspace(-0.12, 0.12, 101))\n",
    "xpts_ypts_pairs = list(zip(xpts.flatten(), ypts.flatten()))\n",
    "grid = np.array(xpts_ypts_pairs)\n",
    "qtree = scipy.spatial.cKDTree(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(xpts_ypts_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _ = plt.hist(list(cluster_set_dict.values()), bins=100, range=(0, 200))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('cluster_results.out', 'wb') as handle:\n",
    "                pickle.dump(cluster_set_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using different t0 values to disentangle contaminated clusters.\n",
    "\n",
    "Why might it be difficult to find particular objects:\n",
    "\n",
    "1. Not very many detections\n",
    "\n",
    "2. Detections spread out in time\n",
    "\n",
    "3. Solution overlapping that of another object\n",
    "\n",
    "4. Object far from the center of the tangent plane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(int)\n",
    "for cluster, val in cluster_set_dict.items():\n",
    "    if len(cluster) >= 5:\n",
    "        '''\n",
    "        for pair in sorted(val):\n",
    "            print(pair)\n",
    "        '''\n",
    "        time_list = []\n",
    "        for i, elt in enumerate(cluster):\n",
    "            print(i, elt, fakeIDs[elt], tps[elt])\n",
    "            time_list.append(tps[elt])\n",
    "        id_set = set([fakeIDs[elt] for elt in cluster])\n",
    "        if len(id_set) == 1:\n",
    "            print('pure:', id_set, max(time_list)-min(time_list))\n",
    "            id = list(id_set)[0]\n",
    "            if len(cluster) > results[id]:\n",
    "                results[id] = len(cluster)\n",
    "        else:\n",
    "            print('contaminated:', id_set)\n",
    "        #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total = 0\n",
    "got = 0\n",
    "for_plot = []\n",
    "for key, params in parameters_dict.items():\n",
    "    if params and params['gamma'] < 0.0022:\n",
    "        total += 1\n",
    "        v = 206265*np.sqrt(params['alpha_dot']*params['alpha_dot']+params['beta_dot']*params['beta_dot'])\n",
    "        if not key in results:\n",
    "            print(key, params['alpha'], params['alpha_dot'], params['beta'], params['beta_dot'], params['gamma'], params['gamma_dot'], params['nobs'], params['dur'])\n",
    "            for_plot.append((params['alpha'], params['beta'], params['gamma_dot'], params['dur'], v, 0))\n",
    "        else:\n",
    "            got += 1\n",
    "            print(key, params['alpha'], params['alpha_dot'], params['beta'], params['beta_dot'], params['gamma'], params['gamma_dot'], params['nobs'], params['dur'], 'got')\n",
    "            #print(key, params['gamma'], params['gamma_dot'], params['alpha'], params['beta'], params['nobs'], params['dur'], 'got')\n",
    "            for_plot.append((params['alpha'], params['beta'], params['gamma_dot'], params['dur'], v, 1))\n",
    "            pass\n",
    "\n",
    "for_plot = np.array(for_plot)\n",
    "got, total, got/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(for_plot[:,0], for_plot[:,1], c=for_plot[:,5], s=10+30*(1-for_plot[:,5]))\n",
    "#plt.xlim(-0.2, 0.2)\n",
    "#plt.ylim(-0.2, 0.2)\n",
    "#fig=plt.figure(figsize=(18, 18), dpi= 80, facecolor='w', edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(for_plot[:,3], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(for_plot[:,4], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(206265*np.abs(for_plot[:,2]), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(for_plot[:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to loop over a few adjacent reference vectors, repeating the search, to see if the results are more complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "thresh = 5\n",
    "rad = (np.pi/180.)*(50./3600.)\n",
    "\n",
    "#gamma = 0.00314923 \n",
    "#gamma_dot = 0.00068910/365.25\n",
    "\n",
    "#gamma_dot = 0.0\n",
    "\n",
    "#gamma_dot = 2.74588074e-07 \n",
    "\n",
    "#alpha = 1.26587450e-01     \n",
    "#alpha_dot = 2.41858516e-07 \n",
    "#beta = -7.61819057e-02    \n",
    "#beta_dot = -2.15609750e-07\n",
    "#gamma = 8.48034701e-04\n",
    "#gamma_dot = 2.74588074e-07   \n",
    "\n",
    "gamma = 0.0\n",
    "gamma_dot = 0.0 \n",
    "\n",
    "gamma_gamma_dot_pairs = list(map(lambda x: (x + gamma, gamma_dot), np.arange(0e-4, 21e-4, 2.0e-4)))\n",
    "\n",
    "#gamma_gamma_dot_pairs = [(gamma, gamma_dot)]\n",
    "\n",
    "delta_alpha_dot = 1.2e-7\n",
    "delta_beta_dot  = 1.2e-7\n",
    "\n",
    "nside=4\n",
    "angDeg = 15\n",
    "xp, yp, zp = ref_vec\n",
    "pix = hp.vec2pix(nside, xp, yp, zp, nest=True)\n",
    "vec = hp.pix2vec(nside, pix, nest=True)\n",
    "neighbors = hp.query_disc(nside, vec, angDeg*np.pi/180., inclusive=True, nest=True)\n",
    "\n",
    "cluster_results = {}\n",
    "\n",
    "re = df[['xe_ec', 'ye_ec', 'ze_ec']].as_matrix()\n",
    "\n",
    "for pix in neighbors:\n",
    "    print(pix)\n",
    "    vec = hp.pix2vec(nside, pix, nest=True)\n",
    "    mat = xyz_to_proj_matrix(vec)\n",
    "\n",
    "    re_proj = np.matmul(re, mat.T)\n",
    "\n",
    "    df['xe_p'] = re_proj[:,0]\n",
    "    df['ye_p'] = re_proj[:,1]\n",
    "    df['ze_p'] = re_proj[:,2]\n",
    "\n",
    "    xyz_proj = defaultdict(np.array)\n",
    "    for expnum in justThese:\n",
    "        raDec = np.array(raDecDict[expnum])\n",
    "        if raDec.shape[0]>0:\n",
    "            xyz = raDec_to_xyz(raDec)\n",
    "            xyz_ec = equatorial_to_ecliptic_v2(xyz)\n",
    "            xyz_proj[expnum] = np.matmul(xyz_ec, mat.T)\n",
    "            \n",
    "    cluster_results[pix], tps, fakeIDs = single_detection_clusters(df, xyz_proj, magIDDict, \n",
    "                                                                   gamma_gamma_dot_pairs, \n",
    "                                                                   delta_alpha_dot, delta_beta_dot, thresh, rad, GM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for pix in neighbors:\n",
    "    results[pix] = defaultdict(int)\n",
    "    for cluster, val in cluster_results[pix].items():\n",
    "        if len(cluster) >= 5:\n",
    "            time_list = []\n",
    "            for i, elt in enumerate(cluster):\n",
    "                #print(i, elt, fakeIDs[elt], tps[elt])\n",
    "                time_list.append(tps[elt])\n",
    "            id_set = set([fakeIDs[elt] for elt in cluster])\n",
    "            if len(id_set) == 1:\n",
    "                print('pure:', id_set, max(time_list)-min(time_list))\n",
    "                id = list(id_set)[0]\n",
    "                if len(cluster) > results[pix][id]:\n",
    "                    results[pix][id] = len(cluster)\n",
    "            else:\n",
    "                print('contaminated:', id_set)\n",
    "        #print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pix in neighbors:\n",
    "    total = 0\n",
    "    got = 0\n",
    "    for_plot = []\n",
    "    for key, params in parameters_dict.items():\n",
    "        if params and params['gamma'] < 0.0022:\n",
    "            total += 1\n",
    "            v = 206265*np.sqrt(params['alpha_dot']*params['alpha_dot']+params['beta_dot']*params['beta_dot'])\n",
    "            if not key in results[pix]:\n",
    "                print(key, params['gamma'], 206265*params['gamma_dot'], params['alpha'], params['beta'], params['nobs'], params['dur'], v)\n",
    "                for_plot.append((params['alpha'], params['beta'], params['gamma_dot'], params['dur'], v, 0))\n",
    "            else:\n",
    "                got += 1\n",
    "                #print(key, params['gamma'], params['gamma_dot'], params['alpha'], params['beta'], params['nobs'], params['dur'], 'got')\n",
    "                for_plot.append((params['alpha'], params['beta'], params['gamma_dot'], params['dur'], v, 1))\n",
    "                pass\n",
    "\n",
    "    for_plot = np.array(for_plot)\n",
    "    print(got, total, got/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like it is basically working.  That is, clusters of points are being formed for some objects, for the chosen parameters.  Now I need to:\n",
    "\n",
    "1. Include a clustering algorithm.  This includes a mechanism to remove duplicate and subset clusters, as well as to defend again duplicate times.\n",
    "2. Include a means to check the IDs of the objects associated with the detections in order to see if the clustering is actually working with a low level of contamination.\n",
    "3. Iterate over multiple parameters.\n",
    "4. Move on to a data set with a realistic number of false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df['EXPNUM', 'xe_ec']:\n",
    "    print(row)\n",
    "    expnum, xe_ec = row\n",
    "    print(expnum, xe_ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raDec = np.array(raDecDict[471041])\n",
    "xyz = raDec_to_xyz(raDec)\n",
    "xyz_ec = equatorial_to_ecliptic_v2(xyz)\n",
    "xyz_proj = np.matmul(xyz_ec, mat.T)\n",
    "xyz_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['EXPNUM']==471040]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['xc'] = xyz[:,0]\n",
    "df['yc'] = xyz[:,1]\n",
    "df['zc'] = xyz[:,2]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is unnecessary\n",
    "def exposure_center(radec_list):\n",
    "    raDeg = np.array(list(map(lambda x: x[0], radec_list)))\n",
    "    decDeg = np.array(list(map(lambda x: x[1], radec_list)))\n",
    "    x = np.cos(decDeg*np.pi/180.)*np.cos(raDeg*np.pi/180.)\n",
    "    y = np.cos(decDeg*np.pi/180.)*np.sin(raDeg*np.pi/180.)  \n",
    "    z = np.sin(decDeg*np.pi/180.)\n",
    "    xm = x.mean()\n",
    "    ym = y.mean()\n",
    "    zm = z.mean()\n",
    "    l = np.sqrt(xm*xm + ym*ym + zm*zm)\n",
    "    return xm/l, ym/l, zm/l\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not necessary\n",
    "exposureDict = defaultdict(dict)\n",
    "filename = 'TNOfakes_exposureinfo.csv'\n",
    "with open(filename, newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i==0 or i==1:\n",
    "            print(row)\n",
    "        key = int(row['EXPNUM'])\n",
    "        exposureDict[key]['ra_center'] = float(row['RADEG'])\n",
    "        exposureDict[key]['dec_center'] = float(row['DECDEG'])        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for expID, exposureD in exposureDict.items():\n",
    "    jd_utc = exposureD['JD']\n",
    "    exposureDict[expID]['obs_pos'] = Observatories.getObservatoryPosition('W84', jd_utc)\n",
    "    #xp, yp, zp = exposure_center(raDecDict[expID])\n",
    "    exposureDict[expID]['exp_center'] = np.array((xp, yp, zp))\n",
    "    exposureDict[expID]['detections'] = np.array(raDecDict[expID])\n",
    "    exposureDict[expID]['magIDs'] = magIDDict[expID]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposureDict[471041]['detections']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposureDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = np.array(exposureDict[471041]['detections'])\n",
    "\n",
    "raDec_to_xyz(foo).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.matmul(raDec_to_xyz(foo), rot_mat.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equatorial_to_ecliptic(raDec_to_xyz(foo)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raDec_to_xyz(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equatorial_to_ecliptic_v2(raDec_to_xyz(np.array(exposureDict[471041]['detections'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nside=8\n",
    "pixels = set()\n",
    "centers = np.zeros((len(exposureDict), 3))\n",
    "for i, expnum in enumerate(sorted(exposureDict.keys())):\n",
    "    xp, yp, zp = exposure_center(raDecDict[expnum])\n",
    "    centers[i] = np.array((xp, yp, zp))\n",
    "    pix = hp.vec2pix(nside, xp, yp, zp, nest=True)\n",
    "    pixels.add(pix)\n",
    "    print(expnum, (xp, yp, zp), pix)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xm, ym, zm = centers.mean(axis=0)\n",
    "pix = hp.vec2pix(nside, xm, ym, zm, nest=True)\n",
    "vec = hp.pix2vec(nside, pix, nest= True)\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(list(exposureDict.values())).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(sorted(exposureDict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.cos(decDeg*np.pi/180.)*np.cos(raDeg*np.pi/180.)\n",
    "y = np.cos(decDeg*np.pi/180.)*np.sin(raDeg*np.pi/180.)  \n",
    "z = np.sin(decDeg*np.pi/180.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "40000/768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the MPC Obs80 format to our internal format\n",
    "\n",
    "Below are routines that read the files after they have been split into their respective formats.  \n",
    "\n",
    "Here I am generating a version of the MPC data that includes just the few string-like fields at the start of each line, followed by jd_tdb, the unit vector to the observed target, and the heliocentric position vector of the observatory at that time.\n",
    "\n",
    "CS0051 had a typo on the magnitude, a missing '.' in the number.  I corrected that manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertObs80(line):\n",
    "    objName   = line[0:5]\n",
    "    provDesig = line[5:12]\n",
    "    disAst    = line[12:13]\n",
    "    note1     = line[13:14]\n",
    "    note2     = line[14:15]\n",
    "    dateObs   = line[15:32]\n",
    "    RA        = line[32:44]\n",
    "    Dec       = line[44:56]\n",
    "    mag       = line[65:70]\n",
    "    filt      = line[70:71]\n",
    "    obsCode   = line[77:80]\n",
    "    return objName, provDesig, disAst, note1, note2, dateObs, RA, Dec, mag, filt, obsCode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This rotation is taking things from equatorial to ecliptic\n",
    "rot_mat = MPC_library.rotate_matrix(-MPC_library.Constants.ecl)\n",
    "def equatorial_to_ecliptic(v, rot_mat=MPC_library.rotate_matrix(-MPC_library.Constants.ecl)):\n",
    "    return np.dot(rot_mat, v.reshape(-1, 1)).flatten()\n",
    "\n",
    "def equatorial_to_ecliptic_v2(v, rot_mat=MPC_library.rotate_matrix(-MPC_library.Constants.ecl)):\n",
    "    return np.matmul(v, rot_mat.T)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equatorial_to_ecliptic(np.array((0, -1, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equatorial_to_ecliptic_v2(np.array((0, 1, 0)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This is inactivated because the results have already been generated and don't need to be redone.\n",
    "#\n",
    "with open('data/itf_new_1_line_ec.mpc', 'w') as outfile:\n",
    "    with open('data/itf_new_1_line.txt', 'r') as f:\n",
    "        outstring = \"#trackletID yr   mn dy      obsCode mag filter  jd_tdb       x_target     y_target     z_target      x_obs       y_obs        z_obs     \\n\"\n",
    "        outfile.write(outstring)\n",
    "        for line in f:\n",
    "            objName, provDesig, disAst, note1, note2, dateObs, RA, Dec, mag, filt, obsCode = convertObs80(line)\n",
    "            jd_utc = MPC_library.date2JD(dateObs)\n",
    "            jd_tdb  = MPC_library.EOP.jdTDB(jd_utc)\n",
    "            raDeg, decDeg = MPC_library.RA2degRA(RA), MPC_library.Dec2degDec(Dec)\n",
    "            x = np.cos(decDeg*np.pi/180.)*np.cos(raDeg*np.pi/180.)\n",
    "            y = np.cos(decDeg*np.pi/180.)*np.sin(raDeg*np.pi/180.)  \n",
    "            z = np.sin(decDeg*np.pi/180.)\n",
    "            xec, yec, zec = equatorial_to_ecliptic(np.array((x, y, z)))\n",
    "            \n",
    "            if filt.isspace():\n",
    "                filt = '-'\n",
    "            if mag.isspace():\n",
    "                mag = '----'\n",
    "            xh, yh, zh = Observatories.getObservatoryPosition(obsCode, jd_utc)\n",
    "            xhec, yhec, zhec = equatorial_to_ecliptic(np.array((xh, yh, zh)))\n",
    "            #outstring = \"%11s %s %4s %5s %s %13.6lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf\\n\"% \\\n",
    "            #      (provDesig, dateObs, obsCode, mag, filt, jd_tdb, x, y, z, xh, yh, zh, xec, yec, zec, xhec, yhec, zhec)\n",
    "            outstring = \"%11s %s %4s %5s %s %13.6lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf\\n\"% \\\n",
    "                (provDesig, dateObs, obsCode, mag, filt, jd_tdb, xec, yec, zec, xhec, yhec, zhec)\n",
    "            outfile.write(outstring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the files into tracklet structure\n",
    "\n",
    "All the routines below are predicated on tracklets.  \n",
    "\n",
    "The criteria for a tracklet are:\n",
    "\n",
    "1) The same obsCode\n",
    "\n",
    "2) The same night (within +/- 0.5 day)\n",
    "\n",
    "3) The same trkSub (observer-assigned trackletID)\n",
    "\n",
    "We make a tracklet key from the tuple (trkSub, dayNumber, obsCode), where dayNumber is the truncated MJD.\n",
    "\n",
    "\n",
    "The basic processing pattern is to \n",
    "1. Organize and sort the tracklets that are in an input file\n",
    "2. Separate the tracklets into time windows, excluding singleton tracklets.  The default is +/- 15 days.\n",
    "\n",
    "These steps only need to be done once, and after these steps the time-separated *.mpc files are not needed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look for time gaps > dt in a sorted list of times\n",
    "def segment_times(times, dt):\n",
    "    result =[]\n",
    "    t0 = times[0]\n",
    "    for i, t in enumerate(times):\n",
    "        if np.abs(t-t0)>dt:\n",
    "            result.append(i)\n",
    "        t0 = t\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return the number of sequential comment lines that were skipped at the\n",
    "# beginning of a file.\n",
    "def count_skip_lines(comment_char, filename):\n",
    "    with open(filename) as infile:\n",
    "        i = 0\n",
    "        line = infile.readline()\n",
    "        while line.startswith(comment_char):\n",
    "            line = infile.readline()\n",
    "            i += 1\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This routine is fundamental.  It takes both the name of the\n",
    "# file that contains the data in the original MPC format and\n",
    "# same file in our format, then it goes through and identifies\n",
    "# tracklets according to a set of criteria.\n",
    "#\n",
    "# The criteria for a tracklet are:\n",
    "# 1) The same obsCode\n",
    "# 2) The same night (within +/- 0.5 day)\n",
    "# 3) The same trkSub (observer-assigned trackletID)\n",
    "#\n",
    "# It returns a dictionary of tracklets for which the keys\n",
    "# are a 3-part tuple composed of a trackletID, a day number,\n",
    "# and an obsCode; a dictionary of the start times of those tracklets,\n",
    "# using the same keys; and a list of the 3-part keys in the order\n",
    "# of those start times.\n",
    "def get_sorted_tracklets_v5(itf_filename_orig, itf_filename):\n",
    "\n",
    "    # Skip the header lines\n",
    "    skip_count = count_skip_lines('#', itf_filename_orig)\n",
    "    infile_orig = open(itf_filename_orig)\n",
    "    for _ in range(skip_count):\n",
    "        infile_orig.readline()\n",
    "        \n",
    "    skip_count = count_skip_lines('#', itf_filename)\n",
    "    infile = open(itf_filename)\n",
    "    for _ in range(skip_count):\n",
    "        infile.readline()\n",
    "\n",
    "    temp_tracklets = defaultdict(list)\n",
    "    \n",
    "    for line, line_orig in zip(infile, infile_orig):\n",
    "        # The next three lines should be replaced with\n",
    "        # more general methods\n",
    "        trackletID = line[0:12]\n",
    "        jd_tdb = float(line[43:57])\n",
    "        obsCode = line[31:34]\n",
    "\n",
    "        # As an intermediate step, we are gathering\n",
    "        # tracklets that might span multiple nights.\n",
    "        temp_key = (trackletID.strip(), obsCode)\n",
    "        temp_tracklets[temp_key].append((line, line_orig))\n",
    "  \n",
    "    # Now iterate over the sets of gathered lines, converting them to sets\n",
    "    # and back to lists to eliminate exact duplicates\n",
    "    for key in temp_tracklets.keys():\n",
    "        temp_tracklets[key]= list(set(temp_tracklets[key]))\n",
    "\n",
    "    # Now iterate over the sets of gathered lines, sorting the lines, and\n",
    "    # splitting them up into tracklets that meet the criteria.\n",
    "    tracklets = defaultdict(list)\n",
    "    tracklets_jd_dict = {}\n",
    "\n",
    "    for temp_key, v in temp_tracklets.items():\n",
    "\n",
    "        trackletID = temp_key[0]\n",
    "        obsCode = temp_key[1]\n",
    "\n",
    "        sortedLines = sorted(v, key=lambda k: float(k[0][43:57]))\n",
    "        sortedTimes = sorted([float(k[0][43:57]) for k in sortedLines])\n",
    "                                        \n",
    "        dt = sortedTimes[-1] - sortedTimes[0]\n",
    "        # if the time span is long, split the tracklet\n",
    "        idx0 = 0\n",
    "        if dt>0.5:\n",
    "            # This looks like an error\n",
    "            idxs = segment_times(sortedTimes, 0.5)\n",
    "            # if len(idxs) >= 0: do something\n",
    "            # if len(idxs) > 0: do a thing\n",
    "            for idx in idxs:\n",
    "                jd_tdb = sortedTimes[idx0]\n",
    "                mjdp = int(jd_tdb-2400000.5)\n",
    "                key = (trackletID, mjdp, obsCode)\n",
    "                if key not in tracklets_jd_dict:\n",
    "                    # Got a new tracklet\n",
    "                    tracklets_jd_dict[key] = jd_tdb\n",
    "                tracklets[key].extend(sortedLines[idx0:idx])\n",
    "                idx0=idx\n",
    "            if len(idxs)>0:\n",
    "                jd_tdb = sortedTimes[idx0]\n",
    "                mjdp = int(jd_tdb-2400000.5)\n",
    "                key = (trackletID, mjdp, obsCode)\n",
    "                if key not in tracklets_jd_dict:\n",
    "                    # Got a new tracklet\n",
    "                    tracklets_jd_dict[key] = jd_tdb\n",
    "                tracklets[key].extend(sortedLines[idx0:])\n",
    "                idx0=idx\n",
    "        # If the time span is not long, just insert the\n",
    "        # lines and start time into the data structures\n",
    "        else:\n",
    "            jd_tdb = sortedTimes[idx0]\n",
    "            mjdp = int(jd_tdb-2400000.5)\n",
    "            key = (trackletID, mjdp, obsCode)\n",
    "            if key not in tracklets_jd_dict:\n",
    "                # Got a new tracklet\n",
    "                tracklets_jd_dict[key] = jd_tdb\n",
    "            tracklets[key].extend(sortedLines)\n",
    "\n",
    "    # We changed this on 20 Mar 2018 to exclude singleton tracklets.  \n",
    "    # The line below was active before, including the singleton tracklets.\n",
    "    trackletKeys = [k for k in tracklets.keys() if len(tracklets[k])>1]\n",
    "    #trackletKeys = [k for k in tracklets.keys()]\n",
    "    sortedTrackletKeys = sorted(trackletKeys, key=lambda k: tracklets_jd_dict[k]) \n",
    "    return tracklets, tracklets_jd_dict, sortedTrackletKeys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This routine separates tracklets into time windows (centered on a list of times within a time range) \n",
    "# and saves the results in pickle files.\n",
    "def separate_time_windows_v5(tracklets, sortedTracklets, tracklets_jd_dict, time_centers, file_stem='data/itf_new_1_line_ec.mpc', dt=15., suff='.mpc'):\n",
    "\n",
    "    files = {}\n",
    "\n",
    "    header='#trackletID yr   mn dy      obsCode mag filter  jd_tdb       x_target     y_target     z_target      x_obs       y_obs        z_obs     '\n",
    "\n",
    "    jds = [tracklets_jd_dict[k] for k in sortedTracklets]\n",
    "    \n",
    "    for t in time_centers:\n",
    "                \n",
    "        i = bisect.bisect(jds, t-dt, 0, len(jds))\n",
    "        j = bisect.bisect(jds, t+dt, 0, len(jds))\n",
    "        \n",
    "        if j>i:\n",
    "            \n",
    "            tmp_tracklets={}\n",
    "            tmp_sortedTracklets=sortedTracklets[i:j]\n",
    "            tmp_tracklets_jd_dict={}\n",
    "            outfile = file_stem.replace('.mpc', '')+'_'+str(t)+'_pm'+str(dt)+suff\n",
    "\n",
    "            for key in tmp_sortedTracklets:\n",
    "                tmp_tracklets[key] = tracklets[key]\n",
    "                tmp_tracklets_jd_dict[key] = tracklets_jd_dict[key]\n",
    "                \n",
    "            with open(outfile, 'wb') as handle:\n",
    "                pickle.dump((tmp_tracklets, tmp_tracklets_jd_dict, tmp_sortedTracklets), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracklets_itf, tracklets_jd_dict_itf, sortedTracklets_itf = get_sorted_tracklets_v5('data/itf_new_1_line.txt', 'data/itf_new_1_line_ec.mpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracklets_itf, tracklets_jd_dict_itf, sortedTracklets_itf = get_sorted_tracklets_v5('data/itf_20180515_1_line.txt', 'data/itf_20180515_1_line_ec.mpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracklets_C, tracklets_jd_dict_C, sortedTracklets_C = get_sorted_tracklets_v5('data/UnnObs_Training_1_line_C.txt', 'data/UnnObs_Training_1_line_C_ec.mpc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to separate the data into time windows that are centered on each new moon, so here's a function that takes an integer and returns a corresponding julian date of new moon, rounded to midnight UTC.  Note that this function is approximate but good enough for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns the jd of new moon, to the nearest half day\n",
    "def lunation_center(n, tref=2457722.0125, p=29.53055):\n",
    "    t = tref + p*n\n",
    "    tp = np.floor(t) + 0.5\n",
    "    return tp\n",
    "\n",
    "time_centers_lunation = [lunation_center(n) for n in range(-825, 25)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_centers_lunation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "separate_time_windows_v5(tracklets_C, sortedTracklets_C, tracklets_jd_dict_C, time_centers_lunation, file_stem='data/UnnObs_Training_1_line_C.mpc', dt=15., suff='.pickle')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "separate_time_windows_v5(tracklets_itf, sortedTracklets_itf, tracklets_jd_dict_itf, time_centers_lunation, file_stem='data/itf_new_1_line_ec.mpc', dt=15., suff='.pickle')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "separate_time_windows_v5(tracklets_itf, sortedTracklets_itf, tracklets_jd_dict_itf, time_centers_lunation, file_stem='data/itf_20180515_1_line_ec.mpc', dt=15., suff='.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the time and vectors from a line, where the line is\n",
    "# in our format.\n",
    "def get_observation_data(line):\n",
    "    jd_tdb = float(line[43:57])\n",
    "    \n",
    "    x_target, y_target, z_target = line[57:97].split()\n",
    "    r_target = np.array([float(x_target), float(y_target), float(z_target)])\n",
    "\n",
    "    x_obs, y_obs, z_obs = line[97:135].split()            \n",
    "    r_obs = np.array([float(x_obs), float(y_obs), float(z_obs)])\n",
    "    \n",
    "    return jd_tdb, r_target, r_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This returns the topocentric distances and new heliocentric\n",
    "# position vectors to the target, given the assumed distance\n",
    "# r and the position vector of the observatory re.\n",
    "def adjust_position(r, rho_target, re):\n",
    "    rho_x, rho_y, rho_z = rho_target\n",
    "    xe, ye, ze = re\n",
    "    Robs = np.sqrt(xe * xe + ye * ye + ze * ze)\n",
    "    cos_phi = -(rho_x * xe + rho_y * ye + rho_z * ze) / Robs\n",
    "    phi = np.arccos(cos_phi)\n",
    "    sin_phi = np.sin(phi)\n",
    "\n",
    "    xx2 = r*r - Robs*sin_phi * Robs*sin_phi\n",
    "    \n",
    "    if xx2 < 0:\n",
    "        None, None\n",
    "\n",
    "    xx = np.sqrt(xx2)\n",
    "    yy = Robs * cos_phi\n",
    "    \n",
    "    rho_p = yy + xx\n",
    "\n",
    "    # This could be done with numpy arrays\n",
    "    x_p = xe + rho_p*rho_x\n",
    "    y_p = ye + rho_p*rho_y\n",
    "    z_p = ze + rho_p*rho_z\n",
    "    \n",
    "    rho_m = yy - xx\n",
    "    \n",
    "    # This could be done with numpy arrays    \n",
    "    x_m = xe + rho_m*rho_x\n",
    "    y_m = ye + rho_m*rho_y\n",
    "    z_m = ze + rho_m*rho_z\n",
    "        \n",
    "    return (rho_p, (x_p, y_p, z_p)), (rho_m, (x_m, y_m, z_m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This routine uses a specified value of gamma (g), \n",
    "# determines the HEALPix index of each tracklet, based\n",
    "# on the heliocentric transformation of the first position\n",
    "# of the tracklet for that value of g, and builds and\n",
    "# returns a dictionary keyed on HEALPix indices with values\n",
    "# being trackletIDs.\n",
    "# This routine will be used later for selecting which\n",
    "# tracklets to process.\n",
    "def index_tracklets_gamma(g, tracklets_dict, nside=8):\n",
    "    pix_dict=defaultdict(list)\n",
    "    r = 1.0/g\n",
    "    for key, value in tracklets_dict.items():\n",
    "        line, line_orig = value[0]\n",
    "        jd_tdb, r_target, r_obs = get_observation_data(line)\n",
    "        rho_r_p, rho_r_m = adjust_position(r, r_target, r_obs)\n",
    "        xp, yp, zp = rho_r_p[1]\n",
    "                \n",
    "        # Calculate HEALPix index\n",
    "        pix = hp.vec2pix(nside, xp, yp, zp, nest=True)\n",
    "        pix_dict[pix].append(key)\n",
    "\n",
    "    return pix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check again\n",
    "# This routine returns the 3-D rotation matrix for the \n",
    "# given reference vector.\n",
    "def xyz_to_proj_matrix(r_ref):\n",
    "    x_ref, y_ref, z_ref = r_ref\n",
    "    r = np.sqrt(x_ref*x_ref + y_ref*y_ref + z_ref*z_ref)\n",
    "    lon0 = np.arctan2(y_ref, x_ref)\n",
    "    lat0 = np.arcsin(z_ref/r)\n",
    "    slon0 = np.sin(lon0)\n",
    "    clon0 = np.cos(lon0)\n",
    "    slat0 = np.sin(lat0)\n",
    "    clat0 = np.cos(lat0)\n",
    "\n",
    "    mat = np.array([[-slon0, clon0, 0], \n",
    "                    [-clon0*slat0, -slon0*slat0, clat0], \n",
    "                    [clon0*clat0, slon0*clat0, slat0 ]])\n",
    "    \n",
    "    return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check again\n",
    "# This routine returns the 3-D rotation matrix for the \n",
    "# given reference vector.\n",
    "def proj_to_xyz_matrix(r_ref):\n",
    "    x_ref, y_ref, z_ref = r_ref\n",
    "    r = np.sqrt(x_ref*x_ref + y_ref*y_ref + z_ref*z_ref)\n",
    "    lon0 = np.arctan2(y_ref, x_ref)\n",
    "    lat0 = np.arcsin(z_ref/r)\n",
    "    slon0 = np.sin(lon0)\n",
    "    clon0 = np.cos(lon0)\n",
    "    slat0 = np.sin(lat0)\n",
    "    clat0 = np.cos(lat0)\n",
    "\n",
    "    mat = np.array([[-slon0, -clon0*slat0, clon0*clat0], \n",
    "                    [clon0,  -slon0*slat0, slon0*clat0], \n",
    "                    [0, clat0,  slat0 ]])\n",
    "    \n",
    "    return mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_tracklet(t_ref, g, gdot, v, GM=MPC_library.Constants.GMsun):\n",
    "    # Here's a version that incorporates radial gravitational\n",
    "    # acceleration\n",
    "    \n",
    "    t_emit = [(obs[0]-obs[1]-t_ref) for obs in v]\n",
    "    acc_z = -GM*g*g\n",
    "    fac =[(1.0 + gdot*t + 0.5*g*acc_z*t*t - g*obs[7]) for obs, t in zip(v, t_emit)]\n",
    "                \n",
    "    A = np.vstack([t_emit, np.ones(len(t_emit))]).T \n",
    "    \n",
    "    x = [obs[2]*f + obs[5]*g for obs, f in zip(v, fac)]                 \n",
    "    mx, cx = np.linalg.lstsq(A, x)[0]\n",
    "            \n",
    "    y = [obs[3]*f + obs[6]*g for obs, f in zip(v, fac)]                 \n",
    "    my, cy = np.linalg.lstsq(A, y)[0]\n",
    "    \n",
    "    return (cx, mx, cy, my, t_emit[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def fit_tracklet_grav(t_ref, g, gdot, v, GM=MPC_library.Constants.GMsun, eps2=1e-16):\n",
    "    # Here's a more sophisticated version\n",
    "        \n",
    "    t_emit = [(obs[0]-obs[1]-t_ref) for obs in v]\n",
    "        \n",
    "    # We will approximate g_x(t), g_y(t), and g_z(t)\n",
    "    # using a Taylor expansion.  \n",
    "    # The first two terms are zero by design.  \n",
    "    #\n",
    "    # Given alpha, beta, gamma,\n",
    "    # we would have r_0^2 = (alpha^2 + beta^2 + 1)*z_0^2\n",
    "    # r^2 = (alpha^2 + beta^2 + 1)/gamma^2 ~ 1/gamma^2\n",
    "    # g_x(t) ~ -0.5*GM*x_0*t^2/r^3 + 1/6*jerk_x*t*t*t\n",
    "    # g_y(t) ~ -0.5*GM*y_0*t^2/r^3 + 1/6*jerk_y*t*t*t\n",
    "    # g_z(t) ~ -0.5*GM*z_0*t^2/r^3 + 1/6*jerk_z*t*t*t\n",
    "    #\n",
    "    # We do not have alpha and beta initially,\n",
    "    # but we assert gamma.\n",
    "    # \n",
    "    # We set alpha=beta=0 initially, least squares solve\n",
    "    # the tracklets and obtain alpha, alpha-dot, beta, \n",
    "    # and beta-dot.\n",
    "    # \n",
    "    # Then we use those values to estimate g_x,\n",
    "    # g_y, and g_z for the next iteration.\n",
    "    #\n",
    "    # The process converges when alpha, alpha-dot,\n",
    "    # beta, beta-dot do not change significantly.\n",
    "    #\n",
    "    # We could also do this same process with a \n",
    "    # Kepler-stepper or a full n-body integration.\n",
    "    \n",
    "    alpha = beta = 0.0\n",
    "    alpha_dot = beta_dot = 0.0\n",
    "    cx, cy = 1.0, 1.0\n",
    "    mx, my = 0.0, 0.0\n",
    "        \n",
    "    while(((cx-alpha)*(cx-alpha) + (cy-beta)*(cy-beta))>eps2):\n",
    "            \n",
    "        alpha, beta = cx, cy\n",
    "        alpha_dot, beta_dot = mx, my\n",
    "            \n",
    "        r2 = (alpha*alpha + beta*beta + 1.0)/(g*g)\n",
    "        r3 = r2*np.sqrt(r2)\n",
    "        r5 = r2*r3\n",
    "\n",
    "        x0 = alpha/g\n",
    "        y0 = beta/g\n",
    "        z0 = 1.0/g\n",
    "            \n",
    "        vx0 = alpha_dot/g\n",
    "        vy0 = beta_dot/g\n",
    "        vz0 = gdot/g\n",
    "            \n",
    "        # An alternative to the Taylor expansion is to\n",
    "        # to kepler step from\n",
    "        # x0, y0, z0 and vx0, vy0, vz0 at time 0\n",
    "        # to those at the times of each observation \n",
    "        # in the tracklet.  With that there will be no \n",
    "        # issue of convergence.\n",
    "        # Then simply subtract off the linear motion\n",
    "        # to give the gravitational perturbation.\n",
    "            \n",
    "        rrdot = x0*vx0 + y0*vy0 + z0*vz0\n",
    "        \n",
    "        acc_x = -GM*x0/r3\n",
    "        acc_y = -GM*y0/r3\n",
    "        acc_z = -GM*z0/r3\n",
    "            \n",
    "        jerk_x = -GM/r5*(r2*vx0 - 3.0*rrdot*x0)\n",
    "        jerk_y = -GM/r5*(r2*vy0 - 3.0*rrdot*y0)\n",
    "        jerk_z = -GM/r5*(r2*vz0 - 3.0*rrdot*z0)\n",
    "\n",
    "        fac =[(1.0 + gdot*t + 0.5*g*acc_z*t*t + (1./6.0)*g*jerk_z*t*t*t - g*obs[7]) for obs, t in zip(v, t_emit)]\n",
    "                \n",
    "        A = np.vstack([t_emit, np.ones(len(t_emit))]).T \n",
    "\n",
    "        x = [obs[2]*f + obs[5]*g - 0.5*g*acc_x*t*t - (1./6.0)*g*jerk_x*t*t*t for obs, f, t in zip(v, fac, t_emit)]                 \n",
    "        mx, cx = np.linalg.lstsq(A, x)[0]\n",
    "            \n",
    "        y = [obs[3]*f + obs[6]*g - 0.5*g*acc_y*t*t - (1./6.0)*g*jerk_y*t*t*t for obs, f, t in zip(v, fac, t_emit)]                 \n",
    "        my, cy = np.linalg.lstsq(A, y)[0]\n",
    "        \n",
    "        return (cx, mx, cy, my, t_emit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am doing the same thing as the previous routine, but without files.\n",
    "#\n",
    "# It takes a reference time (t_ref), a set of z, zdot pairs (z_zdot_pairs), \n",
    "# a reference direction vector (vec), and a set of observation lines that \n",
    "# have been selected for a region of sky and time slice (lines)\n",
    "#\n",
    "# It returns a dictionary of results that have z, zdot pairs as keys and\n",
    "# sets of fitted tracklets as results.  Each result has the form:\n",
    "# \n",
    "# trackletID alpha alpha_dot beta beta_dot t_emit,\n",
    "# where t_emit is the light time-corrected time relative to the reference\n",
    "# time.  The coordinates are now in tangent plane projection.\n",
    "#\n",
    "def transform_to_arrows_v3(t_ref, g_gdot_pairs, vec, trackletKeys, tracklets_dict, fit_tracklet_func=fit_tracklet):\n",
    "        \n",
    "    #GM = MPC_library.Constants.GMsun\n",
    "\n",
    "    rot_mat = MPC_library.rotate_matrix(MPC_library.Constants.ecl)\n",
    "\n",
    "    results_dict = defaultdict(list)\n",
    "    \n",
    "    vec = np.array(vec)\n",
    "    #ecl_vec = np.dot(rot_mat, vec)\n",
    "    #vec = ecl_vec    \n",
    "    vec = vec/np.linalg.norm(vec)\n",
    "    mat = xyz_to_proj_matrix(vec)\n",
    "\n",
    "    for trackletKey in trackletKeys:\n",
    "        for line, line_orig in tracklets_dict[trackletKey]:\n",
    "            jd_tdb, r_target, r_obs = get_observation_data(line)\n",
    "\n",
    "            # Rotate to projection coordinates\n",
    "            theta_x, theta_y, theta_z = np.dot(mat, r_target)\n",
    "        \n",
    "            # Ignore theta_z after this; it should be very nearly 1.\n",
    "\n",
    "            # Rotate to projection coordinates            \n",
    "            xe, ye, ze = np.dot(mat, r_obs)\n",
    "\n",
    "            dlt = ze/MPC_library.Constants.speed_of_light\n",
    "\n",
    "            results_dict[trackletKey].append((jd_tdb, dlt, theta_x, theta_y, theta_z, xe, ye, ze))\n",
    "        \n",
    "    # All the work done above is independent of the z0 and zdot0 values\n",
    "\n",
    "    master_results = {}\n",
    "    for g_gdot in g_gdot_pairs:\n",
    "        g, gdot = g_gdot\n",
    "\n",
    "        results = []\n",
    "        for k, v in results_dict.items():  \n",
    "            \n",
    "            # This condition is new\n",
    "            if len(v)>1:\n",
    "                cx, mx, cy, my, t0 = fit_tracklet_func(t_ref, g, gdot, v)\n",
    "                result = (k, cx, mx, cy, my, t0)\n",
    "                results.append(result)\n",
    "\n",
    "        master_results[g_gdot] = results\n",
    "    \n",
    "    return master_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can probably pass in the clustering function, to reduce code duplication\n",
    "def transform_to_arrows_for_regions_gamma_v2(t_ref, g_gdot_pairs, pixels, pix_dict, tracklets_dict, nside=8, angDeg=5.5, transform_to_arrows_function=transform_to_arrows_v3):\n",
    "    \n",
    "    pixel_results = {}\n",
    "    for pixel in pixels:\n",
    "        vec = hp.pix2vec(nside, pixel, nest=True)\n",
    "        neighbors = hp.query_disc(nside, vec, angDeg*np.pi/180., inclusive=True, nest=True)\n",
    "        trackletKeys = []\n",
    "        for pix in neighbors:\n",
    "            trackletKeys.extend(pix_dict[pix])\n",
    "        print(pixel, len(trackletKeys))\n",
    "        if len(trackletKeys) > 0:\n",
    "            pixel_results[pixel] = transform_to_arrows_function(t_ref, g_gdot_pairs, vec, trackletKeys, tracklets_dict)\n",
    "            \n",
    "    return pixel_results\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can probably pass in the clustering function, to reduce code duplication\n",
    "def transform_to_arrows_for_regions_gamma_select(t_ref, g_gdot_pairs, pixels, pix_dict, tracklets_dict, nside=8, angDeg=5.5, transform_to_arrows_function=transform_to_arrows_and_select):\n",
    "    \n",
    "    pixel_results = {}\n",
    "    for pixel in pixels:\n",
    "        vec = hp.pix2vec(nside, pixel, nest=True)\n",
    "        neighbors = hp.query_disc(nside, vec, angDeg*np.pi/180., inclusive=True, nest=True)\n",
    "        trackletKeys = []\n",
    "        for pix in neighbors:\n",
    "            trackletKeys.extend(pix_dict[pix])\n",
    "        print(pixel, len(trackletKeys))\n",
    "        if len(trackletKeys) > 0:\n",
    "            pixel_results[pixel] = transform_to_arrows_function(t_ref, g_gdot_pairs, vec, trackletKeys, tracklets_dict)\n",
    "            \n",
    "    return pixel_results\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do some training to determine what is a reasonable resolution for z and z-dot (or gamma and gamma-dot).   We also need to train for dt, the scale factor to relate the angular rates to the angles.  I'll first generate a very fine grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only do we want to tune over the velocity scaling factor dt and the cluster size d, we also want to get a sense for how finely we need to sample the gamma, gamma-dot space to have a good detection efficiency.  I am guessing that with a bigger dt and a bigger d we can get away with a courser grid of gamma, gamma-dot, but a larger d comes with a somewhat larger contamination rate from overlapping clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Probably want restructure this so that it is not internally aggregating\n",
    "# the results from differnt regions.\n",
    "\n",
    "def do_sky_v4(dt, rad, master, mincount=3):\n",
    "    cluster_counter = {}\n",
    "    cluster_arrow_dict = {}\n",
    "    for pix, d in master.items():\n",
    "        cluster_counter[pix] = Counter()\n",
    "        cluster_arrow_dict[pix] = defaultdict()\n",
    "        # Probably want to keep the pixel regions separate.\n",
    "        for g_gdot, arrows in d.items():\n",
    "\n",
    "            g, gdot = g_gdot\n",
    "            \n",
    "            # The bit from here\n",
    "            i = 0\n",
    "            label_dict={}\n",
    "            arrow_dict={}\n",
    "            combined=[]\n",
    "            for k, cx, mx, cy, my, t in arrows:\n",
    "                label_dict[i] = k\n",
    "                arrow_dict[k] = [pix, cx, mx, cy, my, g, gdot]\n",
    "                combined.append([cx, mx*dt, cy, my*dt])\n",
    "                i +=1\n",
    "            \n",
    "            # Skip if there are no arrows\n",
    "            if len(combined)==0:\n",
    "                continue\n",
    "            \n",
    "            points=np.array(combined)\n",
    "            # to here can be a function,\n",
    "            # that takes arrows and dt and\n",
    "            # returns label_dict, points array, and arrow_dict\n",
    "\n",
    "            \n",
    "            # Below is just one way of doing the clustering.\n",
    "            # There could be different versions of this\n",
    "            # that use the same input.\n",
    "            #\n",
    "            # The bit from here\n",
    "            tree = scipy.spatial.cKDTree(points)\n",
    "            matches = tree.query_ball_tree(tree, rad)\n",
    "            # to here can be a function, that takes\n",
    "            # points are rad and returns tree and\n",
    "            # matches\n",
    "\n",
    "            for j, match in enumerate(matches):\n",
    "                # There needs to be a function that\n",
    "                # accepts a candidate match (cluster)\n",
    "                # and returns a cluster key if the \n",
    "                # candidate match satisifies the criteria\n",
    "                # and None otherwise.\n",
    "                # The criteria can be the number of\n",
    "                # tracklets on different nights, something\n",
    "                # about RMS.\n",
    "                # The function even might distill contaminated\n",
    "                # clusters.\n",
    "                if len(match)>=mincount:\n",
    "                    cluster_list =[]\n",
    "                    tracklet_params=[]\n",
    "                    for idx in match:\n",
    "                        cluster_list.append(label_dict[idx])\n",
    "                        # At this point, collect the arrow information too,\n",
    "                        # and append it to another list.\n",
    "                    cluster_key = tuple(sorted(cluster_list))\n",
    "                    cluster_arrow_dict[pix][cluster_key] = [arrow_dict[k] for k in cluster_key]\n",
    "                    cluster_counter[pix].update({cluster_key: 1})\n",
    "\n",
    "    return cluster_counter, cluster_arrow_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def member_counts_v3(keys):\n",
    "    trackletIDs = [key[0] for key in keys]\n",
    "    key_counter = Counter(trackletIDs)\n",
    "    return key_counter\n",
    "\n",
    "def unique_clusters_v3(test_set, mincount=3):\n",
    "    # Initialize the results\n",
    "    success_dict = {}\n",
    "    potential_dict = {}\n",
    "    failure_counter = Counter()\n",
    "    # Loop over the clusters\n",
    "    for k in test_set:\n",
    "        stem_counter = member_counts_v3(k)\n",
    "        if len(stem_counter)==1:\n",
    "            # Pure cluster.  Keep longest instance.\n",
    "            for stem, v in stem_counter.items():\n",
    "                if stem not in success_dict:\n",
    "                    success_dict[stem] = v, k\n",
    "                elif v > success_dict[stem][0]:\n",
    "                    success_dict[stem] = v, k\n",
    "        else:\n",
    "            # Mixed cluster.  Keep longest instances\n",
    "            # of stems that have the minimum number \n",
    "            # of members.\n",
    "            potential=False\n",
    "            for stem, v in stem_counter.items():\n",
    "                if v>=mincount:\n",
    "                    potential=True\n",
    "                    if stem not in potential_dict:\n",
    "                        potential_dict[stem] = v, k\n",
    "                    elif v > potential_dict[stem][0]:\n",
    "                        potential_dict[stem] = v, k\n",
    "            if not potential:\n",
    "                failure_counter.update({k:1})\n",
    "            \n",
    "    return success_dict, failure_counter, potential_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "\n",
    "gs =[0.4]\n",
    "gdots = [-0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "# This could accept a cluster_sky regions function, so that there is only one version of this\n",
    "# code.\n",
    "\n",
    "def do_training_run_v4(pixels, tracklets_dict, t_ref, \n",
    "                    g_select,\n",
    "                    transform_to_arrows_for_regions_function=transform_to_arrows_for_regions_gamma_v2,\n",
    "                    g_gdots=g_gdots, mincount=3, nside=8,\n",
    "                    dts=np.arange(5, 35, 5), \n",
    "                    radii=np.arange(0.0001, 0.0100, 0.0001)):\n",
    "    \n",
    "    pix_dict = index_tracklets_gamma(g_select, tracklets_dict, nside=nside)\n",
    "    \n",
    "    # There must be a better name than master.\n",
    "    master = transform_to_arrows_for_regions_function(t_ref, g_gdots, pixels, pix_dict, tracklets_dict)\n",
    "\n",
    "    results_dict = {}\n",
    "\n",
    "    rates_dict={}\n",
    "    \n",
    "    cluster_arrow_dict = {}\n",
    "    \n",
    "    for dt in dts:\n",
    "        print(dt)\n",
    "        for rad in radii:\n",
    "            print(\"dt:\", dt, \"rad:\", rad)\n",
    "            # The result of this pass is populating cluster_counter\n",
    "            rates_dict[dt, rad], _ = do_sky_v3(dt, rad, master, mincount=mincount)\n",
    "            \n",
    "    return rates_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "\n",
    "gs =[0.4]\n",
    "gdots = [-0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "def do_run_v4(pixels, tracklets_dict, t_ref, \n",
    "           g_select, dt, rad,\n",
    "           transform_to_arrows_for_regions_function=transform_to_arrows_for_regions_gamma_v2,\n",
    "           g_gdots=g_gdots, mincount=3, nside=8): \n",
    "\n",
    "    pix_dict = index_tracklets_gamma(g_select, tracklets_dict, nside=nside)\n",
    "    \n",
    "    print(\"here1\")\n",
    "    \n",
    "    master = transform_to_arrows_for_regions_function(t_ref, g_gdots, pixels, pix_dict, tracklets_dict)\n",
    "    \n",
    "    print(\"here2\")\n",
    "\n",
    "    results, cluster_arrow_dict = do_sky_v4(dt, rad, master, mincount=mincount)\n",
    "    \n",
    "    return results, cluster_arrow_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for MBAs\n",
    "\n",
    "Now I am going to set up five training runs for the gamma=0.4 distance class.  I will use a very fine grid in gamma-dot.\n",
    "\n",
    "We are looking for the parameters that give us the highest completeness for an acceptable error rate.  I will look for the parameters, among the five training runs, that achieve that.  Then I will test it on other time slices and data sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am updating this to do the MBA training runs with the tracklet-base approach\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "    for dt in np.arange(5, 35, 5):\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_run_v4.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt], radii=np.logspace(-4, -2, num=30))\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am updating this to do the MBA training runs with the tracklet-base approach\n",
    "nside=8\n",
    "\n",
    "#gs = [0.24, 0.26, 0.28, 0.3, 0.32, 0.34, 0.36, 0.38, 0.4, 0.42, 0.44]\n",
    "gs = [0.3, 0.325, 0.35, 0.375, 0.4, 0.425, 0.45]\n",
    "gdots = [-0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "#for n in [-14, -17, -20, -23]:\n",
    "#for n in [-11]:\n",
    "    for dt in [5]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_run_v4_extras_g_v7.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt], radii=np.logspace(-4, -2, num=30))\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am updating this to do the MBA training runs with the tracklet-base approach\n",
    "nside=8\n",
    "\n",
    "gs = [0.3, 0.35, 0.4, 0.45]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "#for n in [-14, -17, -20, -23]:\n",
    "#for n in [-11]:\n",
    "    for dt in [5]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_run_v4_extras_g_v9.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt], radii=np.logspace(-4, -2, num=30))\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am updating this to do the MBA training runs with the tracklet-base approach\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "#for n in [-14, -17, -20, -23]:\n",
    "#for n in [-11]:\n",
    "    for dt in [5]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_run_v4_extras_g_v10.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt], radii=np.logspace(-4, -2, num=30))\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am updating this to do the MBA training runs with the tracklet-base approach\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "gdots = [-0.008, -0.006, -0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004, 0.006, 0.008]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "#for n in [-14, -17, -20, -23]:\n",
    "#for n in [-11]:\n",
    "    for dt in [5]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_run_v4_extras_g_v8.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt], radii=np.logspace(-4, -2, num=30))\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am updating this to do the MBA training runs with the tracklet-base approach\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "#gs = [0.3, 0.35, 0.4, 0.45]\n",
    "#gs = [0.25, 0.275, 0.3, 0.325, 0.35, 0.375, 0.4, 0.425, 0.45]\n",
    "#gdots = [-0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004]\n",
    "gdots = [-0.004, -0.0035, -0.003, -0.0025, -0.002, -0.0015, -0.001, -0.0005, 0.0, 0.0005, 0.001, 0.0015, 0.002, 0.0025, 0.003, 0.0035, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "#for n in [-14, -17, -20, -23]:\n",
    "#for n in [-11]:\n",
    "    for dt in [5]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_run_v4_extras_g_v5.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt], radii=np.logspace(-4, -2, num=30))\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# This routine is finding all the clusters in a set of HEALPix regions that have a minimum number of tracklets,\n",
    "# enough that these objects would be discovered.\n",
    "def accessible_clusters(g_select, pixels, tracklets_dict, mincount=3, nside=8, angDeg=5.5):\n",
    "\n",
    "    pix_dict = index_tracklets_gamma(g_select, tracklets_dict, nside=nside)\n",
    "    true_counts={}\n",
    "    trackletList_dict = {}\n",
    "    \n",
    "    for pix in pixels:\n",
    "\n",
    "        vec = hp.pix2vec(nside, pix, nest=True)\n",
    "        neighbors = hp.query_disc(nside, vec, angDeg*np.pi/180., inclusive=True, nest=True)\n",
    "        \n",
    "        trackletKeys = []\n",
    "        for npix in neighbors:\n",
    "            trackletKeys.extend(pix_dict[npix])\n",
    "        \n",
    "        trackletList=defaultdict(set)\n",
    "        for trackletKey in trackletKeys:\n",
    "            trackletList[trackletKey[0]].update({trackletKey})\n",
    "\n",
    "        true_counts[pix]=len([k for k, v in trackletList.items() if len(v)>=mincount])\n",
    "        trackletList_dict[pix]=trackletList\n",
    "    \n",
    "    clusterID_set = set()\n",
    "    for pix, cntr in trackletList_dict.items():\n",
    "        for cID, v in cntr.items():\n",
    "            if len(v)>=mincount:\n",
    "                clusterID_set.update({cID})\n",
    "    \n",
    "    return clusterID_set, trackletList_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def pixel_region_tracklets(g_select, pixels, tracklets_dict, nside=8, angDeg=5.5):\n",
    "\n",
    "    pix_dict = index_tracklets_gamma(g_select, tracklets_dict, nside=nside)\n",
    "    tracklet_dict = {}\n",
    "    \n",
    "    for pix in pixels:\n",
    "\n",
    "        vec = hp.pix2vec(nside, pix, nest=True)\n",
    "        neighbors = hp.query_disc(nside, vec, angDeg*np.pi/180., inclusive=True, nest=True)\n",
    "        \n",
    "        trackletKeys = []\n",
    "        for n_pix in neighbors:\n",
    "            trackletKeys.extend(pix_dict[n_pix])\n",
    "            \n",
    "        tracklet_dict[pix] = trackletKeys\n",
    "    \n",
    "    return tracklet_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# This routine is finding all the clusters in a set of HEALPix regions that have a minimum number of tracklets,\n",
    "# enough that these objects would be discovered.\n",
    "def accessible_clusters_v2(g_select, pixels, tracklets_dict, mincount=3, nside=8, angDeg=5.5):\n",
    "\n",
    "    pix_dict = index_tracklets_gamma(g_select, tracklets_dict, nside=nside)\n",
    "    true_counts={}\n",
    "    trackletList_dict = {}\n",
    "    \n",
    "    tracklet_dict = pixel_region_tracklets(g_select, pixels, tracklets_dict, nside=nside, angDeg=angDeg)\n",
    "    \n",
    "    for pix, trackletKeys in tracklet_dict.items():\n",
    "\n",
    "        trackletList=defaultdict(set)\n",
    "        for trackletKey in trackletKeys:\n",
    "            trackletList[trackletKey[0]].update({trackletKey})\n",
    "\n",
    "        if len(trackletList)>0:\n",
    "            trackletList_dict[pix]=trackletList\n",
    "    \n",
    "    clusterID_dict = defaultdict(set)\n",
    "    for pix, cntr in trackletList_dict.items():\n",
    "        for cID, v in cntr.items():\n",
    "            if len(v)>=mincount:\n",
    "                clusterID_dict[pix].update({cID})\n",
    "    \n",
    "    return clusterID_dict, trackletList_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for figures 3 and 4\n",
    "\n",
    "In the next cell I am selecting the tracklets in pixel 281 from lunation -14, using gamma=0.4 and gamma-dot=0.0\n",
    "\n",
    "I am outputting some information that is needed for figures 3 and 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_select=0.4\n",
    "nside=8\n",
    "n=-14\n",
    "dt=5.0\n",
    "pix_runs = {}\n",
    "t_ref = lunation_center(n)\n",
    "pixels=[281]\n",
    "g_gdot = [(0.4, 0.0)]\n",
    "pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "with open(pickle_input_filename, 'rb') as handle:\n",
    "    with open('data/for_figure3.pickle', 'wb') as out_handle:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "        pix_dict = index_tracklets_gamma(g_select, tracklets_dict, nside=nside)\n",
    "        master = transform_to_arrows_for_regions_gamma_v2(t_ref, g_gdot, pixels, pix_dict, tracklets_dict)\n",
    "        theta_master = transform_to_theta_pairs_for_regions(t_ref, pixels, pix_dict, tracklets_dict)\n",
    "        radec_master = transform_to_radec_pairs_for_regions(t_ref, pixels, pix_dict, tracklets_dict)\n",
    "        pickle.dump((radec_master, theta_master, master), out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        results = accessible_clusters_v2(g_select, range(hp.nside2npix(nside)), tracklets_dict)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_select=0.4\n",
    "nside=8\n",
    "n=-14\n",
    "dt=5.0\n",
    "pix_runs = {}\n",
    "t_ref = lunation_center(n)\n",
    "pixels=[281, 282]\n",
    "g_gdot = [(0.4, 0.0)]\n",
    "pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "with open(pickle_input_filename, 'rb') as handle:\n",
    "    with open('data/foo.pickle', 'wb') as out_handle:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "        pix_dict = index_tracklets_gamma(g_select, tracklets_dict, nside=nside)\n",
    "        master = transform_to_arrows_for_regions_gamma_v2(t_ref, g_gdot, pixels, pix_dict, tracklets_dict)\n",
    "        theta_master = transform_to_theta_pairs_for_regions(t_ref, pixels, pix_dict, tracklets_dict)\n",
    "        radec_master = transform_to_radec_pairs_for_regions(t_ref, pixels, pix_dict, tracklets_dict)\n",
    "        pickle.dump((radec_master, theta_master, master), out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        results = accessible_clusters_v2(g_select, range(hp.nside2npix(nside)), tracklets_dict)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results2 = accessible_clusters_v2(g_select, range(hp.nside2npix(nside)), tracklets_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "available=results2[0][281]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now run that one pixel region to see what is found.\n",
    "\n",
    "dt=5\n",
    "rad = 0.002\n",
    "mincount=3\n",
    "thingy4 = do_sky_v4(dt, rad, master, mincount=mincount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "found_new=set(unique_clusters_v3(list(thingy4[0][281].keys()))[0].keys()).union(unique_clusters_v3(list(thingy4[0][281].keys()))[2].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thingy4[0][281]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def found_clusters(cluster_keys):\n",
    "    uc = unique_clusters_v3(cluster_keys)\n",
    "    pure = set(uc[0].keys())\n",
    "    potential = set(uc[2].keys())\n",
    "    found=pure.union(potential)\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn282 = found_clusters(thingy4[0][282].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "available282=results2[0][282]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(fn282), len(available282)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(available), len(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(found), len(found_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select=0.4\n",
    "nside=8\n",
    "n=-14\n",
    "dt=5.0\n",
    "pix_runs = {}\n",
    "t_ref = lunation_center(n)\n",
    "pixels=[281]\n",
    "\n",
    "pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "with open(pickle_input_filename, 'rb') as handle:\n",
    "    with open('data/for_figure4.pickle', 'wb') as out_handle:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets  = pickle.load(handle)\n",
    "        pix_dict = index_tracklets_gamma(g_select, tracklets_dict, nside=nside)\n",
    "        master = transform_to_arrows_for_regions_gamma_v2(t_ref, g_gdots, pixels, pix_dict, tracklets_dict)\n",
    "        pickle.dump(master, out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This whole cell could become a function\n",
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "true_count_sets_v2={}\n",
    "trackletCounter_dict={}\n",
    "trackletCounter_dict_v2={}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "#for n in [-14]:\n",
    "    print(n)\n",
    "    pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    pickle_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_run_v4_extras_g.pickle' % (lunation_center(n), dt)\n",
    "    with open(pickle_input_filename, 'rb') as handle, open(pickle_filename, 'rb') as handle2:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "        run[n] = pickle.load(handle2), _\n",
    "        #run[n] = pickle.load(handle2)\n",
    "        true_count_sets[n], trackletCounter_dict[n]=accessible_clusters(g_select, range(hp.nside2npix(nside)), tracklets_dict)\n",
    "        true_count_sets_v2[n], trackletCounter_dict_v2[n]=accessible_clusters(g_select, range(hp.nside2npix(nside)), tracklets_dict, mincount=2)\n",
    "\n",
    "\n",
    "total_count = np.sum([len(k) for n, k in true_count_sets.items()])\n",
    "\n",
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This whole cell could become a function\n",
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "true_count_sets_v2={}\n",
    "trackletCounter_dict={}\n",
    "trackletCounter_dict_v2={}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "#for n in [-14]:\n",
    "    print(n)\n",
    "    pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    pickle_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_run_v4.pickle' % (lunation_center(n), dt)\n",
    "    with open(pickle_input_filename, 'rb') as handle, open(pickle_filename, 'rb') as handle2:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "        run[n] = pickle.load(handle2)\n",
    "        true_count_sets[n], trackletCounter_dict[n]=accessible_clusters(g_select, range(hp.nside2npix(nside)), tracklets_dict)\n",
    "        true_count_sets_v2[n], trackletCounter_dict_v2[n]=accessible_clusters(g_select, range(hp.nside2npix(nside)), tracklets_dict, mincount=2)\n",
    "\n",
    "\n",
    "total_count = np.sum([len(k) for n, k in true_count_sets.items()])\n",
    "\n",
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ckeys = run[-11][0][5][4]\n",
    "ncs_total = [len(set(cks)) for cks in ckeys]\n",
    "ncs_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(set(ckeys[25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_count_sets[-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grab_run_pickled_results(n, dt, file_stem='data/UnnObs_Training_1_line_C_', suff='_run.pickle'):\n",
    "    pickle_output_filename = file_stem + ('%.1lf_pm15.0_%.1lf' % (lunation_center(n), dt)) + suff\n",
    "    print(pickle_output_filename)\n",
    "    with open(pickle_output_filename, 'rb') as handle:\n",
    "        #inner_run = pickle.load(handle), _\n",
    "        inner_run = pickle.load(handle), _\n",
    "        return inner_run\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt=5\n",
    "n=-14\n",
    "run = grab_run_pickled_results(n, dt, file_stem='data/UnnObs_Training_1_line_C_', suff='_run_v4.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run[0][5][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "used_tracklets=set()\n",
    "for k, v in run[0][5][4][6].items():\n",
    "    for trackletID in v[1]:\n",
    "        used_tracklets.update({trackletID})\n",
    "        \n",
    "for k, v in run[0][5][5][6].items():\n",
    "    for trackletID in v[1]:\n",
    "        used_tracklets.update({trackletID})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(used_tracklets), len(tracklets_dict)\n",
    "# Next iterate over the used tracklets to see how many of them are of non-MBA classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lengthDict=defaultdict(list)\n",
    "for k, v in trackletCounter_dict[281].items():\n",
    "    print(k, len(v))\n",
    "    lengthDict[len(v)].append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, v in lengthDict.items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(true_count_sets[-14]), len(true_count_sets_v2[-14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=-23\n",
    "dt = 5\n",
    "for d, ncs, nes, nps, ckeys, pkeys in  list(zip(*run[0][dt])):\n",
    "    correct= set(ckeys)\n",
    "    potential = set(pkeys)\n",
    "    total = correct.union(potential)\n",
    "    diff = potential.difference(correct)\n",
    "    print(\"%7.5lf %5d %6d %6d %6d %6d\" % (d, len(correct), len(potential), len(total), nes, len(correct)+len(potential)+nes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Don't even think about putting this in\n",
    "# the cell with the corresponding figure.\n",
    "# A pox upon your family.\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 16}\n",
    "\n",
    "mpl.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(run[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=-14\n",
    "fs=14\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, sharey=False, figsize=(8,12))\n",
    "\n",
    "ds_dict={}\n",
    "ncs_total_dict={}\n",
    "nes_total_dict={}\n",
    "nts_total_dict={}\n",
    "\n",
    "lines = [\"-\",\"--\",\"-.\",\":\", \"-\",\"--\",\"-.\"]\n",
    "#for dt in np.arange(5, 35, 5):\n",
    "for i, dt in enumerate([1, 5, 10, 15, 20, 25, 30]):\n",
    "#for dt in [5]:\n",
    "    ncs, nps, nts = [], [], []\n",
    "    for n in [-11]:\n",
    "        run = grab_run_pickled_results(n, dt, file_stem='data/UnnObs_Training_1_line_C_', suff='_run_v4.pickle')\n",
    "        ds, _, nes, _, ckeys, pkeys = run[0][0][dt]\n",
    "        #ds, _, nes, _, ckeys, pkeys = run[0][dt]\n",
    "        nes_total = nes\n",
    "        ncs_total = [len(set(cks)) for cks in ckeys]\n",
    "        nps_total = [len(set(pks)) for pks in pkeys]\n",
    "        nts_total = [len(set(cks).union(set(pks))) for cks, pks in zip(ckeys, pkeys)]\n",
    "\n",
    "    for n in [-14, -17, -20, -23]:\n",
    "    #for n in [-14]:\n",
    "        \n",
    "        run = grab_run_pickled_results(n, dt, file_stem='data/UnnObs_Training_1_line_C_', suff='_run_v4.pickle')\n",
    "        ds, _, nes, _, ckeys, pkeys = run[0][0][dt]\n",
    "        #ds, _, nes, _, ckeys, pkeys = run[0][dt]\n",
    "        nes_total = [np.sum(x) for x in zip(nes, nes_total)]        \n",
    "        ncs = [len(set(cks)) for cks in ckeys]\n",
    "        ncs_total = [np.sum(x) for x in zip(ncs, ncs_total)]\n",
    "        nps = [len(set(pks)) for pks in pkeys]\n",
    "        nps_total = [np.sum(x) for x in zip(nps, nps_total)]        \n",
    "        nts = [len(set(cks).union(set(pks))) for cks, pks in zip(ckeys, pkeys)]\n",
    "        nts_total = [np.sum(x) for x in zip(nts, nts_total)]     \n",
    "\n",
    "    ax1.plot(ds, ncs_total, label='', ls=lines[i])\n",
    "    ax2.plot(ds, nts_total, label=dt, ls=lines[i])\n",
    "    ax3.plot(ds, nes_total, label='', ls=lines[i])\n",
    "    ds_dict[dt] = ds\n",
    "    ncs_total_dict[dt] = ncs_total\n",
    "    nes_total_dict[dt] = nes_total\n",
    "    nts_total_dict[dt] = nts_total\n",
    "\n",
    "ax1.set_xscale(\"log\", nonposx='clip')\n",
    "low_lim, high_lim=1, 22000\n",
    "#ax1.set_yscale(\"log\", nonposx='clip')\n",
    "#ax2.set_yscale(\"log\", nonposx='clip')\n",
    "#ax3.set_yscale(\"log\", nonposx='clip')\n",
    "ax1.set_ylim((low_lim, high_lim))\n",
    "ax2.set_ylim((low_lim, high_lim))\n",
    "ax3.set_ylim((low_lim, high_lim))\n",
    "\n",
    "ax1.axhline(total_count, ls='dashed')\n",
    "ax2.axhline(total_count, ls='dashed')\n",
    "ax3.axhline(total_count, ls='dashed')\n",
    "\n",
    "ax1.axvline(0.002, ls='dashdot')\n",
    "ax2.axvline(0.002, ls='dashdot')\n",
    "ax3.axvline(0.002, ls='dashdot')\n",
    "\n",
    "best_rad = 0.003\n",
    "#ax1.axvline(best_rad, ls='solid')\n",
    "#ax2.axvline(best_rad, ls='solid')\n",
    "#ax3.axvline(best_rad, ls='solid')\n",
    "\n",
    "ax3.set_xlabel('d (cluster radius)')\n",
    "ax1.set_ylabel('N pure clusters', fontsize=fs)\n",
    "ax2.set_ylabel('N total clusters', fontsize=fs)\n",
    "ax3.set_ylabel('N errors', fontsize=fs)\n",
    "ax1.text(0.0001, 15000, r'$\\gamma=0.4$', fontsize=15)\n",
    "ax2.text(0.0001, 15000, r'$\\gamma=0.4$', fontsize=15)\n",
    "ax3.text(0.0001, 15000, r'$\\gamma=0.4$', fontsize=15)\n",
    "\n",
    "band_min=0.001\n",
    "band_max = 0.003\n",
    "#ax1.axvspan(band_min, band_max, 0.004, alpha=0.25, color='gray')\n",
    "#ax2.axvspan(band_min, band_max, 0.004, alpha=0.25, color='gray')\n",
    "#ax3.axvspan(band_min, band_max, 0.004, alpha=0.25, color='gray')\n",
    "\n",
    "#ax2.tick_params(direction='in')\n",
    "ax2.legend(loc='lower right', fontsize=12, title='dt')\n",
    "#plt.savefig('nclusters_vs_d_g0.4.pdf')\n",
    "f.subplots_adjust(hspace=0.1)\n",
    "plt.tight_layout()\n",
    "#f.tight_layout()\n",
    "f.savefig('MBA_lunations_v2.png')\n",
    "f.show()\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=-14\n",
    "fs=14\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, sharey=False, figsize=(8,12))\n",
    "\n",
    "for suff, label, line in zip(['_run_v4.pickle', '_run_v4_extras_g_v9.pickle', '_run_v4_extras_g_v4.pickle', '_run_v4_extras_g_v2.pickle', '_run_v4_extras_g_v10.pickle'],\n",
    "                       ['-, -', 'g, -', '-, gd', 'g, gd', '_, gd'], [\"-\",\"--\",\"-.\",\":\"]):\n",
    "    \n",
    "#r'$\\gamma=0.4$'\n",
    "\n",
    "    ds_dict={}\n",
    "    ncs_total_dict={}\n",
    "    nes_total_dict={}\n",
    "    nts_total_dict={}\n",
    "\n",
    "    #for dt in np.arange(5, 35, 5):\n",
    "    #for dt in [1, 5, 10, 15, 20, 25, 30]:\n",
    "    for dt in [5]:\n",
    "        ncs, nps, nts = [], [], []\n",
    "        for n in [-11]:\n",
    "            run = grab_run_pickled_results(n, dt, file_stem='data/UnnObs_Training_1_line_C_', suff=suff)\n",
    "            ds, _, nes, _, ckeys, pkeys = run[0][0][dt]\n",
    "            #ds, _, nes, _, ckeys, pkeys = run[0][dt]\n",
    "            nes_total = nes\n",
    "            ncs_total = [len(set(cks)) for cks in ckeys]\n",
    "            nps_total = [len(set(pks)) for pks in pkeys]\n",
    "            nts_total = [len(set(cks).union(set(pks))) for cks, pks in zip(ckeys, pkeys)]\n",
    "\n",
    "        for n in [-14, -17, -20, -23]:\n",
    "        #for n in [-14]:\n",
    "\n",
    "            run = grab_run_pickled_results(n, dt, file_stem='data/UnnObs_Training_1_line_C_', suff=suff)\n",
    "            ds, _, nes, _, ckeys, pkeys = run[0][0][dt]\n",
    "            #ds, _, nes, _, ckeys, pkeys = run[0][dt]\n",
    "            nes_total = [np.sum(x) for x in zip(nes, nes_total)]        \n",
    "            ncs = [len(set(cks)) for cks in ckeys]\n",
    "            ncs_total = [np.sum(x) for x in zip(ncs, ncs_total)]\n",
    "            nps = [len(set(pks)) for pks in pkeys]\n",
    "            nps_total = [np.sum(x) for x in zip(nps, nps_total)]        \n",
    "            nts = [len(set(cks).union(set(pks))) for cks, pks in zip(ckeys, pkeys)]\n",
    "            nts_total = [np.sum(x) for x in zip(nts, nts_total)]     \n",
    "\n",
    "    ax1.plot(ds, ncs_total, label='', ls=line)\n",
    "    ax2.plot(ds, nts_total, label=label, ls=line)\n",
    "    ax3.plot(ds, nes_total, label='', ls=line)\n",
    "    ds_dict[dt] = ds\n",
    "    ncs_total_dict[dt] = ncs_total\n",
    "    nes_total_dict[dt] = nes_total\n",
    "    nts_total_dict[dt] = nts_total\n",
    "\n",
    "ax1.set_xscale(\"log\", nonposx='clip')\n",
    "low_lim, high_lim=1, 22000\n",
    "#ax1.set_yscale(\"log\", nonposx='clip')\n",
    "#ax2.set_yscale(\"log\", nonposx='clip')\n",
    "#ax3.set_yscale(\"log\", nonposx='clip')\n",
    "ax1.set_ylim((low_lim, high_lim))\n",
    "ax2.set_ylim((low_lim, high_lim))\n",
    "ax3.set_ylim((low_lim, high_lim))\n",
    "\n",
    "ax1.axhline(total_count, ls='dashed')\n",
    "ax2.axhline(total_count, ls='dashed')\n",
    "ax3.axhline(total_count, ls='dashed')\n",
    "\n",
    "best_rad = 0.003\n",
    "#ax1.axvline(best_rad, ls='solid')\n",
    "#ax2.axvline(best_rad, ls='solid')\n",
    "#ax3.axvline(best_rad, ls='solid')\n",
    "\n",
    "ax3.set_xlabel('d (cluster radius)')\n",
    "ax1.set_ylabel('N pure clusters', fontsize=fs)\n",
    "ax2.set_ylabel('N valid clusters', fontsize=fs)\n",
    "ax3.set_ylabel('N errors', fontsize=fs)\n",
    "ax1.text(0.0001, 15000, r'$\\gamma=0.4$', fontsize=15)\n",
    "ax2.text(0.0001, 15000, r'$\\gamma=0.4$', fontsize=15)\n",
    "ax3.text(0.0001, 15000, r'$\\gamma=0.4$', fontsize=15)\n",
    "\n",
    "band_min=0.001\n",
    "band_max = 0.003\n",
    "#ax1.axvspan(band_min, band_max, 0.004, alpha=0.25, color='gray')\n",
    "#ax2.axvspan(band_min, band_max, 0.004, alpha=0.25, color='gray')\n",
    "#ax3.axvspan(band_min, band_max, 0.004, alpha=0.25, color='gray')\n",
    "\n",
    "ax1.axvline(0.002, ls='dashdot')\n",
    "ax2.axvline(0.002, ls='dashdot')\n",
    "ax3.axvline(0.002, ls='dashdot')\n",
    "\n",
    "#ax2.tick_params(direction='in')\n",
    "ax2.legend(loc='lower right', fontsize=12, title='parameters varied')\n",
    "#plt.savefig('nclusters_vs_d_g0.4.pdf')\n",
    "f.subplots_adjust(hspace=0.1)\n",
    "plt.tight_layout()\n",
    "#f.tight_layout()\n",
    "f.savefig('MBA_lunations_comparison.png')\n",
    "f.show()\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Dynamical Classes\n",
    "\n",
    "Now I am trying to go through the 'C' file to separate out the NEOs, Trojans, Centaurs, and TNOs in order to reproduce the figures that show the linking efficiency, with the new data set and data format.\n",
    "\n",
    "Get all the sorted tracklet information for each of the dynamical classes.\n",
    "\n",
    "Generate time windows for the dynamical classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate a dynamical type dictionary for the 'C' file.\n",
    "with open(\"data/Elements_UnnObs_Training_1_line_C_ec.txt\") as f:\n",
    "    data_txt = f.readlines()\n",
    "\n",
    "# Types of interest\n",
    "typeDict__={}\n",
    "types_ = ['NEO','TRO','CEN','TNO','MBA']\n",
    "for t in types_:\n",
    "    typeDict__[t] = {} \n",
    "\n",
    "for data in data_txt[1:]:\n",
    "    desig,a,e,i,AN,Peri,M,Epoch = data.split() \n",
    "    if a != '-':\n",
    "        a,e,i,AN,Peri,M =float(a),float(e),float(i),float(AN),float(Peri),float(M)\n",
    "\n",
    "        # NEO \n",
    "        if a < 1.25:\n",
    "            typeDict__['NEO'][desig] = data\n",
    "        # TRO \n",
    "        elif a > 5.1 and a < 5.3:\n",
    "            typeDict__['TRO'][desig] = data\n",
    "        # CEN \n",
    "        elif a > 6.0 and a < 30.:\n",
    "            typeDict__['CEN'][desig] = data\n",
    "        # TNO \n",
    "        elif a > 30. and a*(1.-e)>30. :\n",
    "            typeDict__['TNO'][desig] = data\n",
    "        else:\n",
    "            typeDict__['MBA'][desig] = data\n",
    "\n",
    "    else:\n",
    "        typeDict__['MBA'][desig] = data\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the dynamical type dictionary to split out the NEO data lines.\n",
    "#\n",
    "with open('data/UnnObs_Training_1_line_C_NEO.txt', 'w') as outfile:\n",
    "    with open('data/UnnObs_Training_1_line_C.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            objName, provDesig, disAst, note1, note2, dateObs, RA, Dec, mag, filt, obsCode = convertObs80(line)\n",
    "            if provDesig in typeDict__['NEO']:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the 'mpc' file, which is actually our specific format, for the NEOs.\n",
    "#\n",
    "with open('data/UnnObs_Training_1_line_C_NEO_ec.mpc', 'w') as outfile:\n",
    "    with open('data/UnnObs_Training_1_line_C_NEO.txt', 'r') as f:\n",
    "        outstring = \"#trackletID yr   mn dy      obsCode mag filter  jd_tdb       x_target     y_target     z_target      x_obs       y_obs        z_obs     \\n\"\n",
    "        outfile.write(outstring)\n",
    "        for line in f:\n",
    "            objName, provDesig, disAst, note1, note2, dateObs, RA, Dec, mag, filt, obsCode = convertObs80(line)\n",
    "            jd_utc = MPC_library.date2JD(dateObs)\n",
    "            jd_tdb  = MPC_library.EOP.jdTDB(jd_utc)\n",
    "            raDeg, decDeg = MPC_library.RA2degRA(RA), MPC_library.Dec2degDec(Dec)\n",
    "            x = np.cos(decDeg*np.pi/180.)*np.cos(raDeg*np.pi/180.)\n",
    "            y = np.cos(decDeg*np.pi/180.)*np.sin(raDeg*np.pi/180.)  \n",
    "            z = np.sin(decDeg*np.pi/180.)\n",
    "            xec, yec, zec = equatorial_to_ecliptic(np.array((x, y, z)))\n",
    "            \n",
    "            if filt.isspace():\n",
    "                filt = '-'\n",
    "            if mag.isspace():\n",
    "                mag = '----'\n",
    "            xh, yh, zh = Observatories.getObservatoryPosition(obsCode, jd_utc)\n",
    "            xhec, yhec, zhec = equatorial_to_ecliptic(np.array((xh, yh, zh)))\n",
    "            #outstring = \"%11s %s %4s %5s %s %13.6lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf\\n\"% \\\n",
    "            #      (provDesig, dateObs, obsCode, mag, filt, jd_tdb, x, y, z, xh, yh, zh, xec, yec, zec, xhec, yhec, zhec)\n",
    "            outstring = \"%11s %s %4s %5s %s %13.6lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf\\n\"% \\\n",
    "                (provDesig, dateObs, obsCode, mag, filt, jd_tdb, xec, yec, zec, xhec, yhec, zhec)\n",
    "            outfile.write(outstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NEO_tracklets, NEO_tracklets_jd_dict, NEO_sortedTracklets = get_sorted_tracklets_v5('data/UnnObs_Training_1_line_C_NEO.txt', 'data/UnnObs_Training_1_line_C_NEO_ec.mpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "separate_time_windows_v5(NEO_tracklets, NEO_sortedTracklets, NEO_tracklets_jd_dict, time_centers_lunation, file_stem='data/UnnObs_Training_1_line_C_NEO.mpc', dt=15., suff='.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trojans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the dynamical type dictionary to split out the Trojans data lines.\n",
    "#\n",
    "with open('data/UnnObs_Training_1_line_C_TRO.txt', 'w') as outfile:\n",
    "    with open('data/UnnObs_Training_1_line_C.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            objName, provDesig, disAst, note1, note2, dateObs, RA, Dec, mag, filt, obsCode = convertObs80(line)\n",
    "            if provDesig in typeDict__['TRO']:\n",
    "                outfile.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the 'mpc' file, which is actually our specific format, for the Trojans.\n",
    "#\n",
    "with open('data/UnnObs_Training_1_line_C_TRO_ec.mpc', 'w') as outfile:\n",
    "    with open('data/UnnObs_Training_1_line_C_TRO.txt', 'r') as f:\n",
    "        outstring = \"#trackletID yr   mn dy      obsCode mag filter  jd_tdb       x_target     y_target     z_target      x_obs       y_obs        z_obs     \\n\"\n",
    "        outfile.write(outstring)\n",
    "        for line in f:\n",
    "            objName, provDesig, disAst, note1, note2, dateObs, RA, Dec, mag, filt, obsCode = convertObs80(line)\n",
    "            jd_utc = MPC_library.date2JD(dateObs)\n",
    "            jd_tdb  = MPC_library.EOP.jdTDB(jd_utc)\n",
    "            raDeg, decDeg = MPC_library.RA2degRA(RA), MPC_library.Dec2degDec(Dec)\n",
    "            x = np.cos(decDeg*np.pi/180.)*np.cos(raDeg*np.pi/180.)\n",
    "            y = np.cos(decDeg*np.pi/180.)*np.sin(raDeg*np.pi/180.)  \n",
    "            z = np.sin(decDeg*np.pi/180.)\n",
    "            xec, yec, zec = equatorial_to_ecliptic(np.array((x, y, z)))\n",
    "            \n",
    "            if filt.isspace():\n",
    "                filt = '-'\n",
    "            if mag.isspace():\n",
    "                mag = '----'\n",
    "            xh, yh, zh = Observatories.getObservatoryPosition(obsCode, jd_utc)\n",
    "            xhec, yhec, zhec = equatorial_to_ecliptic(np.array((xh, yh, zh)))\n",
    "            #outstring = \"%11s %s %4s %5s %s %13.6lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf\\n\"% \\\n",
    "            #      (provDesig, dateObs, obsCode, mag, filt, jd_tdb, x, y, z, xh, yh, zh, xec, yec, zec, xhec, yhec, zhec)\n",
    "            outstring = \"%11s %s %4s %5s %s %13.6lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf\\n\"% \\\n",
    "                (provDesig, dateObs, obsCode, mag, filt, jd_tdb, xec, yec, zec, xhec, yhec, zhec)\n",
    "            outfile.write(outstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRO_tracklets, TRO_tracklets_jd_dict, TRO_sortedTracklets = get_sorted_tracklets_v5('data/UnnObs_Training_1_line_C_TRO.txt', 'data/UnnObs_Training_1_line_C_TRO_ec.mpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "separate_time_windows_v5(TRO_tracklets, TRO_sortedTracklets, TRO_tracklets_jd_dict, time_centers_lunation, file_stem='data/UnnObs_Training_1_line_C_TRO.mpc', dt=15., suff='.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centaurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the dynamical type dictionary to split out the Centaur data lines.\n",
    "#\n",
    "with open('data/UnnObs_Training_1_line_C_CEN.txt', 'w') as outfile:\n",
    "    with open('data/UnnObs_Training_1_line_C.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            objName, provDesig, disAst, note1, note2, dateObs, RA, Dec, mag, filt, obsCode = convertObs80(line)\n",
    "            if provDesig in typeDict__['CEN']:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the 'mpc' file, which is actually our specific format, for the Centaurs.\n",
    "#\n",
    "with open('data/UnnObs_Training_1_line_C_CEN_ec.mpc', 'w') as outfile:\n",
    "    with open('data/UnnObs_Training_1_line_C_CEN.txt', 'r') as f:\n",
    "        outstring = \"#trackletID yr   mn dy      obsCode mag filter  jd_tdb       x_target     y_target     z_target      x_obs       y_obs        z_obs     \\n\"\n",
    "        outfile.write(outstring)\n",
    "        for line in f:\n",
    "            objName, provDesig, disAst, note1, note2, dateObs, RA, Dec, mag, filt, obsCode = convertObs80(line)\n",
    "            jd_utc = MPC_library.date2JD(dateObs)\n",
    "            jd_tdb  = MPC_library.EOP.jdTDB(jd_utc)\n",
    "            raDeg, decDeg = MPC_library.RA2degRA(RA), MPC_library.Dec2degDec(Dec)\n",
    "            x = np.cos(decDeg*np.pi/180.)*np.cos(raDeg*np.pi/180.)\n",
    "            y = np.cos(decDeg*np.pi/180.)*np.sin(raDeg*np.pi/180.)  \n",
    "            z = np.sin(decDeg*np.pi/180.)\n",
    "            xec, yec, zec = equatorial_to_ecliptic(np.array((x, y, z)))\n",
    "            \n",
    "            if filt.isspace():\n",
    "                filt = '-'\n",
    "            if mag.isspace():\n",
    "                mag = '----'\n",
    "            xh, yh, zh = Observatories.getObservatoryPosition(obsCode, jd_utc)\n",
    "            xhec, yhec, zhec = equatorial_to_ecliptic(np.array((xh, yh, zh)))\n",
    "            #outstring = \"%11s %s %4s %5s %s %13.6lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf\\n\"% \\\n",
    "            #      (provDesig, dateObs, obsCode, mag, filt, jd_tdb, x, y, z, xh, yh, zh, xec, yec, zec, xhec, yhec, zhec)\n",
    "            outstring = \"%11s %s %4s %5s %s %13.6lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf\\n\"% \\\n",
    "                (provDesig, dateObs, obsCode, mag, filt, jd_tdb, xec, yec, zec, xhec, yhec, zhec)\n",
    "            outfile.write(outstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CEN_tracklets, CEN_tracklets_jd_dict, CEN_sortedTracklets = get_sorted_tracklets_v5('data/UnnObs_Training_1_line_C_CEN.txt', 'data/UnnObs_Training_1_line_C_CEN_ec.mpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "separate_time_windows_v5(CEN_tracklets, CEN_sortedTracklets, CEN_tracklets_jd_dict, time_centers_lunation, file_stem='data/UnnObs_Training_1_line_C_CEN.mpc', dt=15., suff='.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TNOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the dynamical type dictionary to split out the TNO data lines.\n",
    "#\n",
    "with open('data/UnnObs_Training_1_line_C_TNO.txt', 'w') as outfile:\n",
    "    with open('data/UnnObs_Training_1_line_C.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            objName, provDesig, disAst, note1, note2, dateObs, RA, Dec, mag, filt, obsCode = convertObs80(line)\n",
    "            if provDesig in typeDict__['TNO']:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the 'mpc' file, which is actually our specific format, for the TNOs.\n",
    "#\n",
    "with open('data/UnnObs_Training_1_line_C_TNO_ec.mpc', 'w') as outfile:\n",
    "    with open('data/UnnObs_Training_1_line_C_TNO.txt', 'r') as f:\n",
    "        outstring = \"#trackletID yr   mn dy      obsCode mag filter  jd_tdb       x_target     y_target     z_target      x_obs       y_obs        z_obs     \\n\"\n",
    "        outfile.write(outstring)\n",
    "        for line in f:\n",
    "            objName, provDesig, disAst, note1, note2, dateObs, RA, Dec, mag, filt, obsCode = convertObs80(line)\n",
    "            jd_utc = MPC_library.date2JD(dateObs)\n",
    "            jd_tdb  = MPC_library.EOP.jdTDB(jd_utc)\n",
    "            raDeg, decDeg = MPC_library.RA2degRA(RA), MPC_library.Dec2degDec(Dec)\n",
    "            x = np.cos(decDeg*np.pi/180.)*np.cos(raDeg*np.pi/180.)\n",
    "            y = np.cos(decDeg*np.pi/180.)*np.sin(raDeg*np.pi/180.)  \n",
    "            z = np.sin(decDeg*np.pi/180.)\n",
    "            xec, yec, zec = equatorial_to_ecliptic(np.array((x, y, z)))\n",
    "            \n",
    "            if filt.isspace():\n",
    "                filt = '-'\n",
    "            if mag.isspace():\n",
    "                mag = '----'\n",
    "            xh, yh, zh = Observatories.getObservatoryPosition(obsCode, jd_utc)\n",
    "            xhec, yhec, zhec = equatorial_to_ecliptic(np.array((xh, yh, zh)))\n",
    "            #outstring = \"%11s %s %4s %5s %s %13.6lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf\\n\"% \\\n",
    "            #      (provDesig, dateObs, obsCode, mag, filt, jd_tdb, x, y, z, xh, yh, zh, xec, yec, zec, xhec, yhec, zhec)\n",
    "            outstring = \"%11s %s %4s %5s %s %13.6lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf %12.7lf\\n\"% \\\n",
    "                (provDesig, dateObs, obsCode, mag, filt, jd_tdb, xec, yec, zec, xhec, yhec, zhec)\n",
    "            outfile.write(outstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TNO_tracklets, TNO_tracklets_jd_dict, TNO_sortedTracklets = get_sorted_tracklets_v5('data/UnnObs_Training_1_line_C_TNO.txt', 'data/UnnObs_Training_1_line_C_TNO_ec.mpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "separate_time_windows_v5(TNO_tracklets, TNO_sortedTracklets, TNO_tracklets_jd_dict, time_centers_lunation, file_stem='data/UnnObs_Training_1_line_C_TNO.mpc', dt=15., suff='.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linking Pure Dynamical Classes\n",
    "\n",
    "Now we need to do some actual linking for each of the dynamical classes, both with gamma=0.4 and with gamma values that are appropriate for the distance classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am doing an NEO run with the tracklet-based approach\n",
    "nside=8\n",
    "\n",
    "gs = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "g_select = 0.7\n",
    "gdots = [-0.008, -0.006, -0.004, -0.002, 0.0, 0.002, 0.004, 0.006, 0.008]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "    for dt in [1, 5, 10, 15, 20, 25, 30]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_NEO_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_NEO_%.1lf_pm15.0_%.1lf_gNEO_train.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt],  radii=np.arange(0.001, 0.060, 0.001))\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am doing an NEO run with the tracklet-based approach, but with an MBA distance\n",
    "nside=8\n",
    "gs = [0.4]\n",
    "g_select=0.4\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "    #for dt in np.arange(5, 35, 5):\n",
    "    for dt in [1]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_NEO_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_NEO_%.1lf_pm15.0_%.1lf_g0.4_train.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt], radii=np.arange(0.001, 0.060, 0.001))\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "    print(n)\n",
    "    pickle_input_filename = 'data/UnnObs_Training_1_line_C_NEO_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    pickle_filename = 'data/UnnObs_Training_1_line_C_NEO_%.1lf_pm15.0_%.1lf_gNEO_train.pickle' % (lunation_center(n), dt)\n",
    "    with open(pickle_input_filename, 'rb') as handle, open(pickle_filename, 'rb') as handle2:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "        run[n] = pickle.load(handle2)\n",
    "        true_count_sets[n], trackletCounter_dict=accessible_clusters(g_select, range(hp.nside2npix(nside)), tracklets_dict)\n",
    "\n",
    "neo_total_count = sum([len(k) for n, k in true_count_sets.items()])\n",
    "neo_total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_figure_data(ns, file_stem='data/UnnObs_Training_1_line_C_NEO_', suff='_gNEO_train.pickle'):\n",
    "    ds_results={}\n",
    "    nes_total_results={}\n",
    "    ncs_total_results={}\n",
    "    nts_total_results={}\n",
    "    for dt in [1, 5, 10, 15, 20, 25, 30]:\n",
    "        ncs, nps, nts = [], [], []\n",
    "        for n in ns[:1]:\n",
    "            run, _ = grab_run_pickled_results(n, dt, file_stem=file_stem, suff=suff)\n",
    "            ds, _, nes, _, ckeys, pkeys = run[0][dt]\n",
    "            nes_total = nes\n",
    "            ncs_total = [len(set(cks)) for cks in ckeys]\n",
    "            nps_total = [len(set(pks)) for pks in pkeys]\n",
    "            nts_total = [len(set(cks).union(set(pks))) for cks, pks in zip(ckeys, pkeys)]\n",
    "\n",
    "        for n in ns[1:]:\n",
    "            run, _ = grab_run_pickled_results(n, dt, file_stem=file_stem, suff=suff)\n",
    "            ds, _, nes, _, ckeys, pkeys = run[0][dt]\n",
    "            nes_total = [sum(x) for x in zip(nes, nes_total)]        \n",
    "            ncs = [len(set(cks)) for cks in ckeys]\n",
    "            ncs_total = [sum(x) for x in zip(ncs, ncs_total)]\n",
    "            nps = [len(set(pks)) for pks in pkeys]\n",
    "            nps_total = [sum(x) for x in zip(nps, nps_total)]        \n",
    "            nts = [len(set(cks).union(set(pks))) for cks, pks in zip(ckeys, pkeys)]\n",
    "            nts_total = [sum(x) for x in zip(nts, nts_total)] \n",
    "\n",
    "        ds_results[dt]=ds\n",
    "        nes_total_results[dt]=nes_total\n",
    "        ncs_total_results[dt]=ncs_total\n",
    "        nts_total_results[dt]=nts_total\n",
    "\n",
    "    return ds_results, ncs_total_results, nes_total_results, nts_total_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns=[-11, -14, -17, -20, -23]\n",
    "n=-11\n",
    "g=0.7\n",
    "\n",
    "\n",
    "fs=14\n",
    "f, (ax1, ax2) = plt.subplots(2, sharex=True, sharey=False, figsize=(8,12))\n",
    "\n",
    "ds, ncs_total, nes_total, nts_total = get_figure_data(ns, file_stem='data/UnnObs_Training_1_line_C_NEO_', suff='_g0.4_train.pickle')\n",
    "\n",
    "for k, v in sorted(ds.items()):\n",
    "    ax1.plot(ds[k], ncs_total[k], label=k)\n",
    "\n",
    "ds, ncs_total, nes_total, nts_total = get_figure_data(ns, file_stem='data/UnnObs_Training_1_line_C_NEO_', suff='_gNEO_train.pickle')\n",
    "\n",
    "for k, v in sorted(ds.items()):\n",
    "    ax2.plot(ds[k], ncs_total[k], label='')\n",
    "\n",
    "ax1.set_xscale(\"log\", nonposx='clip')\n",
    "#ax1.set_ylim((-200,3200))\n",
    "#ax2.set_ylim((-200,3200))\n",
    "#ax3.set_ylim((-200,3200))\n",
    "\n",
    "ax1.axhline(neo_total_count, ls='dashed')\n",
    "ax2.axhline(neo_total_count, ls='dashed')\n",
    "#ax3.axhline(total_count, ls='dashed')\n",
    "\n",
    "best_rad = 0.003\n",
    "#ax1.axvline(best_rad, ls='solid')\n",
    "#ax2.axvline(best_rad, ls='solid')\n",
    "#ax3.axvline(best_rad, ls='solid')\n",
    "\n",
    "ax2.set_xlabel('d (cluster radius)')\n",
    "ax1.set_ylabel('N pure clusters', fontsize=fs)\n",
    "ax2.set_ylabel('N pure clusters', fontsize=fs)\n",
    "ax1.text(0.001, 165, r'$\\gamma=0.4$', fontsize=15)\n",
    "ax2.text(0.001, 165, r'$\\gamma=0.5-0.9$', fontsize=15)\n",
    "\n",
    "ylow, yhigh = -10, 200\n",
    "\n",
    "ax1.set_ylim(ylow, yhigh)\n",
    "ax2.set_ylim(ylow, yhigh)\n",
    "\n",
    "MBAband_min=0.001\n",
    "MBAband_max = 0.003\n",
    "ax1.axvspan(MBAband_min, MBAband_max, alpha=0.25, color='gray')\n",
    "band_min=0.001\n",
    "band_max = 0.003\n",
    "ax2.axvspan(band_min, band_max, alpha=0.25, color='gray')\n",
    "#ax3.axvspan(band_min, band_max, 0.004, alpha=0.25, color='gray')\n",
    "\n",
    "#ax2.tick_params(direction='in')\n",
    "ax1.legend(loc='lower right', fontsize=12, title='dt')\n",
    "#plt.savefig('nclusters_vs_d_g0.4.pdf')\n",
    "f.subplots_adjust(hspace=0.1)\n",
    "#f.tight_layout()\n",
    "f.savefig('NEOs_lunations.png')\n",
    "f.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trojans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Here I am doing an Trojan run with the tracklet-based approach\n",
    "nside=8\n",
    "\n",
    "gs = [0.2]\n",
    "g_select = 0.2\n",
    "gdots = [-0.008, -0.006, -0.004, -0.002, 0.0, 0.002, 0.004, 0.006, 0.008]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "# Needed to skip -16 because there was no data in that time window.\n",
    "for n in [-11, -12, -13, -14, -15, -17, -18, -19, -20, -21, -22, -23]:\n",
    "#for n in [-22, -23]:    \n",
    "    print(n)\n",
    "#    for dt in np.arange(5, 35, 5):\n",
    "    for dt in [1]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_TRO_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_TRO_%.1lf_pm15.0_%.1lf_g0.2_train.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt],  radii=np.arange(0.0001, 0.0400, 0.0002))\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am doing an Trojan run with the tracklet-based approach\n",
    "# Here is a second try, using a different set of gamma-dots\n",
    "nside=8\n",
    "\n",
    "gs = [0.2]\n",
    "g_select = 0.2\n",
    "gdot_max = 6.5e-3*np.power(g_select/0.4, 1.5)\n",
    "np.linspace(-gdot_max, gdot_max, 9)\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "# Needed to skip -16 because there was no data in that time window.\n",
    "for n in [-11, -12, -13, -14, -15, -17, -18, -19, -20, -21, -22, -23]:\n",
    "#for n in [-22, -23]:    \n",
    "    print(n)\n",
    "    for dt in [1, 5, 10, 15, 20, 25, 30]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_TRO_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_TRO_%.1lf_pm15.0_%.1lf_g0.2_train_v2.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt],  radii=np.arange(0.0001, 0.0400, 0.0002))\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am doing a Trojan run with the tracklet-based approach, but with an MBA distance\n",
    "nside=8\n",
    "gs = [0.4]\n",
    "g_select=0.4\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "# Needed to skip -16 because there was no data in that time window.\n",
    "for n in [-11, -12, -13, -14, -15, -17, -18, -19, -20, -21, -22, -23]:\n",
    "    for dt in [1, 5, 10, 15, 20, 25, 30]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_TRO_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_TRO_%.1lf_pm15.0_%.1lf_g0.4_train.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt],  radii=np.arange(0.0001, 0.0400, 0.0002))\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "# Nothing in -16\n",
    "for n in [-11, -12, -13, -14, -15, -17, -18, -19, -20, -21, -22, -23]:\n",
    "    print(n)\n",
    "    pickle_input_filename = 'data/UnnObs_Training_1_line_C_TRO_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    pickle_filename = 'data/UnnObs_Training_1_line_C_TRO_%.1lf_pm15.0_%.1lf_g0.4_train.pickle' % (lunation_center(n), dt)\n",
    "    with open(pickle_input_filename, 'rb') as handle, open(pickle_filename, 'rb') as handle2:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "        run[n] = pickle.load(handle2)\n",
    "        true_count_sets[n], trackletCounter_dict=accessible_clusters(g_select, range(hp.nside2npix(nside)), tracklets_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tro_total_count = sum([len(k) for n, k in true_count_sets.items()])\n",
    "tro_total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns=[-11, -12, -13, -14, -15, -17, -18, -19, -20, -21, -22, -23]\n",
    "n=-11\n",
    "g=0.7\n",
    "\n",
    "\n",
    "fs=14\n",
    "f, (ax1, ax2) = plt.subplots(2, sharex=True, sharey=False, figsize=(8,12))\n",
    "\n",
    "ds, ncs_total, nes_total, nts_total = get_figure_data(ns, file_stem='data/UnnObs_Training_1_line_C_TRO_', suff='_g0.4_train.pickle')\n",
    "\n",
    "for k, v in sorted(ds.items()):\n",
    "    ax1.plot(ds[k], ncs_total[k], label=k)\n",
    "\n",
    "ds, ncs_total, nes_total, nts_total = get_figure_data(ns, file_stem='data/UnnObs_Training_1_line_C_TRO_', suff='_g0.2_train.pickle')\n",
    "\n",
    "for k, v in sorted(ds.items()):\n",
    "    ax2.plot(ds[k], ncs_total[k], label='')\n",
    "\n",
    "ax1.set_xscale(\"log\", nonposx='clip')\n",
    "#ax1.set_ylim((-200,3200))\n",
    "#ax2.set_ylim((-200,3200))\n",
    "#ax3.set_ylim((-200,3200))\n",
    "\n",
    "ax1.axhline(tro_total_count, ls='dashed')\n",
    "ax2.axhline(tro_total_count, ls='dashed')\n",
    "#ax3.axhline(total_count, ls='dashed')\n",
    "\n",
    "best_rad = 0.003\n",
    "#ax1.axvline(best_rad, ls='solid')\n",
    "#ax2.axvline(best_rad, ls='solid')\n",
    "#ax3.axvline(best_rad, ls='solid')\n",
    "\n",
    "ax2.set_xlabel('d (cluster radius)')\n",
    "ax1.set_ylabel('N pure clusters', fontsize=fs)\n",
    "ax2.set_ylabel('N pure clusters', fontsize=fs)\n",
    "ax1.text(0.0002, 140, r'$\\gamma=0.4$', fontsize=15)\n",
    "ax2.text(0.0002, 140, r'$\\gamma=0.2$', fontsize=15)\n",
    "\n",
    "ylow, yhigh = -0.5, 170\n",
    "\n",
    "ax1.set_ylim(ylow, yhigh)\n",
    "ax2.set_ylim(ylow, yhigh)\n",
    "\n",
    "MBAband_min=0.001\n",
    "MBAband_max = 0.003\n",
    "ax1.axvspan(MBAband_min, MBAband_max, alpha=0.25, color='gray')\n",
    "band_min=0.001\n",
    "band_max = 0.003\n",
    "ax2.axvspan(band_min, band_max, alpha=0.25, color='gray')\n",
    "#ax3.axvspan(band_min, band_max, 0.004, alpha=0.25, color='gray')\n",
    "\n",
    "#ax2.tick_params(direction='in')\n",
    "ax1.legend(loc='lower right', fontsize=12, title='dt')\n",
    "#plt.savefig('nclusters_vs_d_g0.4.pdf')\n",
    "f.subplots_adjust(hspace=0.1)\n",
    "#f.tight_layout()\n",
    "f.savefig('TROs_lunations.png')\n",
    "f.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centaurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am doing a Centaur run with the tracklet-based approach\n",
    "nside=8\n",
    "\n",
    "gs = [0.1]\n",
    "g_select = 0.1\n",
    "gdots = [-0.008, -0.006, -0.004, -0.002, 0.0, 0.002, 0.004, 0.006, 0.008]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-11, -12, -13, -14, -15, -16, -17, -18, -19, -20, -21, -22, -23]:\n",
    "    for dt in [1, 5, 10, 15, 20, 25, 30]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_CEN_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_CEN_%.1lf_pm15.0_%.1lf_g0.1_train.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt],  radii=np.arange(0.0001, 0.0400, 0.0002))\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am doing a Centaur run with the tracklet-based approach, but with an MBA distance\n",
    "nside=8\n",
    "gs = [0.4]\n",
    "g_select=0.4\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "# Needed to skip -16 because there was no data in that time window.\n",
    "for n in [-11, -12, -13, -14, -15, -17, -18, -19, -20, -21, -22, -23]:\n",
    "    for dt in [1, 5, 10, 15, 20, 25, 30]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_CEN_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_CEN_%.1lf_pm15.0_%.1lf_g0.4_train.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt],  radii=np.arange(0.0001, 0.0400, 0.0002))\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "# Nothing in -16\n",
    "for n in [-11, -12, -13, -14, -15, -16, -17, -18, -19, -20, -21, -22, -23]:\n",
    "    print(n)\n",
    "    pickle_input_filename = 'data/UnnObs_Training_1_line_C_CEN_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    pickle_filename = 'data/UnnObs_Training_1_line_C_CEN_%.1lf_pm15.0_%.1lf_g0.4_train.pickle' % (lunation_center(n), dt)\n",
    "    with open(pickle_input_filename, 'rb') as handle, open(pickle_filename, 'rb') as handle2:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "        run[n] = pickle.load(handle2)\n",
    "        true_count_sets[n], trackletCounter_dict=accessible_clusters(g_select, range(hp.nside2npix(nside)), tracklets_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cen_total_count = sum([len(k) for n, k in true_count_sets.items()])\n",
    "cen_total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns=[-11, -12, -13, -14, -15, -17, -18, -19, -20, -21, -22, -23]\n",
    "n=-11\n",
    "g=0.7\n",
    "\n",
    "\n",
    "fs=14\n",
    "f, (ax1, ax2) = plt.subplots(2, sharex=True, sharey=False, figsize=(8,12))\n",
    "\n",
    "ds, ncs_total, nes_total, nts_total = get_figure_data(ns, file_stem='data/UnnObs_Training_1_line_C_CEN_', suff='_g0.4_train.pickle')\n",
    "\n",
    "for k, v in sorted(ds.items()):\n",
    "    ax1.plot(ds[k], ncs_total[k], label=k)\n",
    "\n",
    "ds, ncs_total, nes_total, nts_total = get_figure_data(ns, file_stem='data/UnnObs_Training_1_line_C_CEN_', suff='_g0.1_train.pickle')\n",
    "\n",
    "for k, v in sorted(ds.items()):\n",
    "    ax2.plot(ds[k], ncs_total[k], label='')\n",
    "\n",
    "ax1.set_xscale(\"log\", nonposx='clip')\n",
    "#ax1.set_ylim((-200,3200))\n",
    "#ax2.set_ylim((-200,3200))\n",
    "#ax3.set_ylim((-200,3200))\n",
    "\n",
    "ax1.axhline(cen_total_count, ls='dashed')\n",
    "ax2.axhline(cen_total_count, ls='dashed')\n",
    "#ax3.axhline(total_count, ls='dashed')\n",
    "\n",
    "best_rad = 0.003\n",
    "#ax1.axvline(best_rad, ls='solid')\n",
    "#ax2.axvline(best_rad, ls='solid')\n",
    "#ax3.axvline(best_rad, ls='solid')\n",
    "\n",
    "ax2.set_xlabel('d (cluster radius)')\n",
    "ax1.set_ylabel('N pure clusters', fontsize=fs)\n",
    "ax2.set_ylabel('N pure clusters', fontsize=fs)\n",
    "ax1.text(0.0002, 22, r'$\\gamma=0.4$', fontsize=15)\n",
    "ax2.text(0.0002, 22, r'$\\gamma=0.1$', fontsize=15)\n",
    "\n",
    "ylow, yhigh = -0.5, 32\n",
    "\n",
    "ax1.set_ylim(ylow, yhigh)\n",
    "ax2.set_ylim(ylow, yhigh)\n",
    "\n",
    "MBAband_min=0.001\n",
    "MBAband_max = 0.003\n",
    "ax1.axvspan(MBAband_min, MBAband_max, alpha=0.25, color='gray')\n",
    "band_min=0.001\n",
    "band_max = 0.003\n",
    "ax2.axvspan(band_min, band_max, alpha=0.25, color='gray')\n",
    "#ax3.axvspan(band_min, band_max, 0.004, alpha=0.25, color='gray')\n",
    "\n",
    "#ax2.tick_params(direction='in')\n",
    "ax1.legend(loc='lower right', fontsize=12, title='dt')\n",
    "#plt.savefig('nclusters_vs_d_g0.4.pdf')\n",
    "f.subplots_adjust(hspace=0.1)\n",
    "#f.tight_layout()\n",
    "f.savefig('CENs_lunations.png')\n",
    "f.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TNOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am doing a TNO run with the tracklet-based approach\n",
    "nside=8\n",
    "\n",
    "gs = [0.03]\n",
    "g_select = 0.03\n",
    "gdots = [-0.008, -0.006, -0.004, -0.002, 0.0, 0.002, 0.004, 0.006, 0.008]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-11, -12, -13, -14, -15, -16, -17, -18, -19, -20, -21, -22, -23]:\n",
    "    for dt in [1, 5, 10, 15, 20, 25, 30]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_TNO_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_TNO_%.1lf_pm15.0_%.1lf_g0.03_train.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt],  radii=np.arange(0.0001, 0.1000, 0.0002))\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am doing a TNO run with the tracklet-based approach, but with an MBA distance\n",
    "nside=8\n",
    "gs = [0.4]\n",
    "g_select=0.4\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-11, -12, -13, -14, -15, -16, -17, -18, -19, -20, -21, -22, -23]:\n",
    "    for dt in [1, 5, 10, 15, 20, 25, 30]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_TNO_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_TNO_%.1lf_pm15.0_%.1lf_g0.4_train.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt],  radii=np.arange(0.0001, 0.1000, 0.0002))\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "for n in [-11, -12, -13, -14, -15, -16, -17, -18, -19, -20, -21, -22, -23]:\n",
    "    print(n)\n",
    "    pickle_input_filename = 'data/UnnObs_Training_1_line_C_TNO_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    pickle_filename = 'data/UnnObs_Training_1_line_C_TNO_%.1lf_pm15.0_%.1lf_g0.4_train.pickle' % (lunation_center(n), dt)\n",
    "    with open(pickle_input_filename, 'rb') as handle, open(pickle_filename, 'rb') as handle2:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "        run[n] = pickle.load(handle2)\n",
    "        true_count_sets[n], trackletCounter_dict=accessible_clusters(g_select, range(hp.nside2npix(nside)), tracklets_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tno_total_count = sum([len(k) for n, k in true_count_sets.items()])\n",
    "tno_total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns=[-11, -12, -13, -14, -15, -16, -17, -18, -19, -20, -21, -22, -23]\n",
    "n=-11\n",
    "g=0.7\n",
    "\n",
    "\n",
    "fs=14\n",
    "f, (ax1, ax2) = plt.subplots(2, sharex=True, sharey=False, figsize=(8,12))\n",
    "\n",
    "ds, ncs_total, nes_total, nts_total = get_figure_data(ns, file_stem='data/UnnObs_Training_1_line_C_TNO_', suff='_g0.4_train.pickle')\n",
    "\n",
    "for k, v in sorted(ds.items()):\n",
    "    ax1.plot(ds[k], ncs_total[k], label=k)\n",
    "\n",
    "ds, ncs_total, nes_total, nts_total = get_figure_data(ns, file_stem='data/UnnObs_Training_1_line_C_TNO_', suff='_g0.03_train.pickle')\n",
    "\n",
    "for k, v in sorted(ds.items()):\n",
    "    ax2.plot(ds[k], ncs_total[k], label='')\n",
    "\n",
    "ax1.set_xscale(\"log\", nonposx='clip')\n",
    "#ax1.set_ylim((-200,3200))\n",
    "#ax2.set_ylim((-200,3200))\n",
    "#ax3.set_ylim((-200,3200))\n",
    "\n",
    "ax1.axhline(tno_total_count, ls='dashed')\n",
    "ax2.axhline(tno_total_count, ls='dashed')\n",
    "#ax3.axhline(total_count, ls='dashed')\n",
    "\n",
    "best_rad = 0.003\n",
    "#ax1.axvline(best_rad, ls='solid')\n",
    "#ax2.axvline(best_rad, ls='solid')\n",
    "#ax3.axvline(best_rad, ls='solid')\n",
    "\n",
    "ax2.set_xlabel('d (cluster radius)')\n",
    "ax1.set_ylabel('N pure clusters', fontsize=fs)\n",
    "ax2.set_ylabel('N pure clusters', fontsize=fs)\n",
    "ax1.text(0.0003, 12, r'$\\gamma=0.4$', fontsize=15)\n",
    "ax2.text(0.0003, 12, r'$\\gamma=0.03$', fontsize=15)\n",
    "\n",
    "ylow, yhigh = -0.5, 16\n",
    "\n",
    "ax1.set_ylim(ylow, yhigh)\n",
    "ax2.set_ylim(ylow, yhigh)\n",
    "\n",
    "MBAband_min=0.001\n",
    "MBAband_max = 0.003\n",
    "ax1.axvspan(MBAband_min, MBAband_max, alpha=0.25, color='gray')\n",
    "band_min=0.001\n",
    "band_max = 0.003\n",
    "ax2.axvspan(band_min, band_max, alpha=0.25, color='gray')\n",
    "#ax3.axvspan(band_min, band_max, 0.004, alpha=0.25, color='gray')\n",
    "\n",
    "#ax2.tick_params(direction='in')\n",
    "ax1.legend(loc='lower right', fontsize=12, title='dt')\n",
    "#plt.savefig('nclusters_vs_d_g0.4.pdf')\n",
    "f.subplots_adjust(hspace=0.1)\n",
    "#f.tight_layout()\n",
    "f.savefig('TNOs_lunations.png')\n",
    "f.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Now that we have done runs to determine the hyper-parameters, we will now test those parameters on the data from other lunations.  We'll start with the MBA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am updating this to do the MBA training runs with the tracklet-base approach\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "dt = 5\n",
    "rad = 2e-3\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "#for n in [-11, -14, -17, -20, -23]:\n",
    "for n in [-12, -15, -18, -21, -24]:\n",
    "    for dt in [dt]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_run_v5.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt], radii=[rad])\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "dt = 5\n",
    "rad = 2e-3\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-12, -15, -18, -21, -24]:\n",
    "    for dt in [5]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_run_v4.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, dt, rad, g_gdots=g_gdots)\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "dt = 5\n",
    "rad = 2e-3\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-12, -15, -18, -21, -24]:\n",
    "    for dt in [5]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_run_v4.1.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_run_v4(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, dt, rad, g_gdots=g_gdots)\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am trying to make sense of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "true_count_sets_v2={}\n",
    "trackletCounter_dict={}\n",
    "trackletCounter_dict_v2={}\n",
    "tracklets_dict={}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "for n in [-12, -15, -18, -21, -24]:\n",
    "#for n in [-14]:\n",
    "    print(n)\n",
    "    pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    pickle_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_run_v4.pickle' % (lunation_center(n), dt)\n",
    "    with open(pickle_input_filename, 'rb') as handle, open(pickle_filename, 'rb') as handle2:\n",
    "        tracklets_dict[n], _, _  = pickle.load(handle)\n",
    "        run[n] = pickle.load(handle2)\n",
    "        true_count_sets[n], trackletCounter_dict[n]=accessible_clusters(g_select, range(hp.nside2npix(nside)), tracklets_dict[n])\n",
    "        true_count_sets_v2[n], trackletCounter_dict_v2[n]=accessible_clusters(g_select, range(hp.nside2npix(nside)), tracklets_dict[n], mincount=2)\n",
    "\n",
    "total_count = sum([len(k) for n, k in true_count_sets.items()])\n",
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "true_count_sets_v2={}\n",
    "trackletCounter_dict={}\n",
    "trackletCounter_dict_v2={}\n",
    "tracklets_dict={}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "for n in [-12, -15, -18, -21, -24]:\n",
    "#for n in [-14]:\n",
    "    print(n)\n",
    "    pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    pickle_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_run_v4.1.pickle' % (lunation_center(n), dt)\n",
    "    with open(pickle_input_filename, 'rb') as handle, open(pickle_filename, 'rb') as handle2:\n",
    "        tracklets_dict[n], _, _  = pickle.load(handle)\n",
    "        run[n] = pickle.load(handle2)\n",
    "        true_count_sets[n], trackletCounter_dict[n]=accessible_clusters_v2(g_select, range(hp.nside2npix(nside)), tracklets_dict[n])\n",
    "        true_count_sets_v2[n], _ =accessible_clusters_v2(g_select, range(hp.nside2npix(nside)), tracklets_dict[n], mincount=2)\n",
    "\n",
    "total_count = sum([len(k) for n, k in true_count_sets.items()])\n",
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(true_count_sets[-24][364]), len(true_count_sets_v2[-24][364])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trackletCounter_dict[-24][364]['K14U49Q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=-24\n",
    "success_tracklets=set()\n",
    "for pix in range(hp.nside2npix(nside)):\n",
    "    if pix in run[n][0]:\n",
    "        for k, v in unique_clusters_v3(run[-24][0][pix])[0].items():\n",
    "            for tracklet in v[1]:\n",
    "                success_tracklets.update({tracklet})\n",
    "                \n",
    "\n",
    "potential_tracklets=set()\n",
    "for pix in range(hp.nside2npix(nside)):\n",
    "    if pix in run[n][0]:\n",
    "        for k, v in unique_clusters_v3(run[-24][0][pix])[2].items():\n",
    "            for tracklet in v[1]:\n",
    "                if tracklet[0]==k and (tracklet not in success_tracklets):\n",
    "                    potential_tracklets.update({tracklet})\n",
    "            \n",
    "\n",
    "failure_tracklets=set()\n",
    "for pix in range(hp.nside2npix(nside)):\n",
    "    if pix in run[n][0]:\n",
    "        for key in unique_clusters_v3(run[-24][0][pix])[1].keys():\n",
    "            for tracklet in key:\n",
    "                if (tracklet not in success_tracklets) and (tracklet not in potential_tracklets):\n",
    "                    failure_tracklets.update({tracklet})\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=-24\n",
    "indirect_success_tracklets=set()\n",
    "for tracklet in success_tracklets:\n",
    "    trkSub = tracklet[0]\n",
    "    for pix in range(hp.nside2npix(nside)):\n",
    "        for  trcklt in trackletCounter_dict[n][pix][trkSub]:\n",
    "            indirect_success_tracklets.update({trcklt})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trackletCounter_dict[-24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(success_tracklets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(indirect_success_tracklets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "designation_dict=defaultdict(set)\n",
    "for tracklet in indirect_success_tracklets:\n",
    "    designation_dict[tracklet[0]].update({tracklet})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tracklets=set()\n",
    "for tracklet in indirect_success_tracklets:\n",
    "    for trcklt in designation_dict[tracklet[0]]:\n",
    "        all_tracklets.update({trcklt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(all_tracklets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(tracklets_dict[-24]), len(success_tracklets), len(potential_tracklets), len(failure_tracklets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn_sum, tc_sum = 0, 0\n",
    "for n in [-12, -15, -18, -21, -24]:\n",
    "    for pix in range(hp.nside2npix(nside)):\n",
    "        if pix in run[n][0]:\n",
    "            fn = found_clusters(run[n][0][pix].keys())\n",
    "            tc = true_count_sets[n][pix]\n",
    "            fn_sum += len(fn)\n",
    "            tc_sum += len(tc)\n",
    "            print(n, pix, len(fn), len(tc))\n",
    "print(fn_sum, tc_sum, fn_sum/tc_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(success_tracklets), len(potential_tracklets), len(failure_tracklets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am doing an NEO run with the tracklet-based approach\n",
    "nside=8\n",
    "\n",
    "gs = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "g_select = 0.7\n",
    "gdots = [-0.008, -0.006, -0.004, -0.002, 0.0, 0.002, 0.004, 0.006, 0.008]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "rad=3e-2\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-12, -15, -18, -21, -24]:\n",
    "    for dt in [5]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_test_select.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            # Read in the full data\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            # Cull out the MBAs\n",
    "            culled_tracklets_dict, culled_tracklets_jd_dict, culled_sortedTracklets = index_tracklets_gamma_select(0.4, 0.0, t_ref, tracklets_dict, tracklets_jd_dict, adot_min=3.5e-3, adot_max=5.5e-3, bdot_min=-2e-3, bdot_max=2e-3, nside=8)\n",
    "            # Do the run on the set of culled tracklets.\n",
    "            run = do_run_v3(range(hp.nside2npix(nside)), culled_tracklets_dict,  t_ref, g_select, dt, rad, transform_to_arrows_for_regions_function=transform_to_arrows_for_regions_gamma_select, g_gdots=g_gdots)\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am reading tracklet data and writing out culled tracklet data.\n",
    "nside=8\n",
    "\n",
    "gs = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "g_select = 0.7\n",
    "gdots = [-0.008, -0.006, -0.004, -0.002, 0.0, 0.002, 0.004, 0.006, 0.008]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "rad=3e-2\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-11, -12, -13, -14, -15, -16, -17, -18, -19, -20, -21, -22, -23, -24]:\n",
    "    for dt in [5]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_culled.pickle' % (lunation_center(n))\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            # Read in the full data\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            # Cull out the MBAs\n",
    "            culled_tracklets_dict, culled_tracklets_jd_dict, culled_sortedTracklets = index_tracklets_gamma_select(0.4, 0.0, t_ref, tracklets_dict, tracklets_jd_dict, adot_min=3.5e-3, adot_max=5.5e-3, bdot_min=-2e-3, bdot_max=2e-3, nside=8)\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump((culled_tracklets_dict, culled_tracklets_jd_dict, culled_sortedTracklets), out_handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am doing an NEO run with the tracklet-based approach, but with the culled data set.\n",
    "nside=8\n",
    "\n",
    "gs = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "g_select = 0.7\n",
    "gdots = [-0.008, -0.006, -0.004, -0.002, 0.0, 0.002, 0.004, 0.006, 0.008]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "rad=3e-2\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "    for dt in [5]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_culled.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_test_select_culled.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            # Read in the full data\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            # Do the run on the set of culled tracklets.\n",
    "            run = do_run_v4(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, dt, rad, transform_to_arrows_for_regions_function=transform_to_arrows_for_regions_gamma_select, g_gdots=g_gdots)\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to check (1) which NEO tracklets were excluded by the culling and (2) which NEOs were found in the post-culling data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing the culling process on the training lunations.  What I hope I am doing here is to\n",
    "# see what fraction of the different dynamical categories of objects might be thrown out by\n",
    "# the rate-based culling process.\n",
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "\n",
    "in_culled = defaultdict(list)\n",
    "not_in_culled = defaultdict(list)\n",
    "\n",
    "for n in [-12, -15, -18, -21, -24]:\n",
    "    print(n)\n",
    "    t_ref = lunation_center(n)\n",
    "    pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    with open(pickle_input_filename, 'rb') as handle:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "        # Cull out the MBAs\n",
    "        culled_tracklets_dict, culled_tracklets_jd_dict, culled_sortedTracklets = index_tracklets_gamma_select(0.4, 0.0, t_ref, tracklets_dict, tracklets_jd_dict, adot_min=3.5e-3, adot_max=5.5e-3, bdot_min=-2e-3, bdot_max=2e-3, nside=8)\n",
    "        # Tally up what which ones were and were not culled\n",
    "        for tracklet in tracklets_dict:\n",
    "            for label in ['NEO', 'TRO', 'CEN', 'TNO', 'MBA']:\n",
    "                if tracklet[0] in typeDict__[label]:\n",
    "                    if tracklet in culled_tracklets_dict:\n",
    "                        in_culled[label].append(tracklet)\n",
    "                    else:\n",
    "                        not_in_culled[label].append(tracklet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(in_culled['NEO']), len(not_in_culled['NEO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(in_culled['TRO']), len(not_in_culled['TRO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(in_culled['CEN']), len(not_in_culled['CEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(in_culled['TNO']), len(not_in_culled['TNO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "    print(n)\n",
    "    pickle_input_filename = 'data/UnnObs_Training_1_line_C_NEO_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    pickle_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_NEO_test_select_v0.pickle' % (lunation_center(n), dt)\n",
    "                \n",
    "    with open(pickle_input_filename, 'rb') as handle, open(pickle_filename, 'rb') as handle2:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "        run[n] = pickle.load(handle2)\n",
    "        true_count_sets[n], trackletCounter_dict=accessible_clusters(g_select, range(hp.nside2npix(nside)), tracklets_dict)\n",
    "\n",
    "neo_total_count = sum([len(k) for n, k in true_count_sets.items()])\n",
    "neo_total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "trackletCounter_dict={}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "for n in [-12, -15, -18, -21, -24]:\n",
    "    print(n)\n",
    "    pickle_input_filename = 'data/UnnObs_Training_1_line_C_NEO_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    pickle_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_NEO_test_select_0.1.pickle' % (lunation_center(n), dt)\n",
    "                \n",
    "    with open(pickle_input_filename, 'rb') as handle, open(pickle_filename, 'rb') as handle2:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "        culled_tracklets_dict, culled_tracklets_jd_dict, culled_sortedTracklets = index_tracklets_gamma_select(0.4, 0.0, t_ref, tracklets_dict, tracklets_jd_dict, adot_min=3.5e-3, adot_max=5.5e-3, bdot_min=-2e-3, bdot_max=2e-3, nside=8)\n",
    "        run[n] = pickle.load(handle2)\n",
    "        true_count_sets[n], trackletCounter_dict[n]=accessible_clusters_v2(g_select, range(hp.nside2npix(nside)), culled_tracklets_dict)\n",
    "\n",
    "#neo_total_count = sum([len(k) for n, k in true_count_sets.items()])\n",
    "#neo_total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "true_count_sets_v2={}\n",
    "trackletCounter_dict={}\n",
    "trackletCounter_dict_v2={}\n",
    "tracklets_dict={}\n",
    "culled_tracklets_dict={}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "for n in [-12, -15, -18, -21, -24]:\n",
    "#for n in [-14]:\n",
    "    print(n)\n",
    "    pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    pickle_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_NEO_test_select_0.1.pickle' % (lunation_center(n), dt)\n",
    "\n",
    "    with open(pickle_input_filename, 'rb') as handle, open(pickle_filename, 'rb') as handle2:\n",
    "        tracklets_dict[n], tracklets_jd_dict[n], sortedTracklets,  = pickle.load(handle)\n",
    "        culled_tracklets_dict[n], _, _ = index_tracklets_gamma_select(0.4, 0.0, t_ref, tracklets_dict[n], tracklets_jd_dict[n], adot_min=3.5e-3, adot_max=5.5e-3, bdot_min=-2e-3, bdot_max=2e-3, nside=8)\n",
    "\n",
    "        run[n] = pickle.load(handle2)\n",
    "        true_count_sets[n], trackletCounter_dict[n]=accessible_clusters_v2(g_select, range(hp.nside2npix(nside)), culled_tracklets_dict[n])\n",
    "        true_count_sets_v2[n], _ =accessible_clusters_v2(g_select, range(hp.nside2npix(nside)), culled_tracklets_dict[n], mincount=2)\n",
    "\n",
    "#total_count = sum([len(k) for n, k in true_count_sets.items()])\n",
    "#total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_count_sets[-24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn_sum, tc_sum = 0, 0\n",
    "for n in [-12, -15, -18, -21, -24]:\n",
    "    for pix in range(hp.nside2npix(nside)):\n",
    "        if pix in run[n][0]:\n",
    "            fn = found_clusters(run[n][0][pix].keys())\n",
    "            tc = true_count_sets[n][pix]\n",
    "            fn_sum += len(fn)\n",
    "            tc_sum += len(tc)\n",
    "            print(n, pix, len(fn), len(tc))\n",
    "print(fn_sum, tc_sum, fn_sum/tc_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am doing a Trojan test run with the tracklet-based approach, but with the culled data set.\n",
    "nside=8\n",
    "\n",
    "gs = [0.2]\n",
    "g_select = 0.2\n",
    "gdots = [-0.008, -0.006, -0.004, -0.002, 0.0, 0.002, 0.004, 0.006, 0.008]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "rad=3e-2\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "    for dt in [5]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_TRO_test_select_v0.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            # Read in the full data\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            # Cull out the MBAs\n",
    "            culled_tracklets_dict, culled_tracklets_jd_dict, culled_sortedTracklets = index_tracklets_gamma_select(0.4, 0.0, t_ref, tracklets_dict, tracklets_jd_dict, adot_min=3.5e-3, adot_max=5.5e-3, bdot_min=-2e-3, bdot_max=2e-3, nside=8)\n",
    "            # Do the run on the set of culled tracklets.\n",
    "            run = do_run_v3(range(hp.nside2npix(nside)), culled_tracklets_dict,  t_ref, g_select, dt, rad, transform_to_arrows_for_regions_function=transform_to_arrows_for_regions_gamma_select, g_gdots=g_gdots)\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am doing a TNO test run with the tracklet-based approach\n",
    "nside=8\n",
    "\n",
    "gs = [0.03]\n",
    "g_select = 0.03\n",
    "gdots = [-0.008, -0.006, -0.004, -0.002, 0.0, 0.002, 0.004, 0.006, 0.008]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "rad=3e-2\n",
    "\n",
    "# Looping over five lunation centers, separated by 3 months each\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "    for dt in [5]:\n",
    "        pix_runs = {}\n",
    "        t_ref = lunation_center(n)\n",
    "        pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "        pickle_output_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_TNO_test_select_v0.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            # Read in the full data\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            # Cull out the MBAs\n",
    "            culled_tracklets_dict, culled_tracklets_jd_dict, culled_sortedTracklets = index_tracklets_gamma_select(0.4, 0.0, t_ref, tracklets_dict, tracklets_jd_dict, adot_min=3.5e-3, adot_max=5.5e-3, bdot_min=-2e-3, bdot_max=2e-3, nside=8)\n",
    "            # Do the run on the set of culled tracklets.\n",
    "            run = do_run_v3(range(hp.nside2npix(nside)), culled_tracklets_dict,  t_ref, g_select, dt, rad, transform_to_arrows_for_regions_function=transform_to_arrows_for_regions_gamma_select, g_gdots=g_gdots)\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = [0.2]\n",
    "g_select = 0.2\n",
    "gdot_max = 6.5e-3*np.power(g_select/0.4, 1.5)\n",
    "np.linspace(-gdot_max, gdot_max, 9)\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "for n in [-12, -15, -18, -21, -24]:\n",
    "    print(n)\n",
    "    pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    pickle_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0_%.1lf_run_v5.pickle' % (lunation_center(n), dt)\n",
    "    with open(pickle_input_filename, 'rb') as handle, open(pickle_filename, 'rb') as handle2:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "        run[n] = pickle.load(handle2)\n",
    "        true_count_sets[n], trackletCounter_dict=accessible_clusters_v2(g_select, range(hp.nside2npix(nside)), tracklets_dict)\n",
    "\n",
    "\n",
    "total_count = sum([len(k) for n, k in true_count_sets.items()])\n",
    "\n",
    "total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trackletCounter_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "culled_tracklets_dict, culled_tracklets_jd_dict, culled_sortedTracklets = index_tracklets_gamma_select(0.4, 0.0, lunation_center(-24), tracklets_dict, tracklets_jd_dict, adot_min=3.5e-3, adot_max=5.5e-3, bdot_min=-2e-3, bdot_max=2e-3, nside=8)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(tracklets_dict), len(culled_tracklets_dict), len(culled_tracklets_jd_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Testing the culling process on the training lunations.  What I hope I am doing here is to\n",
    "# see what fraction of the different dynamical categories of objects might be thrown out by\n",
    "# the rate-based culling process.\n",
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "\n",
    "in_culled = defaultdict(list)\n",
    "not_in_culled = defaultdict(list)\n",
    "\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "    print(n)\n",
    "    t_ref = lunation_center(n)\n",
    "    pickle_input_filename = 'data/UnnObs_Training_1_line_C_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    with open(pickle_input_filename, 'rb') as handle:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "        # Cull out the MBAs\n",
    "        culled_tracklets_dict, culled_tracklets_jd_dict, culled_sortedTracklets = index_tracklets_gamma_select(0.4, 0.0, t_ref, tracklets_dict, tracklets_jd_dict, adot_min=3.5e-3, adot_max=5.5e-3, bdot_min=-2e-3, bdot_max=2e-3, nside=8)\n",
    "        # Tally up what which ones were and were not culled\n",
    "        for tracklet in tracklets_dict:\n",
    "            for label in ['NEO', 'TRO', 'CEN', 'TNO', 'MBA']:\n",
    "                if tracklet[0] in typeDict__[label]:\n",
    "                    if tracklet in culled_tracklets_dict:\n",
    "                        in_culled[label].append(tracklet)\n",
    "                    else:\n",
    "                        not_in_culled[label].append(tracklet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(in_culled['NEO']), len(not_in_culled['NEO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(in_culled['TRO']), len(not_in_culled['TRO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(in_culled['CEN']), len(not_in_culled['CEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(in_culled['TNO']), len(not_in_culled['TNO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_select=0.03\n",
    "results = accessible_clusters(g_select, range(hp.nside2npix(nside)), culled_tracklets_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "culled_tracklets_jd_dict[('K14S21H',  57003,  'F51')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run[-15][0][5][4][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for key, v in run[-12][1][(5, 0.002)].items():\n",
    "    print(key, v)\n",
    "    print(len(key), len(v))\n",
    "    print()\n",
    "    i += 1\n",
    "    if i>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pure_count=0\n",
    "error_count=0\n",
    "valid_count=0\n",
    "for n in [-12, -15, -18, -21, -24]:\n",
    "    pure_count += run[n][0][5][1][0]\n",
    "    error_count += run[n][0][5][2][0]\n",
    "    valid_count += run[n][0][5][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pure_count=0\n",
    "error_count=0\n",
    "valid_count=0\n",
    "for n in [-12, -15, -18, -21, -24]:\n",
    "#for n in [-12]:\n",
    "    print(run[n][0][5][1], run[n][0][5][2], run[n][0][5][3])\n",
    "    pure_count += run[n][0][5][1][0]\n",
    "    error_count += run[n][0][5][2][0]\n",
    "    valid_count += run[n][0][5][3][0]\n",
    "pure_count, error_count, valid_count, pure_count+error_count+valid_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[(n, len(k)) for n, k in true_count_sets.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "typeDict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = ['MBA', 'TNO', 'NEO', 'TRO', 'CEN']\n",
    "for label in labels:\n",
    "    found_pure=0\n",
    "    found_valid=0\n",
    "    not_found=0\n",
    "    nobjects=0\n",
    "    for n in [-12, -15, -18, -21, -24]:\n",
    "    #for n in [-15]:\n",
    "        pure = set(run[n][0][5][4][0].keys())\n",
    "        total = set(run[n][0][5][4][0].keys()).union(run[n][0][5][5][0].keys())\n",
    "        for id in true_count_sets[n]:\n",
    "            if id in typeDict__[label]:\n",
    "                nobjects += 1\n",
    "                if id in pure:\n",
    "                    #print(n, id, label, 'found pure')\n",
    "                    found_pure += 1\n",
    "                elif id in total:\n",
    "                    #print(n, id, label, 'found valid')\n",
    "                    found_valid += 1\n",
    "                else:\n",
    "                    #print(n, id, label, 'not found')\n",
    "                    not_found += 1\n",
    "    print(label, found_pure, found_valid, not_found, nobjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "12211+1+68+52+9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run[-12][0][5][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in [-12, -15, -18, -21, -24]:\n",
    "    print(n, len(run[n][0][5][4][0]), len(run[n][0][5][5][0]), len(true_count_sets[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errors=0\n",
    "pure=0\n",
    "pure_valid=0\n",
    "accessible=0\n",
    "for n in [-12, -15, -18, -21, -24]:\n",
    "    pure += len(set(run[n][0][5][4][0].keys()))\n",
    "    pure_valid += len(set(run[n][0][5][4][0].keys()).union(run[n][0][5][5][0].keys()))\n",
    "    accessible += len(true_count_sets[n])\n",
    "    errors += run[n][0][5][2][0]\n",
    "    print(n, run[n][0][5][2][0], len(set(run[n][0][5][4][0].keys())), len(set(run[n][0][5][4][0].keys()).union(run[n][0][5][5][0].keys())), len(true_count_sets[n]))\n",
    "\n",
    "pure, pure_valid, accessible, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ITF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_select=0.4\n",
    "nside=8\n",
    "n=-14\n",
    "dt=5.0\n",
    "pix_runs = {}\n",
    "t_ref = lunation_center(n)\n",
    "pixels=[281]\n",
    "g_gdot = [(0.4, 0.0)]\n",
    "pickle_input_filename = 'data/itf_new_1_line_ec_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "pickle_output_filename = 'data/itf_new_1_line_%.1lf_pm15.0_%.1lf_run_v4.pickle' % (lunation_center(n), dt)\n",
    "with open(pickle_input_filename, 'rb') as handle:\n",
    "    with open('data/for_figure9.pickle', 'wb') as out_handle:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "        pix_dict = index_tracklets_gamma(g_select, tracklets_dict, nside=nside)\n",
    "        master = transform_to_arrows_for_regions_gamma_v2(t_ref, g_gdot, pixels, pix_dict, tracklets_dict)\n",
    "        theta_master = transform_to_theta_pairs_for_regions(t_ref, pixels, pix_dict, tracklets_dict)\n",
    "        radec_master = transform_to_radec_pairs_for_regions(t_ref, pixels, pix_dict, tracklets_dict)\n",
    "        pickle.dump((radec_master, theta_master, master), out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        results = accessible_clusters(g_select, range(hp.nside2npix(nside)), tracklets_dict)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select=0.4\n",
    "nside=8\n",
    "n=-14\n",
    "dt=5.0\n",
    "pix_runs = {}\n",
    "t_ref = lunation_center(n)\n",
    "pixels=[281]\n",
    "\n",
    "pickle_input_filename = 'data/itf_new_1_line_ec_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "pickle_output_filename = 'data/itf_new_1_line_%.1lf_pm15.0_%.1lf_run_v4.pickle' % (lunation_center(n), dt)\n",
    "with open(pickle_input_filename, 'rb') as handle:\n",
    "    with open('data/for_figure10.pickle', 'wb') as out_handle:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets  = pickle.load(handle)\n",
    "        pix_dict = index_tracklets_gamma(g_select, tracklets_dict, nside=nside)\n",
    "        master = transform_to_arrows_for_regions_gamma_v2(t_ref, g_gdots, pixels, pix_dict, tracklets_dict)\n",
    "        pickle.dump(master, out_handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now doing some ITF searches.\n",
    "import os.path\n",
    "\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "dt = 5\n",
    "rad = 0.002\n",
    "\n",
    "# Looping over many lunations.\n",
    "for n in range(-824, 11):\n",
    "    t_ref = lunation_center(n)\n",
    "    pickle_input_filename = 'data/itf_new_1_line_ec_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    if os.path.isfile(pickle_input_filename):\n",
    "        pickle_output_filename = 'data/itf_new_1_line_%.1lf_pm15.0_%.1lf_run.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, dt, rad, g_gdots=g_gdots)\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now doing some ITF searches.\n",
    "import os.path\n",
    "\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "dt = 5\n",
    "rad = 0.002\n",
    "\n",
    "# Looping over many lunations.\n",
    "for n in range(-824, 11):\n",
    "    t_ref = lunation_center(n)\n",
    "    pickle_input_filename = 'data/itf_20180515_1_line_ec_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    if os.path.isfile(pickle_input_filename):\n",
    "        pickle_output_filename = 'data/itf_20180515_1_line_%.1lf_pm15.0_%.1lf_run.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, dt, rad, g_gdots=g_gdots)\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Doing a run over a range of cluster radii to generate an s-curve.\n",
    "import os.path\n",
    "\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "dt = 5\n",
    "#rad = 0.0002\n",
    "rad = 0.0006\n",
    "\n",
    "# Looping over many lunations.\n",
    "#for n in range(-824, 11):\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "    t_ref = lunation_center(n)\n",
    "    pickle_input_filename = 'data/itf_new_1_line_ec_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    if os.path.isfile(pickle_input_filename):\n",
    "        pickle_output_filename = 'data/itf_new_1_line_%.1lf_pm15.0_%.1lf_run_training.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v4(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt], radii=np.logspace(-4, -2, num=30))\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "dt = 5\n",
    "#rad = 0.0002\n",
    "rad = 0.0006\n",
    "\n",
    "# Looping over many lunations.\n",
    "#for n in range(-824, 11):\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "    t_ref = lunation_center(n)\n",
    "    pickle_input_filename = 'data/itf_new_1_line_ec_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    if os.path.isfile(pickle_input_filename):\n",
    "        pickle_output_filename = 'data/itf_new_1_line_%.1lf_pm15.0_%.1lf_run_training_2t.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_training_run_v4(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, g_gdots=g_gdots, dts = [dt], radii=np.logspace(-4, -2, num=30), mincount=2)\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "trackletCounter_dict={}\n",
    "tracklets_dict={}\n",
    "tracklets_jd_dict={}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "    print(n)\n",
    "    pickle_input_filename = 'data/itf_new_1_line_ec_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    pickle_filename = 'data/itf_new_1_line_%.1lf_pm15.0_%.1lf_run_training.pickle' % (lunation_center(n), dt)\n",
    "    with open(pickle_input_filename, 'rb') as handle, open(pickle_filename, 'rb') as handle2:\n",
    "        tracklets_dict[n], tracklets_jd_dict[n], sortedTracklets,  = pickle.load(handle)\n",
    "        run[n] = pickle.load(handle2)\n",
    "        #true_count_sets[n], trackletCounter_dict[n]=accessible_clusters(g_select, range(hp.nside2npix(nside)), tracklets_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "trackletCounter_dict={}\n",
    "tracklets_dict={}\n",
    "tracklets_jd_dict={}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "for n in [-11, -14, -17, -20, -23]:\n",
    "    print(n)\n",
    "    pickle_input_filename = 'data/itf_new_1_line_ec_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    pickle_filename = 'data/itf_new_1_line_%.1lf_pm15.0_%.1lf_run_training_2t.pickle' % (lunation_center(n), dt)\n",
    "    with open(pickle_input_filename, 'rb') as handle, open(pickle_filename, 'rb') as handle2:\n",
    "        tracklets_dict[n], tracklets_jd_dict[n], sortedTracklets,  = pickle.load(handle)\n",
    "        run[n] = pickle.load(handle2)\n",
    "        #true_count_sets[n], trackletCounter_dict[n]=accessible_clusters(g_select, range(hp.nside2npix(nside)), tracklets_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run[-14][(5, 0.00067233575364993351)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for k, v in run[-11].items():\n",
    "    print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grab_run_pickled_results_v2(n, dt, file_stem='data/UnnObs_Training_1_line_C_', suff='_run.pickle'):\n",
    "    pickle_output_filename = file_stem + ('%.1lf_pm15.0_%.1lf' % (lunation_center(n), dt)) + suff\n",
    "    print(pickle_output_filename)\n",
    "    with open(pickle_output_filename, 'rb') as handle:\n",
    "        #inner_run = pickle.load(handle), _\n",
    "        inner_run = pickle.load(handle)\n",
    "        return inner_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=-14\n",
    "fs=14\n",
    "\n",
    "f, (ax1) = plt.subplots(1, sharex=True, sharey=False, figsize=(8,5))\n",
    "\n",
    "ns_total=[]\n",
    "ds = []\n",
    "\n",
    "\n",
    "#for dt in np.arange(5, 35, 5):\n",
    "#for dt in [1, 5, 10, 15, 20, 25, 30]:\n",
    "for dt in [5]:\n",
    "    ncs, nps, nts = [], [], []\n",
    "    for n in [-11]:\n",
    "        run = grab_run_pickled_results_v2(n, dt, file_stem='data/itf_new_1_line_', suff='_run_training.pickle')\n",
    "        for k in sorted([k for k, v in run.items()]):\n",
    "            ds.append(k[1])\n",
    "            ns_total.append(len(run[k]))\n",
    "\n",
    "    for n in [-14, -17, -20, -23]:\n",
    "        \n",
    "        ns=[]\n",
    "        run = grab_run_pickled_results_v2(n, dt, file_stem='data/itf_new_1_line_', suff='_run_training.pickle')\n",
    "        for k in sorted([k for k, v in run.items()]):\n",
    "            ns.append(len(run[k]))\n",
    "        ns_total = [np.sum(x) for x in zip(ns, ns_total)]        \n",
    "\n",
    "    ax1.plot(ds, ns_total, label='')\n",
    "\n",
    "ax1.set_xscale(\"log\", nonposx='clip')\n",
    "ax1.set_yscale(\"log\", nonposx='clip')\n",
    "low_lim, high_lim=100, 1e7\n",
    "ax1.set_ylim((low_lim, high_lim))\n",
    "\n",
    "ax1.axvline(6e-4, ls='dashed')\n",
    "ax1.axvline(2e-3, ls='dashed')\n",
    "\n",
    "best_rad = 0.003\n",
    "#ax3.axvline(best_rad, ls='solid')\n",
    "\n",
    "ax1.set_ylabel('N clusters', fontsize=fs)\n",
    "ax1.text(0.0001, 15000, r'$\\gamma=0.4$', fontsize=15)\n",
    "\n",
    "\n",
    "\n",
    "#band_min=0.001\n",
    "#band_max = 0.003\n",
    "#ax1.axvspan(band_min, band_max, 0.004, alpha=0.25, color='gray')\n",
    "\n",
    "f.subplots_adjust(hspace=0.1)\n",
    "plt.tight_layout()\n",
    "f.savefig('s_curve.png')\n",
    "f.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=-14\n",
    "fs=14\n",
    "\n",
    "f, (ax1) = plt.subplots(1, sharex=True, sharey=False, figsize=(8,5))\n",
    "\n",
    "ns_total=[]\n",
    "ds = []\n",
    "\n",
    "\n",
    "for dt in [5]:\n",
    "    ncs, nps, nts = [], [], []\n",
    "    for n in [-11]:\n",
    "        run = grab_run_pickled_results_v2(n, dt, file_stem='data/itf_new_1_line_', suff='_run_training.pickle')\n",
    "        for k in sorted([k for k, v in run.items()]):\n",
    "            ds.append(k[1])\n",
    "            ns_total.append(len(run[k]))\n",
    "\n",
    "    for n in [-14, -17, -20, -23]:\n",
    "        ns=[]\n",
    "        run = grab_run_pickled_results_v2(n, dt, file_stem='data/itf_new_1_line_', suff='_run_training.pickle')\n",
    "        for k in sorted([k for k, v in run.items()]):\n",
    "            ns.append(len(run[k]))\n",
    "        ns_total = [np.sum(x) for x in zip(ns, ns_total)]        \n",
    "\n",
    "    ax1.plot(ds, ns_total, label=r'$N_a\\geq 3$', ls='-')\n",
    "\n",
    "ns_total=[]\n",
    "ds = []\n",
    "#for dt in np.arange(5, 35, 5):\n",
    "#for dt in [1, 5, 10, 15, 20, 25, 30]:\n",
    "for dt in [5]:\n",
    "    ncs, nps, nts = [], [], []\n",
    "    for n in [-11]:\n",
    "        run = grab_run_pickled_results_v2(n, dt, file_stem='data/itf_new_1_line_', suff='_run_training_2t.pickle')\n",
    "        for k in sorted([k for k, v in run.items()]):\n",
    "            ds.append(k[1])\n",
    "            ns_total.append(len(run[k]))\n",
    "\n",
    "    for n in [-14, -17, -20, -23]:\n",
    "        ns=[]\n",
    "        run = grab_run_pickled_results_v2(n, dt, file_stem='data/itf_new_1_line_', suff='_run_training_2t.pickle')\n",
    "        for k in sorted([k for k, v in run.items()]):\n",
    "            ns.append(len(run[k]))\n",
    "        ns_total = [np.sum(x) for x in zip(ns, ns_total)]        \n",
    "\n",
    "    ax1.plot(ds, ns_total, label=r'$N_a\\geq 2$', ls='-.')\n",
    "\n",
    "ax1.set_xscale(\"log\", nonposx='clip')\n",
    "ax1.set_yscale(\"log\", nonposx='clip')\n",
    "low_lim, high_lim=100, 1e7\n",
    "ax1.set_ylim((low_lim, high_lim))\n",
    "\n",
    "ax1.axvline(6e-4, ls='dashed')\n",
    "ax1.axvline(2e-3, ls='dashed')\n",
    "\n",
    "best_rad = 0.003\n",
    "#ax3.axvline(best_rad, ls='solid')\n",
    "\n",
    "ax1.set_ylabel('N clusters', fontsize=fs)\n",
    "ax1.text(0.0001, 50000, r'$\\gamma=0.4$', fontsize=15)\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "\n",
    "#band_min=0.001\n",
    "#band_max = 0.003\n",
    "#ax1.axvspan(band_min, band_max, 0.004, alpha=0.25, color='gray')\n",
    "\n",
    "f.subplots_adjust(hspace=0.1)\n",
    "plt.tight_layout()\n",
    "f.savefig('s_curve_2t.png')\n",
    "f.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ns_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds=[]\n",
    "ns=[]\n",
    "for k in sorted([k for k, v in run[-20].items()]):\n",
    "    ds.append(k[1])\n",
    "    ns.append(len(run[-20][k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(ds, ns)\n",
    "plt.xscale(\"log\", nonposx='clip')\n",
    "plt.yscale(\"log\", nonposx='clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now doing some ITF searches.\n",
    "import os.path\n",
    "\n",
    "nside=8\n",
    "\n",
    "gs = [0.3, 0.35, 0.4, 0.45]\n",
    "gdots = [-0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "dt = 5\n",
    "#rad = 0.0002\n",
    "rad = 0.0006\n",
    "\n",
    "# Looping over many lunations.\n",
    "#for n in range(-824, 11):\n",
    "#for n in range(-24, 11):\n",
    "for n in [-23]:\n",
    "    t_ref = lunation_center(n)\n",
    "    pickle_input_filename = 'data/itf_new_1_line_ec_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    if os.path.isfile(pickle_input_filename):\n",
    "        pickle_output_filename = 'data/itf_new_1_line_%.1lf_pm15.0_%.1lf_run_sr_v3.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, dt, rad, g_gdots=g_gdots)\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now doing some ITF searches.\n",
    "import os.path\n",
    "\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "dt = 5\n",
    "#rad = 0.0002\n",
    "rad = 0.0006\n",
    "\n",
    "# Looping over many lunations.\n",
    "for n in range(11, -825, -1):\n",
    "#for n in range(-24, 11):\n",
    "#for n in [-23]:\n",
    "    t_ref = lunation_center(n)\n",
    "    pickle_input_filename = 'data/itf_new_1_line_ec_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    if os.path.isfile(pickle_input_filename):\n",
    "        pickle_output_filename = 'data/itf_new_1_line_%.1lf_pm15.0_%.1lf_run_from_s_curve.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, dt, rad, g_gdots=g_gdots)\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now doing some ITF searches.\n",
    "import os.path\n",
    "\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "dt = 5\n",
    "#rad = 0.0002\n",
    "rad = 0.0006\n",
    "\n",
    "# Looping over many lunations.\n",
    "for n in range(11, -825, -1):\n",
    "#for n in range(-24, 11):\n",
    "#for n in [-23]:\n",
    "    t_ref = lunation_center(n)\n",
    "    pickle_input_filename = 'data/itf_new_1_line_ec_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    if os.path.isfile(pickle_input_filename):\n",
    "        pickle_output_filename = 'data/itf_new_1_line_%.1lf_pm15.0_%.1lf_run_from_s_curve_2t.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, dt, rad, g_gdots=g_gdots, mincount=2)\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now doing some ITF searches.\n",
    "import os.path\n",
    "\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "dt = 5\n",
    "#rad = 0.0002\n",
    "rad = 0.0006\n",
    "\n",
    "# Looping over many lunations.\n",
    "for n in range(11, -825, -1):\n",
    "#for n in range(-24, 11):\n",
    "#for n in [-23]:\n",
    "    t_ref = lunation_center(n)\n",
    "    pickle_input_filename = 'data/itf_20180515_1_line_ec_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    if os.path.isfile(pickle_input_filename):\n",
    "        pickle_output_filename = 'data/itf_20180515_1_line_%.1lf_pm15.0_%.1lf_run_from_s_curve_2t.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, dt, rad, g_gdots=g_gdots, mincount=2)\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now doing some ITF searches.\n",
    "import os.path\n",
    "\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "dt = 5\n",
    "#rad = 0.0002\n",
    "rad = 0.0006\n",
    "\n",
    "# Looping over many lunations.\n",
    "#for n in range(40, 25, -1):\n",
    "for n in range(6, -825, -1):\n",
    "#for n in range(-24, 11):\n",
    "#for n in [-23]:\n",
    "    t_ref = lunation_center(n)\n",
    "    pickle_input_filename = 'data/itf_20180515_1_line_ec_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    if os.path.isfile(pickle_input_filename):\n",
    "        pickle_output_filename = 'data/itf_20180515_1_line_%.1lf_pm15.0_%.1lf_run_from_s_curve_2t.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = do_run_v3(range(hp.nside2npix(nside)), tracklets_dict,  t_ref, g_select, dt, rad, g_gdots=g_gdots, mincount=2)\n",
    "            with open(pickle_output_filename, 'wb') as out_handle:\n",
    "                pickle.dump(run, out_handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lunation_center(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now doing some ITF searches.\n",
    "import os.path\n",
    "\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "dt = 5\n",
    "#rad = 0.0002\n",
    "rad = 0.0006\n",
    "\n",
    "times=[]\n",
    "nt=[]\n",
    "nc=[]\n",
    "# Looping over many lunations.\n",
    "for n in range(-824, 11, +1):\n",
    "#for n in range(-24, 11):\n",
    "#for n in [-23]:\n",
    "    t_ref = lunation_center(n)\n",
    "    pickle_input_filename = 'data/itf_new_1_line_ec_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    if os.path.isfile(pickle_input_filename):\n",
    "        pickle_output_filename = 'data/itf_new_1_line_%.1lf_pm15.0_%.1lf_run_from_s_curve.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle, open(pickle_output_filename, 'rb') as handle2:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = pickle.load(handle2), _\n",
    "            times.append(lunation_center(n)-2400000.5)\n",
    "            nt.append(len(tracklets_dict))\n",
    "            nc.append(len(run[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now doing some ITF searches.\n",
    "import os.path\n",
    "\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "g_select = 0.4\n",
    "\n",
    "dt = 5\n",
    "#rad = 0.0002\n",
    "rad = 0.0006\n",
    "\n",
    "times=[]\n",
    "nt=[]\n",
    "nc_2t=[]\n",
    "# Looping over many lunations.\n",
    "for n in range(-824, 11, +1):\n",
    "#for n in range(-24, 11):\n",
    "#for n in [-23]:\n",
    "    t_ref = lunation_center(n)\n",
    "    pickle_input_filename = 'data/itf_new_1_line_ec_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    if os.path.isfile(pickle_input_filename):\n",
    "        pickle_output_filename = 'data/itf_new_1_line_%.1lf_pm15.0_%.1lf_run_from_s_curve_2t.pickle' % (lunation_center(n), dt)\n",
    "        with open(pickle_input_filename, 'rb') as handle, open(pickle_output_filename, 'rb') as handle2:\n",
    "            tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "            run = pickle.load(handle2), _\n",
    "            times.append(lunation_center(n)-2400000.5)\n",
    "            nt.append(len(tracklets_dict))\n",
    "            nc_2t.append(len(run[0][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('data/PostValidationClusters.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = np.loadtxt('data/PostValidationClusters2Tracklet.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(times, nc, label=r'$N_t\\geq 3$', color='red')\n",
    "plt.plot(times, nc_2t, label=r'$N_t\\geq 2$', color='orange')\n",
    "plt.plot(times, nt, label=r'$N_t$', color='green')\n",
    "plt.plot(data[:, 0]-2400000.5, data[:, 1], label=r'$N_{fitted,3}$', color='black')\n",
    "plt.plot(data2[:, 0]-2400000.5, data2[:, 1], label=r'$N_{fitted,2}$', color='blue')\n",
    "\n",
    "plt.xlim(50000, 58200)\n",
    "plt.ylim(1, 100000)\n",
    "plt.yscale(\"log\", nonposx='clip')\n",
    "plt.xlabel('MJD')\n",
    "plt.ylabel(r'$N_c, N_t$')\n",
    "#plt.legend()\n",
    "plt.legend(loc=2, prop={'size': 6})\n",
    "plt.savefig('number_clusters_tracklets.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum(data2[:,1])-np.sum(data[:,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for time, t, c in zip(times, nt, nc):\n",
    "    print(time, t, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lunation_center(-11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(run[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_select=0.4\n",
    "nside=8\n",
    "n=-11\n",
    "dt=5.0\n",
    "pix_runs = {}\n",
    "t_ref = lunation_center(n)\n",
    "pixels=[281]\n",
    "pixels=[342]\n",
    "g_gdot = [(0.4, 0.0)]\n",
    "pickle_input_filename = 'data/itf_new_1_line_ec_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "pickle_output_filename = 'data/itf_new_1_line_%.1lf_pm15.0_%.1lf_run.pickle' % (lunation_center(n), dt)\n",
    "with open(pickle_input_filename, 'rb') as handle:\n",
    "    with open('data/for_figure3.pickle', 'wb') as out_handle:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "        pix_dict = index_tracklets_gamma(g_select, tracklets_dict, nside=nside)\n",
    "        master = transform_to_arrows_for_regions_gamma_v2(t_ref, g_gdot, pixels, pix_dict, tracklets_dict)\n",
    "        theta_master = transform_to_theta_pairs_for_regions(t_ref, pixels, pix_dict, tracklets_dict)\n",
    "        radec_master = transform_to_radec_pairs_for_regions(t_ref, pixels, pix_dict, tracklets_dict)\n",
    "        pickle.dump((radec_master, theta_master, master), out_handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        results = accessible_clusters_v2(g_select, range(hp.nside2npix(nside)), tracklets_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "trackletCounter_dict={}\n",
    "tracklets_dict={}\n",
    "tracklets_jd_dict={}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "for n in [-11, -12, -13, -14, -15, -16, -17, -18, -19, -20, -21, -22, -23, -24]:\n",
    "    print(n)\n",
    "    pickle_input_filename = 'data/itf_new_1_line_ec_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    pickle_filename = 'data/itf_new_1_line_%.1lf_pm15.0_%.1lf_run_sr_v2.pickle' % (lunation_center(n), dt)\n",
    "    with open(pickle_input_filename, 'rb') as handle, open(pickle_filename, 'rb') as handle2:\n",
    "        tracklets_dict[n], tracklets_jd_dict[n], sortedTracklets,  = pickle.load(handle)\n",
    "        run[n] = pickle.load(handle2)\n",
    "        #true_count_sets[n], trackletCounter_dict[n]=accessible_clusters(g_select, range(hp.nside2npix(nside)), tracklets_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#total_count = sum([len(k) for n, k in true_count_sets.items()])\n",
    "#total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(run[-23][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracklets_dict[-23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nightNumber2date(nightNumber):\n",
    "    yr, mn, dy, hr = novas.cal_date(2400000.5+nightNumber)\n",
    "    dateString=\"%d%02d%02d\"%(yr, mn, dy)\n",
    "    return dateString\n",
    "\n",
    "def formatCluster(clusters, tracklets_jd_dict):\n",
    "    #print(run[-23][1][tuple(cluster)])\n",
    "    trkSub_set = Counter()\n",
    "    out_string=''\n",
    "    for cluster in clusters:\n",
    "        cluster = sorted(cluster, key=lambda x: tracklets_jd_dict[x])\n",
    "        A_desig = cluster[0][0].rstrip()\n",
    "        for tracklet in cluster:\n",
    "            #print(tracklet)\n",
    "            trkSub = tracklet[0]\n",
    "            trkSub_set.update({trkSub: 1})\n",
    "            ds = nightNumber2date(tracklet[1])\n",
    "            obsCode = tracklet[2]\n",
    "            out_string += \"%-9s(%-8s %12s %s\\n\" % (trkSub, A_desig, obsCode, ds)\n",
    "    return out_string, trkSub_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outstring, trksub_set = formatCluster(run[-23][0][0:100], tracklets_jd_dict[-23])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(0.2*24*5)/(3600*3600)/2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in range(-11, -25, -1):\n",
    "    print(n, len(run[n][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('hundred_v3.txt', 'w') as file:\n",
    "    file.write(outstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trksub_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_arrows=[[427, -0.17567806734210514, 0.0042757069983811614, -0.02412975283705987, 0.00021958825754706345, 0.4, 0.004], [427, -0.17543694459549289, 0.0043708753119676286, -0.024031248266783017, 0.00025804857354184887, 0.4, 0.004], [427, -0.17373899559551922, 0.0043404611463907332, -0.025318491889080728, 0.00018377086522694395, 0.4, 0.004], [427, -0.17497902613460772, 0.0042127028982437776, -0.02396721624445575, 0.00017977770450257167, 0.4, 0.004], [427, -0.17525776430802231, 0.0042096326590561469, -0.024382424932594588, 0.00034851354846388667, 0.4, 0.004], [427, -0.17617153188789403, 0.0043240084542567423, -0.025058221425584549, 0.00021180733702921844, 0.4, 0.004]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arrows=np.array([x[1:5] for x in full_arrows])\n",
    "arrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.quiver(arrows[:,0], arrows[:,2], arrows[:,1], arrows[:,3], scale=0.05, width=0.006)\n",
    "plt.xlim(-0.2, -0.16)\n",
    "plt.ylim(-0.0255, -0.0225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run[-23][1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracklets_dict[-23][('P10hLsT', 57039, 'F51')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(outstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=-23\n",
    "outstring=''\n",
    "for cluster in run[n][0]:\n",
    "    outstring += formatCluster(cluster, tracklets_jd_dict[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "36S0BZ  (K01Q51N ​​          699 20010628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[(n, len(k)) for n, k in true_count_sets.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(run[-23][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run[-23][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_found=0\n",
    "for n in range(-23, -10):\n",
    "    print(n, len(run[n][0][5][4][0]), len(run[n][0][5][5][0]))\n",
    "    total_found += len(run[n][0][5][4][0]) + len(run[n][0][5][5][0])\n",
    "    \n",
    "total_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "run = {}\n",
    "true_count_sets = {}\n",
    "dt=5\n",
    "g_select=0.4\n",
    "for n in [-11, -12, -13, -14, -15, -16, -17, -18, -19, -20, -21, -22, -23]:\n",
    "    print(n)\n",
    "    pickle_input_filename = 'data/UnnObs_Training_1_line_C_TNO_%.1lf_pm15.0.pickle' % (lunation_center(n))\n",
    "    pickle_filename = 'data/UnnObs_Training_1_line_C_TNO_%.1lf_pm15.0_%.1lf_g0.4_train.pickle' % (lunation_center(n), dt)\n",
    "    with open(pickle_input_filename, 'rb') as handle, open(pickle_filename, 'rb') as handle2:\n",
    "        tracklets_dict, tracklets_jd_dict, sortedTracklets,  = pickle.load(handle)\n",
    "        run[n] = pickle.load(handle2)\n",
    "        true_count_sets[n], trackletCounter_dict=accessible_clusters(g_select, range(hp.nside2npix(nside)), tracklets_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tno_total_count = sum([len(k) for n, k in true_count_sets.items()])\n",
    "tno_total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master[342]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other\n",
    "\n",
    "The stuff below here is earlier work that may or may not be relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_histogram(mergedCounter):\n",
    "    hist_dict=defaultdict(list)\n",
    "    for k, v in mergedCounter.items():\n",
    "        hist_dict[v].append(k)\n",
    "    return hist_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "num_occ=Counter()\n",
    "for pix in range(hp.nside2npix(nside)):\n",
    "    hist=make_histogram(mergedCounter_dict[pix])\n",
    "    for k, v in hist.items():\n",
    "        num_occ.update({k: len(v)})\n",
    "        \n",
    "num_occ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for dt in np.arange(5, 30, 5):\n",
    "    pixels=list(pix_runs.keys())\n",
    "    ds = pix_runs[pixels[0]][dt][0]\n",
    "    nclusters = pix_runs[pixels[0]][dt][1]\n",
    "    nerrors = pix_runs[pixels[0]][dt][2]\n",
    "    keys = pix_runs[pixels[0]][dt][3]\n",
    "    for pix in pixels[1:]:\n",
    "        nclusters = list(map(add, nclusters, pix_runs[pix][dt][1]))\n",
    "        nerrors = list(map(add, nerrors, pix_runs[pix][dt][2]))\n",
    "    nclusters=np.array(nclusters)\n",
    "    \n",
    "    plt.plot(ds, nclusters, label=dt)\n",
    "\n",
    "plt.xscale(\"log\", nonposx='clip')\n",
    "plt.axhline(true_count, ls='dashed')\n",
    "plt.xlabel('d (cluster radius)')\n",
    "plt.ylabel('N clusters')\n",
    "plt.text(0.001, 400, r'$\\gamma=0.4$', fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('nclusters_vs_d_z2.5.pdf')\n",
    "plt.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pbasis_to_State(v):\n",
    "    alpha, alpha_dot, beta, beta_dot, gamma, gamma_dot = v\n",
    "    z = 1.0/gamma\n",
    "    x = alpha*z\n",
    "    y = beta*z\n",
    "    xd = alpha_dot*z\n",
    "    yd = beta_dot*z\n",
    "    zd = gamma_dot*z\n",
    "    return kc.State(x, y, z, xd, yd, zd)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GMsun = MPC_library.Constants.GMsun\n",
    "g=0.4\n",
    "mm = np.sqrt(GMsun*g**3)\n",
    "adot = mm\n",
    "kc.keplerian(GMsun, pbasis_to_State([0.0, 0, 0, adot, g, 0.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GMsun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pbasis_to_elements(GM, pbasis, r_ref):\n",
    "    mat = proj_to_xyz_matrix(r_ref)\n",
    "    state = pbasis_to_State(pbasis)\n",
    "    x, y, z, xd, yd, zd = state.x, state.y, state.z, state.xd, state.yd, state.zd\n",
    "    xp, yp, zp = np.dot(mat, np.array((x, y, z)))\n",
    "    xdp, ydp, zdp =  np.dot(mat, np.array((xd, yd, zd)))\n",
    "    statep = kc.State(xp, yp, zp, xdp, ydp, zdp)\n",
    "    return kc.keplerian(GMsun, statep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pbasis_to_elements(GMsun, [0.0, -adot, 0, 0.0, g, 0], (1, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dt in np.arange(5, 30, 5):\n",
    "    pixels=list(pix_runs.keys())\n",
    "    ds = pix_runs[pixels[0]][dt][0]\n",
    "    nclusters = pix_runs[pixels[0]][dt][1]\n",
    "    nerrors = pix_runs[pixels[0]][dt][2]\n",
    "    for pix in pixels[1:]:\n",
    "        nclusters = list(map(add, nclusters, pix_runs[pix][dt][1]))\n",
    "        nerrors = list(map(add, nerrors, pix_runs[pix][dt][2]))\n",
    "    nclusters=np.array(nclusters)\n",
    "    nerrors=np.array(nerrors)\n",
    "            \n",
    "    \n",
    "    plt.plot(ds, nerrors, label=dt)\n",
    "\n",
    "plt.xscale(\"log\", nonposx='clip')\n",
    "plt.ylim((0,3000))\n",
    "plt.xlabel('d (cluster radius)')\n",
    "plt.ylabel('N errors')\n",
    "plt.text(0.0005, 1000, r'$\\gamma=0.4$', fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('nerrors_vs_d_z2.5.pdf')\n",
    "plt.show()\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for dt in np.arange(5, 30, 5):\n",
    "    pixels=list(pix_runs.keys())\n",
    "    ds = pix_runs[pixels[0]][dt][0]\n",
    "    nclusters = pix_runs[pixels[0]][dt][1]\n",
    "    nerrors = pix_runs[pixels[0]][dt][2]\n",
    "    for pix in pixels[1:]:\n",
    "        nclusters = list(map(add, nclusters, pix_runs[pix][dt][1]))\n",
    "        nerrors = list(map(add, nerrors, pix_runs[pix][dt][2]))\n",
    "    nclusters=np.array(nclusters)\n",
    "    nerrors=np.array(nerrors)    \n",
    "    \n",
    "    plt.plot(nerrors/true_count, nclusters/true_count, label=dt)\n",
    "\n",
    "plt.xlim((0.0001, 0.1))\n",
    "#plt.xscale(\"log\", nonposx='clip')\n",
    "#plt.ylim((0, 1))\n",
    "plt.xlabel('Error rate')\n",
    "plt.ylabel('Fraction complete')\n",
    "plt.text(0.05, 0.2, r'$\\gamma=0.4$', fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('AUCish_z2.5.pdf')\n",
    "plt.show()\n",
    "# \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max(nclusters), true_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to plot where the errors are occurring, to see if they are happening near quadrature.  It doesn't seem to be the case.\n",
    "\n",
    "Here's a working goal.  Let's try to achieve 90% completeness with minimal errors.  From the runs I have done, dt=20 does achieve 90% completeness, with a 4% error rate.  \n",
    "\n",
    "I can try a few more dt values, around 20.\n",
    "\n",
    "An alternative approach is to aim to be less complete but with a lower error rate.  And then remove those clusters and iterate.  \n",
    "\n",
    "The errors are false positives.  That is, we say they are clusters, but they are at least contaminated.\n",
    "\n",
    "From the plot below it looks like we want dt=10 days, a completeness of 85%, and an error rate of 1%.  We achieve that at d=0.0008.\n",
    "\n",
    "Actually, if we go with dt=50 days, we can achieve 50% completeness with not recorded errors (or an error rate of less than 0.1%.  Why don't we do with that for a first passs?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "error_rate_limit=1e-3\n",
    "training_dict={}\n",
    "for n in [-14]:\n",
    "    infilename='data/UnnObs_Training_1_line_A_ec_labelled_%.1lf_pm15.0_r2.5.trans' % (lunation_center(n))\n",
    "    pickle_filename = infilename.rstrip('trans') + 'train.v3_pickle'\n",
    "    dt=15\n",
    "    with open(pickle_filename, 'rb') as handle:\n",
    "        pix_runs = pickle.load(handle)\n",
    "        pixels=list(pix_runs.keys())\n",
    "    \n",
    "        # Should save these results in files\n",
    "        clusterID_set, mergedCounter_dict, mergedTime_dict=accessible_clusters(list(pix_runs.keys()), infilename=infilename)\n",
    "        true_count=len(clusterID_set)\n",
    "        true_count\n",
    "\n",
    "        print(n)\n",
    "        for i in range(99):\n",
    "            errs=0\n",
    "            clusts=0\n",
    "            trues=0\n",
    "            for pix in list(pix_runs.keys()):\n",
    "                nclusters = pix_runs[pixels[pix]][dt][1][i]\n",
    "                nerrors = pix_runs[pixels[pix]][dt][2][i]\n",
    "                errs += nerrors\n",
    "                clusts += nclusters\n",
    "            if float(errs)/true_count<error_rate_limit:\n",
    "                print(i, pix_runs[pixels[pix]][dt][0][i], errs, clusts, true_count)\n",
    "            else:\n",
    "                training_dict[n] = pix_runs[pixels[pix]][dt][0][i], errs, clusts, true_count\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the average of the cluster radii that yield the highest completeness for an error rate less than 0.1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_radius = np.mean([v[0] for k, v in training_dict.items()])\n",
    "cluster_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = [0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "gs =[0.4]\n",
    "gdots = [-0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "def do_test_run(pixels, infilename, t_ref, \n",
    "                    cluster_sky_function=cluster_sky_regions,\n",
    "                    g_gdots=g_gdots, mincount=3, \n",
    "                    dt=15,\n",
    "                    rad=0.00124):                \n",
    "    \n",
    "    master = cluster_sky_function(g_gdots, pixels, t_ref, infilename)\n",
    "\n",
    "    results_dict={}\n",
    "    rates_dict={}\n",
    "    cluster_counter = Counter()\n",
    "    for pix, d in master.items():\n",
    "        for z_zdot, arrows in d.items():\n",
    "\n",
    "            # The bit from here\n",
    "            i = 0\n",
    "            label_dict={}\n",
    "            combined=[]\n",
    "            for k, cx, mx, cy, my, t in arrows:\n",
    "                label_dict[i] = k\n",
    "                combined.append([cx, mx*dt, cy, my*dt])\n",
    "                i +=1\n",
    "            points=np.array(combined)\n",
    "            # to here can be a function,\n",
    "            # that takes arrows and dt and\n",
    "            # returns label_dict and points array\n",
    "            \n",
    "            # The bit from here\n",
    "            tree = scipy.spatial.cKDTree(points)\n",
    "            matches = tree.query_ball_tree(tree, rad)\n",
    "            # to here can be a function, that takes\n",
    "            # points are rad and returns tree and\n",
    "            # matches\n",
    "                    \n",
    "            for j, match in enumerate(matches):\n",
    "                if len(match)>=mincount:\n",
    "                    # From here is about making a cluster key \n",
    "                    # from the indices and label dictionary\n",
    "                    cluster_list =[]\n",
    "                    for idx in match:\n",
    "                        cluster_list.append(label_dict[idx].strip())\n",
    "                        #tracklet_params.append(combined[idx])\n",
    "                    cluster_key='|'.join(sorted(cluster_list))\n",
    "                    # to here\n",
    "                    \n",
    "                    cluster_counter.update({cluster_key: 1})\n",
    "            \n",
    "    test_set = list(cluster_counter.keys()) \n",
    "    success_dict, failure_counter = unique_clusters(test_set)\n",
    "   \n",
    "    values = len(success_dict), len(failure_counter), list(success_dict.keys()), list(failure_counter.keys())\n",
    "\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am timing a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_runs={}\n",
    "for n in [-14]:\n",
    "    infilename='data/UnnObs_Training_1_line_A_ec_labelled_%.1lf_pm15.0_r2.5.trans' % (lunation_center(n))\n",
    "    pickle_filename = infilename.rstrip('trans') + 'test.pickle'\n",
    "    t_ref=lunation_center(n)\n",
    "    pixels=range(hp.nside2npix(nside))\n",
    "    test_runs[n] = do_test_run(pixels, infilename, t_ref, mincount=3, rad=0.00124)\n",
    "    with open(pickle_filename, 'wb') as handle:\n",
    "        pickle.dump(test_runs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_runs={}\n",
    "for n in [-14]:\n",
    "    infilename='data/itf_new_1_line_ec_%.1lf_pm15.0_r2.5.trans' % (lunation_center(n))\n",
    "    pickle_filename = infilename.rstrip('trans') + 'test.pickle'\n",
    "    t_ref=lunation_center(n)\n",
    "    pixels=range(hp.nside2npix(nside))\n",
    "    test_runs[n] = do_test_run(pixels, infilename, t_ref, mincount=3, rad=0.00124)\n",
    "    with open(pickle_filename, 'wb') as handle:\n",
    "        pickle.dump(test_runs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "\n",
    "gs =[0.4]\n",
    "gdots = [-0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "def do_run_v2(pixels, tracklets_dict, t_ref, \n",
    "           g_select, dt, rad,\n",
    "           transform_to_arrows_for_regions_function=transform_to_arrows_for_regions_gamma_v2,\n",
    "           g_gdots=g_gdots, mincount=3, nside=8): \n",
    "    \n",
    "    pix_dict = index_tracklets_gamma(g_select, tracklets_dict, nside=nside)\n",
    "\n",
    "    print(\"here1\")\n",
    "    \n",
    "    master = transform_to_arrows_for_regions_function(t_ref, g_gdots, pixels, pix_dict, tracklets_dict)\n",
    "    \n",
    "    print(\"here2\")\n",
    "    results = do_sky_v2(dt, rad, master, mincount=mincount)\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "g_select = 0.4\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "dt = 5\n",
    "rad = 0.002\n",
    "\n",
    "# Looping over lunation, separated by 3 months each\n",
    "for n in range(-824, 11):\n",
    "    t = lunation_center(n)\n",
    "    print(t)\n",
    "    infilename='data/itf_new_1_line_ec_%.1lf_pm15.0.trans' % (t)\n",
    "    pickle_filename = infilename.rstrip('trans') + 'g0.4_dt5_rad0.002.pickle'\n",
    "    run = do_run_v2(range(hp.nside2npix(nside)), td[n], t, 0.4, 5, 0.002, g_gdots=g_gdots, mincount=3)\n",
    "    lrun = list(run)\n",
    "    if len(lrun)>0:\n",
    "        with open(pickle_filename, 'wb') as handle:\n",
    "            pickle.dump(lrun, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 800\n",
    "run = do_run_v2(range(hp.nside2npix(nside)), td[i], time_centers_lunation[i], 0.4, 5, 0.002, g_gdots=g_gdots, mincount=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(td[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracklets_subset={}\n",
    "for cluster in run:\n",
    "    for tracklet in cluster:\n",
    "        tracklets_subset[tracklet] = tracklets_n[tracklet]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_filename = 'data/UnnObs_C_' + '%05d_' %(i) + 'train.pickle'\n",
    "with open(pickle_filename, 'wb') as handle:\n",
    "    pickle.dump((run, tracklets_subset), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(list(unique_clusters_v2(run[21:135])[0].keys())))\n",
    "print(len(list(unique_clusters_v2(run[21:135])[1].keys())))\n",
    "print(len(list(unique_clusters_v2(run[21:135])[2].keys())))\n",
    "for cluster in run[21:35]:\n",
    "    for key in cluster:\n",
    "        print(key)\n",
    "        for line in tracklets_n[key]:\n",
    "            print(line[1].strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(td[800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(td[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(td[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster = (('P10h7YQ', 57016, 'F51'),  ('P10h7YS', 57016, 'F51'),  ('P10h7YU', 57016, 'F51'))\n",
    "for key in cluster:\n",
    "    print(key)\n",
    "    for line in tracklets5[key]:\n",
    "        print(line[1].strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for cluster in run:\n",
    "    for i, key in enumerate(cluster):\n",
    "        print(i, key)\n",
    "        for line, line_orig in tracklets5[key]:\n",
    "            print(line_orig.strip())\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracklets5[('P10rP83', 57416, 'F51')]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracklets5[('XBBA30E', 57416, 'G96')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tracklet_time_windows(tracklets, sortedTracklets, tracklets_jd_dict, time_centers, dt=15.):\n",
    "\n",
    "    jds = [tracklets_jd_dict[k] for k in sortedTracklets]\n",
    "    \n",
    "    results =[]\n",
    "    \n",
    "    for t in time_centers:\n",
    "                \n",
    "        i = bisect.bisect(jds, t-dt, 0, len(jds))\n",
    "        j = bisect.bisect(jds, t+dt, 0, len(jds))\n",
    "        results.append((i, j))\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = tracklet_time_windows(tracklets5, sortedTracklets5, tracklets_jd_dict5, time_centers, dt=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = tracklet_time_windows(tracklets_n, sortedTracklets_n, tracklets_jd_dict_n, time_centers_lunation, dt=15.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_centers_lunation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results[24000], time_centers[24000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sortedTracklets5[2802251:2805370]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def temp_dict(i, results, tracklets, sortedTracklets):\n",
    "    temp_dict = {k: tracklets[k] for k in sortedTracklets[results[i][0]:results[i][1]]}\n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "td=[]\n",
    "for i in range(len(results)):\n",
    "    td.append(temp_dict(i, results, tracklets_n, sortedTracklets_n))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results[-10], results[-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(results), len(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "td[800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "g_select = 0.4\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "dt = 25\n",
    "rad = 0.001\n",
    "\n",
    "# Looping over lunation, separated by 3 months each\n",
    "for n in [-11, -12, -13, -14, -15, -16, -17, -18, -19, -20, -21, -22, -23]:\n",
    "    infilename='data/itf_new_1_line_ec_%.1lf_pm15.0_g.trans' % (lunation_center(n))\n",
    "    pickle_filename = infilename.rstrip('trans') + 'g0.4_dt25_rad0.001.pickle'\n",
    "    run = do_run(range(hp.nside2npix(nside)), infilename, lunation_center(n), g_select, dt, rad, g_gdots=g_gdots)\n",
    "    lrun = list(run)\n",
    "    with open(pickle_filename, 'wb') as handle:\n",
    "        pickle.dump(lrun, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "g_select = 0.4\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "dt = 25\n",
    "rad = 0.0001\n",
    "\n",
    "# Looping over lunation, separated by 3 months each\n",
    "for t in time_centers:\n",
    "    print(t)\n",
    "    infilename='data/itf_new_1_line_ec_%.1lf_pm0.5_g.trans' % (t)\n",
    "    if os.path.isfile(infilename):\n",
    "        pickle_filename = infilename.rstrip('trans') + 'g0.4_dt25_rad0.0001.pickle'\n",
    "        run = do_run(range(hp.nside2npix(nside)), infilename, t, g_select, dt, rad, g_gdots=g_gdots, mincount=2)\n",
    "        lrun = list(run)\n",
    "        if len(lrun)>0:\n",
    "            with open(pickle_filename, 'wb') as handle:\n",
    "                pickle.dump(lrun, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "nside=8\n",
    "\n",
    "gs = [0.4]\n",
    "g_select = 0.4\n",
    "gdots = [-0.004, -0.002, 0.0, 0.002, 0.004]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "dt = 25\n",
    "rad = 0.0001\n",
    "\n",
    "# Looping over lunation, separated by 3 months each\n",
    "for t in time_centers:\n",
    "    print(t)\n",
    "    infilename='data/itf_new_1_line_ec_%.1lf_pm0.5_g.trans' % (t)\n",
    "    if os.path.isfile(infilename):\n",
    "        pickle_filename = infilename.rstrip('trans') + 'g0.4_dt25_rad0.001.pickle'\n",
    "        run = do_run(range(hp.nside2npix(nside)), infilename, t, g_select, dt, rad, g_gdots=g_gdots, mincount=2)\n",
    "        lrun = list(run)\n",
    "        if len(lrun)>0:\n",
    "            with open(pickle_filename, 'wb') as handle:\n",
    "                pickle.dump(lrun, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "pairs = {}\n",
    "for t in time_centers:\n",
    "    infilename='data/itf_new_1_line_ec_%.1lf_pm0.5_g.trans' % (t)\n",
    "    pickle_filename = infilename.rstrip('trans') + 'g0.4_dt25_rad0.0001.pickle'\n",
    "    if os.path.isfile(pickle_filename):\n",
    "        with open(pickle_filename, 'rb') as handle:\n",
    "            pairs[t] = pickle.load(handle)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "keys = sorted(pairs.keys())\n",
    "for t in keys:\n",
    "    print(t)\n",
    "    for key in pairs[t]:\n",
    "        print(key)\n",
    "    if(i>10000):\n",
    "        break\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lunation_center(-11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I need to update this with the new tracklet keys, but that should be easy\n",
    "def get_original_tracklets_dict(filename='data/UnnObs_Training_1_line_A.mpc'):\n",
    "    tracklets = defaultdict(list)\n",
    "    with open(filename) as infile:\n",
    "        i=0\n",
    "        for line in infile:\n",
    "            if not line.startswith('#'):\n",
    "                desig = line[0:12].strip()\n",
    "                tracklets[desig].append(i)\n",
    "                i +=1\n",
    "    return tracklets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "            jd_tdb = sortedTimes[idx0]\n",
    "            mjdp = int(jd_tdb-2400000.5)\n",
    "            key = (trackletID, mjdp, obsCode)\n",
    "            if key not in tracklets_jd_dict:\n",
    "                # Got a new tracklet\n",
    "                tracklets_jd_dict[key] = jd_tdb\n",
    "            tracklets[key].extend(sortedLines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_original_observation_array(filename='data/UnnObs_Training_1_line_A.txt'):\n",
    "    with open(filename) as infile:\n",
    "        i=0\n",
    "        data=[]\n",
    "        for line in infile:\n",
    "            if not line.startswith('#'):\n",
    "                data.append(line)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UnnObs_tracklets_dict = get_original_tracklets_dict()\n",
    "UnnObs_obs_array = get_original_observation_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itf_tracklets_dict = get_original_tracklets_dict(filename='data/itf_new_1_line_ec.mpc')\n",
    "itf_obs_array = get_original_observation_array(filename='data/itf_new_1_line.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_observations(cluster_key, tracklets_dict, observation_array, sep='|'):\n",
    "    array=[]\n",
    "    for key in cluster_key.split(sep):\n",
    "        indices = tracklets_dict[key]\n",
    "        for idx in indices:\n",
    "            array.append(observation_array[idx].rstrip())\n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs_dict={}\n",
    "for cluster_key in lrun:\n",
    "    obs_dict[cluster_key] = get_observations(cluster_key, itf_tracklets_dict, itf_obs_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys = sorted(pairs.keys())\n",
    "obs_dict ={}\n",
    "for t in keys:\n",
    "    for cluster_key in pairs[t]:\n",
    "        obs_dict[cluster_key] = get_observations(cluster_key, itf_tracklets_dict, itf_obs_array)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(obs_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for k, v in obs_dict.items():\n",
    "    print(k)\n",
    "    for line in v:\n",
    "        print('blah', line)\n",
    "    if i>200:\n",
    "        break\n",
    "    i +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to look for exact duplicate and near duplicate tracklets.  Most of the clusters I have been processing have just two constituent tracklets, because I have restricted the time to one day.  So it will be relatively easy to check for duplicates, but it won't be hard to check all pairs of tracklets even if there is the occasional cluster with more than two tracklets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "5/(24*3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def same_observation(line1, line2):\n",
    "    objName1, provDesig1, disAst1, note11, note21, dateObs1, RA1, Dec1, mag1, filt1, obsCode1 = convertObs80(line1)\n",
    "    objName2, provDesig2, disAst2, note12, note22, dateObs2, RA2, Dec2, mag2, filt2, obsCode2 = convertObs80(line2)\n",
    "    print(line1.rstrip())\n",
    "    print(line2.rstrip())\n",
    "    return (obsCode1==obsCode2) and (RA1==RA2) and (Dec1==Dec2) and (dateObs1==dateObs2)\n",
    "\n",
    "def raDec2xyz(RA, Dec):\n",
    "    raDeg, decDeg = MPC_library.RA2degRA(RA), MPC_library.Dec2degDec(Dec)\n",
    "    x = np.cos(decDeg*np.pi/180.)*np.cos(raDeg*np.pi/180.)\n",
    "    y = np.cos(decDeg*np.pi/180.)*np.sin(raDeg*np.pi/180.)  \n",
    "    z = np.sin(decDeg*np.pi/180.)\n",
    "    return x, y, z\n",
    "\n",
    "def similar_observation(line1, line2, delta_t, delta_angle):\n",
    "    objName1, provDesig1, disAst1, note11, note21, dateObs1, RA1, Dec1, mag1, filt1, obsCode1 = convertObs80(line1)\n",
    "    objName2, provDesig2, disAst2, note12, note22, dateObs2, RA2, Dec2, mag2, filt2, obsCode2 = convertObs80(line2)\n",
    "\n",
    "    jd_utc1 = MPC_library.date2JD(dateObs1)\n",
    "    jd_utc2 = MPC_library.date2JD(dateObs2)\n",
    "    \n",
    "    if (obsCode1 != obsCode2) or (np.abs(jd_utc2-jd_utc1) > delta_t):\n",
    "        return False\n",
    "    else:\n",
    "        x1, y1, z1 = raDec2xyz(RA1, Dec1)\n",
    "        x2, y2, z2 = raDec2xyz(RA2, Dec2)\n",
    "        return np.arccos(np.round(x1*x2 + y1*y2 + z1*z2, 10) < delta_angle)\n",
    "    \n",
    "def time_similar_observation(line1, line2, delta_t):\n",
    "    objName1, provDesig1, disAst1, note11, note21, dateObs1, RA1, Dec1, mag1, filt1, obsCode1 = convertObs80(line1)\n",
    "    objName2, provDesig2, disAst2, note12, note22, dateObs2, RA2, Dec2, mag2, filt2, obsCode2 = convertObs80(line2)\n",
    "\n",
    "    jd_utc1 = MPC_library.date2JD(dateObs1)\n",
    "    jd_utc2 = MPC_library.date2JD(dateObs2)\n",
    "    \n",
    "    return (obsCode1 == obsCode2) and (np.abs(jd_utc2-jd_utc1) <= delta_t)\n",
    "\n",
    "def same_observatory_time(line1, line2):\n",
    "    objName1, provDesig1, disAst1, note11, note21, dateObs1, RA1, Dec1, mag1, filt1, obsCode1 = convertObs80(line1)\n",
    "    objName2, provDesig2, disAst2, note12, note22, dateObs2, RA2, Dec2, mag2, filt2, obsCode2 = convertObs80(line2)\n",
    "    return (obsCode1==obsCode2) and (dateObs1==dateObs2)\n",
    "\n",
    "def same_tracklet(key1, key2, tracklets_dict, observation_array):\n",
    "    indices1 = tracklets_dict[key1]\n",
    "    indices2 = tracklets_dict[key2]\n",
    "    return all([same_observation(observation_array[idx1], observation_array[idx2]) for idx1, idx2 in zip(indices1, indices2)])\n",
    "\n",
    "def similar_tracklet(key1, key2, tracklets_dict, observation_array, delta_t, delta_angle):\n",
    "    indices1 = tracklets_dict[key1]\n",
    "    indices2 = tracklets_dict[key2]\n",
    "    return all([similar_observation(observation_array[idx1], observation_array[idx2], delta_t, delta_angle) for idx1, idx2 in zip(indices1, indices2)])\n",
    "\n",
    "def time_similar_tracklet(key1, key2, tracklets_dict, observation_array, delta_t):\n",
    "    indices1 = tracklets_dict[key1]\n",
    "    indices2 = tracklets_dict[key2]\n",
    "    return all([time_similar_observation(observation_array[idx1], observation_array[idx2], delta_t) for idx1, idx2 in zip(indices1, indices2)])\n",
    "\n",
    "def check_for_duplicates(cluster_key, tracklets_dict, observation_array, sep='|', delta_t=5.8e-5, delta_angle=5e-5):\n",
    "    array=[]\n",
    "    print(cluster_key)\n",
    "    if len(cluster_key.split(sep))>2:\n",
    "        print('Long one')\n",
    "    for key1 in cluster_key.split(sep):\n",
    "        for key2 in cluster_key.split(sep):\n",
    "            if key1>=key2:\n",
    "                continue\n",
    "            if same_tracklet(key1, key2, tracklets_dict, observation_array):\n",
    "                print(key1, key2, 'are the same')\n",
    "            elif similar_tracklet(key1, key2, tracklets_dict, observation_array, delta_t, delta_angle):\n",
    "                print(key1, key2, 'are similar')\n",
    "            elif time_similar_tracklet(key1, key2, tracklets_dict, observation_array, delta_t):\n",
    "                print(key1, key2, 'are time similar')\n",
    "                print(delta_t)\n",
    "            else:\n",
    "                print(key1, key2, 'are distinct')\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys = sorted(pairs.keys())\n",
    "obs_dict ={}\n",
    "for t in keys:\n",
    "    for cluster_key in pairs[t]:\n",
    "        check_for_duplicates(cluster_key, itf_tracklets_dict, itf_obs_array)\n",
    "        obs_dict[cluster_key] = get_observations(cluster_key, itf_tracklets_dict, itf_obs_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.round(1+1e-8, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_obs(tracklet, tracklets_dict=itf_tracklets_dict, tracklets_obs_array=itf_obs_array):\n",
    "    for idx in tracklets_dict[tracklet]:\n",
    "        print(tracklets_obs_array[idx].rstrip())\n",
    "\n",
    "print_obs('W031C4')\n",
    "print()\n",
    "print_obs('W031C5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=-14\n",
    "pickle_filename = \"run_%d_v3.pickle\" %(n)\n",
    "with open(pickle_filename, 'wb') as handle:\n",
    "    pickle.dump(obs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(obs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Should just passing the selection function, to avoid code duplication\n",
    "def generate_sky_region_files(pixels, infilename='data/UnnObs_Training_1_line_ec_labelled_2457308.5_pm15.0_r2.5.trans', nside=8, n=-14, angDeg=7.5, g=0.4, gdot=0.0):\n",
    "    hp_dict = defaultdict(list)\n",
    "    with open(infilename) as file:\n",
    "        for line in file:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            pix = int(line.split()[-1])\n",
    "            hp_dict[pix].append(line)\n",
    "\n",
    "    #for i in range(hp.nside2npix(nside)):\n",
    "    #for i in [281, 574]:\n",
    "    for i in pixels:\n",
    "        vec = hp.pix2vec(nside, i, nest=True)\n",
    "        neighbors = hp.query_disc(nside, vec, angDeg*np.pi/180., inclusive=True, nest=True)\n",
    "        lines = []\n",
    "        for pix in neighbors:\n",
    "            for line in hp_dict[pix]:\n",
    "                lines.append(line)\n",
    "        outfilename = infilename.rstrip('.trans') + '_hp_' + ('%03d' % (i)) + '_g'+ ('%.2lf' % (g))+'_gdot' + ('%+5.1le' % (gdot))\n",
    "        if len(lines) > 0:\n",
    "            select_positions_z(lunation_center(n), g, gdot, vec, lines, outfilename)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lunation_center(-14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdots = [-0.006, -0.005, -0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006]\n",
    "for gdot in gdots:\n",
    "    generate_sky_region_files([281], infilename='data/UnnObs_Training_1_line_A_ec_labelled_2457308.5_pm15.0_r2.5.trans', n=-14, gdot=gdot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdots = [-0.006, -0.005, -0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006]\n",
    "for gdot in gdots:\n",
    "    generate_sky_region_files([281], infilename='data/itf_new_1_line_ec_2457308.5_pm15.0_r2.5.trans', n=-14, gdot=gdot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdots = [-0.006, -0.005, -0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006]\n",
    "for gdot in gdots:\n",
    "    generate_sky_region_files([273], infilename='data/UnnObs_Training_1_line_C_ec_labelled_TNO__2457308.5_pm15.0_r2.5.trans', n=-14, gdot=gdot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "def make_figure(filename):\n",
    "    plt.ioff()\n",
    "    mxs, cxs, mys, cys, dts =[], [], [], [], []\n",
    "    for line in open(filename):\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        desig, cx, mx, cy, my, dt = line.split()\n",
    "        mxs.append(float(mx))\n",
    "        cxs.append(float(cx))\n",
    "        mys.append(float(my))\n",
    "        cys.append(float(cy))\n",
    "        dts.append(float(dt))\n",
    "    \n",
    "    fig=plt.figure(figsize=(18, 16))\n",
    "\n",
    "    #norm = Normalize()\n",
    "    #norm.autoscale(colors)\n",
    "    \n",
    "    colormap = cm.inferno\n",
    "\n",
    "    plt.quiver(cxs, cys, mxs, mys, dts, scale=0.3, width=0.0006)\n",
    "\n",
    "    plt.xlim(-0.1, 0.1)\n",
    "    plt.ylim(-0.1, 0.1)\n",
    "    plt.xlabel('alpha')\n",
    "    plt.ylabel('beta')\n",
    "    outfile = filename+'.pdf'\n",
    "    plt.savefig(outfile)\n",
    "    plt.close()\n",
    "    plt.ion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "def make_figure_desig(filename):\n",
    "    desig_dict={}\n",
    "    i=0\n",
    "    plt.ioff()\n",
    "    mxs, cxs, mys, cys, dts =[], [], [], [], []\n",
    "    desig_vals = []\n",
    "    for line in open(filename):\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        desig, cx, mx, cy, my, dt = line.split()\n",
    "        stem = desig[:-4]\n",
    "        if stem not in desig_dict:\n",
    "            desig_dict[stem]=i\n",
    "            i += 1\n",
    "        mxs.append(float(mx))\n",
    "        cxs.append(float(cx))\n",
    "        mys.append(float(my))\n",
    "        cys.append(float(cy))\n",
    "        dts.append(float(dt))\n",
    "        desig_vals.append(desig_dict[stem])\n",
    "    \n",
    "    fig=plt.figure(figsize=(18, 16))\n",
    "\n",
    "    #norm = Normalize()\n",
    "    #norm.autoscale(colors)\n",
    "    \n",
    "    colormap = cm.inferno\n",
    "\n",
    "    plt.quiver(cxs, cys, mxs, mys, desig_vals, scale=0.3, width=0.0006)\n",
    "\n",
    "    plt.xlim(-0.1, 0.1)\n",
    "    plt.ylim(-0.1, 0.1)\n",
    "    plt.xlabel('alpha')\n",
    "    plt.ylabel('beta')\n",
    "    outfile = filename+'.pdf'\n",
    "    plt.savefig(outfile)\n",
    "    plt.close()\n",
    "    plt.ion()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = [0.4]\n",
    "gdots = [-0.006, -0.005, -0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "n=-14\n",
    "for g_gdot in g_gdots:\n",
    "    g, gdot = g_gdot\n",
    "    for i in [281, 574]:\n",
    "    #for i in range(hp.nside2npix(nside)):\n",
    "        infilename = 'data/UnnObs_Training_1_line_A_ec_labelled_'+str(lunation_center(n))+'_pm15.0_r2.5.trans'\n",
    "        filename = infilename.rstrip('.trans') + '_hp_%03d_g%4.2lf_gdot%+3.1le' % (i, g, gdot)\n",
    "        try:\n",
    "            make_figure(filename)\n",
    "        except:\n",
    "            pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = [0.4]\n",
    "gdots = [-0.006, -0.005, -0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "n=-14\n",
    "for g_gdot in g_gdots:\n",
    "    g, gdot = g_gdot\n",
    "    for i in [273]:\n",
    "    #for i in range(hp.nside2npix(nside)):\n",
    "        infilename = 'data/UnnObs_Training_1_line_C_ec_labelled_TNO__'+str(lunation_center(n))+ '_pm15.0_r2.5.trans'\n",
    "        filename = infilename.rstrip('.trans') + '_hp_%03d_g%4.2lf_gdot%+3.1le' % (i, g, gdot)\n",
    "        try:\n",
    "            make_figure_desig(filename)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = [0.4]\n",
    "#gdots = [-0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004]\n",
    "gdots = [-0.006, -0.005, -0.004, -0.003, -0.002, -0.001, 0.0, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006]\n",
    "g_gdots = [(x,y) for x in gs for y in gdots]\n",
    "\n",
    "n=-14\n",
    "for g_gdot in g_gdots:\n",
    "    g, gdot = g_gdot\n",
    "    for i in [281, 574]:\n",
    "    #for i in range(hp.nside2npix(nside)):\n",
    "        infilename = 'data/itf_new_1_line_ec_'+str(lunation_center(n))+'_pm15.0_r2.5.trans'\n",
    "        filename = infilename.rstrip('.trans') + '_hp_%03d_g%4.2lf_gdot%+3.1le' % (i, g, gdot)\n",
    "        try:\n",
    "            make_figure(filename)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a game plan:\n",
    "\n",
    "* Verify the dark time numbers --DONE.\n",
    "* Stick with a smallish time window (+/- 15 days) --DONE.\n",
    "* Do heliocentric transformation for r=2.5 AU (rdot=0) --DONE.\n",
    "* Do selection_positions_z for all the time slices.  This will generate a large number of files.\n",
    "* Identify the largest files.  Find a way to not generate empty files.\n",
    "* Run clustering on each file, identifying candidate clusters.\n",
    "* Somehow vet the candidate clusters.  I can think of three ways: (1) use Gareth's code, (2) use Bernstein's code, or (3) fit a simple linear model.   The third would be a very quick way of clustering tracklets within a lunation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The r2.5 run is just to separate the tracklets into sky regions.  Given those sky regions, I can examine a runs of z and zdot.  It seems that a simple grid of a few values in z (3 or 4) and zdot (7 or 8) might be enough.\n",
    "\n",
    "Perhaps it is possible to do all the clustering for a sky region of a given time slice all in one pass.  It's not necessary to write out the intermediate files.  Given a set of tracklets, transform for a (z, zdot) value and then search for clusters.\n",
    "\n",
    "If a cluster is found, make a key of the tuple of sorted trackletIDs and then add that key to a dictionary or counter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here's a new game plan:\n",
    "\n",
    "* Select 5 different disjoint time slices.  These will be +/- 15 days and +/- 45 days.\n",
    "* Run the training on those time slices for all sky regions for both +/- 15 days and +/- 45 days.  The former requires only the gamma-dot parameter to be varied, for a few values of gammma.  The latter probably requires a finer grid in gamma and gamma-dot.  In both cases, the hyperparameters that we are tuning are the cluster radius and the time factor dt.\n",
    "* If I want to do repeated universal kepler steps, I need to make that fast in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I want to plan how to process all the months of the ITF.\n",
    "\n",
    "The parameters are dt=15 days and rad=0.0020 radians.  This yields high completeness and a low error rate.\n",
    "\n",
    "For each time slice:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lunation_center(-16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "nside = 8\n",
    "itf_runs={}\n",
    "#for n in range(-824, 11):\n",
    "for n in range(-14, 11):\n",
    "    infilename='data/itf_new_1_line_ec_%.1lf_pm15.0_r2.5.trans' % (lunation_center(n))\n",
    "    print(n, infilename)\n",
    "    if not os.path.isfile(infilename):\n",
    "        continue\n",
    "    pickle_filename = infilename.rstrip('trans') + 'test.pickle'\n",
    "    t_ref=lunation_center(n)\n",
    "    pixels=range(hp.nside2npix(nside))\n",
    "    itf_runs[n] = do_run(pixels, infilename, t_ref, mincount=3, rad=0.00200)\n",
    "    with open(pickle_filename, 'wb') as handle:\n",
    "        pickle.dump(itf_runs[n], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lunation_center(-17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, requests\n",
    "test = 'blah, blah, blah'\n",
    "payload = {'source': test.rstrip(\"\\n\")}\n",
    "r = requests.get('https://minorplanetcenter.net/mpc_orbit', params=payload)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for n in range(-824, 11):\n",
    "    if n==-104 or n==-82:\n",
    "        continue\n",
    "    infilename='data/itf_new_1_line_ec_%.1lf_pm15.0_r2.5.trans' % (lunation_center(n))\n",
    "    pickle_filename = infilename.rstrip('trans') + 'test.pickle'\n",
    "    outfilename='data/itf_new_1_line_ec_%+03d_%.1lf_pm15.0_g0.4_clusters.pickle' % (n, lunation_center(n))\n",
    "    print(n, pickle_filename, outfilename)\n",
    "    if not os.path.isfile(pickle_filename):\n",
    "            continue\n",
    "    with open(pickle_filename, 'rb') as handle:\n",
    "        itf_run = pickle.load(handle)\n",
    "        obs_dict={}\n",
    "        for cluster_key in itf_run.keys():\n",
    "            obs_dict[cluster_key] = get_observations(cluster_key, itf_tracklets_dict, itf_obs_array)\n",
    "        with open(outfilename, 'wb') as handle:\n",
    "            pickle.dump(obs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obs_dict"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
